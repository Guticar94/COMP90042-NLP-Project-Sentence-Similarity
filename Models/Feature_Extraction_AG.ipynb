{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train claims\n",
    "with open('../data/train-claims.json', 'r') as f:\n",
    "    df_train = pd.DataFrame(json.load(f)).transpose()\n",
    "\n",
    "# Read dev claims\n",
    "with open('../data/dev-claims.json', 'r') as f:\n",
    "    df_dev = pd.DataFrame(json.load(f)).transpose()\n",
    "\n",
    "# Read evidence\n",
    "with open('../data/evidence.json', 'r') as f:\n",
    "    evidence = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\n",
      "DISPUTED\n",
      "['evidence-442946', 'evidence-1194317', 'evidence-12171']\n",
      "\n",
      "evidence-442946: At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.\n",
      "evidence-1194317: Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.\n",
      "evidence-12171: Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.\n"
     ]
    }
   ],
   "source": [
    "print(df_train.iloc[0,0])\n",
    "print(df_train.iloc[0,1])\n",
    "print(df_train.iloc[0,2])\n",
    "print()\n",
    "print(f'{df_train.iloc[0,2][0]}: {evidence[df_train.iloc[0,2][0]]}')\n",
    "print(f'{df_train.iloc[0,2][1]}: {evidence[df_train.iloc[0,2][1]]}')\n",
    "print(f'{df_train.iloc[0,2][2]}: {evidence[df_train.iloc[0,2][2]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform text to numbers and inverse\n",
    "def txt_encod(evidence: list(), to_id=False, to_txt=False):\n",
    "    # evidence: Evidence text \n",
    "\n",
    "    # Transform to id\n",
    "    if to_id and to_txt:\n",
    "        print('Error: You have to pass only one true parameter')\n",
    "        return 0\n",
    "    if to_id:\n",
    "        return [vocab.index(token) for token in evidence]\n",
    "    # Transform to text\n",
    "    elif to_txt:\n",
    "        return [vocab[token] for token in evidence]\n",
    "    else:\n",
    "        print('Error: You have to pass to_id or to_txt parameter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evidence claims\n",
    "ev = [j for i,j in evidence.items()]\n",
    "# Add BOS and EOS\n",
    "evidence_texts = ['[' + ev + ']' for ev in ev[:1000]]\n",
    "# Tokenize text\n",
    "evidence_texts = [tokenize.word_tokenize(text) for text in evidence_texts]\n",
    "# Get Vocabulary list\n",
    "vocab = set()\n",
    "[[vocab.add(tok) for tok in text] for text in evidence_texts]\n",
    "vocab = list(vocab)\n",
    "# Texts to numeric\n",
    "num_txts = [np.array(txt_encod(text, True)) for text in evidence_texts]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data to tensor batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.text = texts\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        texts = torch.tensor(self.text[idx], dtype=torch.float32)\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.int64).reshape(-1,1)\n",
    "        return texts, labels\n",
    "    \n",
    "# Define collate (pre_process) function\n",
    "def collate_batch(batch):  \n",
    "    texts, labels = zip(*batch)\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True).to(device)\n",
    "    return texts, labels\n",
    "\n",
    "# Instanciate DataLoader\n",
    "bs = 32\n",
    "tr_ds = Dataset(num_txts, range(len(num_txts)))\n",
    "tr_dl = DataLoader(tr_ds, batch_size=bs, collate_fn=collate_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.1 Positional encoding to embed the data</h3></center>\n",
    "\n",
    "<center><img src=../Images/pos_encoder.png alt=\"drawing\" width=\"300\"></center>\n",
    "\n",
    "<center>Details on:</center>\n",
    "<center><a href=\"https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\"><ph>A Gentle Introduction to Positional Encoding in Transformer Models</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Positional embeding function\n",
    "class positionalEmbeding(nn.Module):\n",
    "    def __init__(self, model_dim, max_len = 5000,  drop = 0.2):\n",
    "        # Inputs:\n",
    "        # model_dim: Vocabulary of the training set\n",
    "        # max_len: Max number of tokens in an input sentence\n",
    "        # Return: Positional Embeding Matrix\n",
    "        super(positionalEmbeding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=drop)                                                                   # Dropout layer\n",
    "        \n",
    "        # Positional embeding matrix \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)                                 # Positional increasing vector [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))      # Division tern for the sin/cos functions\n",
    "        pe = torch.zeros(max_len, model_dim)                                                                # Matrix of 0's [max_len,d_model]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)                                                        # 0::2 means starting with index 0, step = 2\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)                                                        # 1::2 means starting with index 1, step = 2\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)                                                                # [max_len, 1, 1]\n",
    "        self.register_buffer('pe', pe)                                                                      # Adds pos encoder to the model state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input:\n",
    "        # x: Embeding matrix dim\n",
    "        self.pe[:x.size(0), :]      # Create the embeding\n",
    "        return self.dropout(x)      # Apply dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.2 Transformer model (Passage Ranking)</h3></center>\n",
    "<center>Source papers:</center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Attention Is All You Need</ph></a></center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Text and Code Embeddings by Contrastive Pre-Training</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class passageRanking(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(passageRanking, self)\n",
    "        self.positionalEncoding = TransformerEncoderLayer(d_model=44, nhead=44)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
