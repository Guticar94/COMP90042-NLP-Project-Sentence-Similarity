{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train claims\n",
    "with open('../data/train-claims.json', 'r') as f:\n",
    "    df_train = pd.DataFrame(json.load(f)).transpose()\n",
    "\n",
    "# Read dev claims\n",
    "with open('../data/dev-claims.json', 'r') as f:\n",
    "    df_dev = pd.DataFrame(json.load(f)).transpose()\n",
    "\n",
    "# Read evidence\n",
    "with open('../data/evidence.json', 'r') as f:\n",
    "    evidence = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\n",
      "DISPUTED\n",
      "['evidence-442946', 'evidence-1194317', 'evidence-12171']\n",
      "\n",
      "evidence-442946: At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.\n",
      "evidence-1194317: Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.\n",
      "evidence-12171: Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.\n"
     ]
    }
   ],
   "source": [
    "print(df_train.iloc[0,0])\n",
    "print(df_train.iloc[0,1])\n",
    "print(df_train.iloc[0,2])\n",
    "print()\n",
    "print(f'{df_train.iloc[0,2][0]}: {evidence[df_train.iloc[0,2][0]]}')\n",
    "print(f'{df_train.iloc[0,2][1]}: {evidence[df_train.iloc[0,2][1]]}')\n",
    "print(f'{df_train.iloc[0,2][2]}: {evidence[df_train.iloc[0,2][2]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform text to numbers and inverse\n",
    "def txt_encod(evidence: list(), to_id=False, to_txt=False):\n",
    "    # evidence: Evidence text \n",
    "\n",
    "    # Transform to id\n",
    "    if to_id and to_txt:\n",
    "        print('Error: You have to pass only one true parameter')\n",
    "        return 0\n",
    "    if to_id:\n",
    "        return [vocab.index(token) for token in evidence]\n",
    "    # Transform to text\n",
    "    elif to_txt:\n",
    "        return [vocab[token] for token in evidence]\n",
    "    else:\n",
    "        print('Error: You have to pass to_id or to_txt parameter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evidence claims\n",
    "ev = [j for i,j in evidence.items()]\n",
    "# Add BOS and EOS\n",
    "evidence_texts = ['[' + ev + ']' for ev in ev[:1000]]\n",
    "# Tokenize text\n",
    "evidence_texts = [tokenize.word_tokenize(text) for text in evidence_texts]\n",
    "# Get Vocabulary list\n",
    "vocab = set()\n",
    "[[vocab.add(tok) for tok in text] for text in evidence_texts]\n",
    "vocab = list(vocab)\n",
    "# Texts to numeric\n",
    "num_txts = [np.array(txt_encod(text, True)) for text in evidence_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data to tensor batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.text = texts\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        texts = torch.tensor(self.text[idx])\n",
    "        labels = torch.tensor(self.labels[idx]).reshape(-1,1)\n",
    "        return texts, labels\n",
    "    \n",
    "# Define collate (pre_process) function\n",
    "def collate_batch(batch):  \n",
    "    texts, labels = zip(*batch)\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True).to(device)\n",
    "    return texts, labels\n",
    "\n",
    "# Instanciate DataLoader\n",
    "bs = 32\n",
    "tr_ds = Dataset(num_txts, range(len(num_txts)))\n",
    "tr_dl = DataLoader(tr_ds, batch_size=bs, collate_fn=collate_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.1 Positional encoding to embed the data</h3></center>\n",
    "\n",
    "<center><img src=../Images/pos_encoder.png alt=\"drawing\" width=\"300\"></center>\n",
    "\n",
    "<center>Details on:</center>\n",
    "<center><a href=\"https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\"><ph>A Gentle Introduction to Positional Encoding in Transformer Models</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "max_len_ = max([len(i) for i in num_txts]) # Maximum number of tokens in a sentence\n",
    "# Positional embeding function\n",
    "class positionalEmbeding(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop = 0.2, max_len = max_len_):\n",
    "        # Inputs:\n",
    "        # embedding_dim: Length of input embeding\n",
    "        # max_len: Max number of tokens in an input sentence\n",
    "        # Return: Positional Embeding Matrix\n",
    "        super(positionalEmbeding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=drop)                                                                           # Dropout layer\n",
    "        \n",
    "        # Positional embeding matrix \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)                                         # Positional increasing vector [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))      # Division term for the sin/cos functions\n",
    "        pe = torch.zeros(max_len, embedding_dim)                                                                    # Matrix of 0's [max_len, embedding_dim]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)                                                                # 0::2 means starting with index 0, step = 2\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)                                                                # 1::2 means starting with index 1, step = 2\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)                                                                        # Resize pos encoder [max_len, 1, embedding_dim]\n",
    "        self.register_buffer('pe', pe)                                                                              # Adds pos encoder to the model state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input:\n",
    "        # x: Embeding matrix [batch_size, text_length, embedding_dim]\n",
    "        x = x + self.pe[:x.size(0), :x.size(1)]      # Sum the position embeding\n",
    "        return self.dropout(x)              # Apply dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.2 Multihead attention</h3></center>\n",
    "<center><img src=../Images/attention.png alt=\"drawing\" width=\"600\"></center>\n",
    "\n",
    "<center>Details on:</center>\n",
    "<center><a href=\"https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\"><ph>Build your own Transformer from scratch using Pytorch</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert embedding_dim % num_heads == 0, \"in_size must be divisible by num_heads\"\n",
    "\n",
    "        self.embedding_dim = embedding_dim                      # Embeding input size\n",
    "        self.num_heads = num_heads                              # Num heads of multihead attention model\n",
    "        self.head_dim = embedding_dim // num_heads              # Embedding parameters for each head\n",
    "        \n",
    "        # Instanciate weights\n",
    "        self.W_q = nn.Linear(embedding_dim, embedding_dim)      # Query weights\n",
    "        self.W_k = nn.Linear(embedding_dim, embedding_dim)      # Key weights\n",
    "        self.W_v = nn.Linear(embedding_dim, embedding_dim)      # Values weights\n",
    "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    # scaled_dot_product_attention\n",
    "    def dot_prd_attn(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)   # MatMult (Q*K)\n",
    "        if mask is not None: attn_scores = attn_scores.masked_fill(mask == 0, -1e9)     # Masking (Optional)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)                                 # Softmax\n",
    "        output = torch.matmul(attn_probs, V)                                            # MatMult (Probs*V)\n",
    "        return output\n",
    "    \n",
    "    # Function to split attention heads\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, embedding_dim = x.size()\n",
    "        return x.view(batch_size, self.num_heads, seq_length, self.head_dim)\n",
    "    # Function to join attention heads\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, head_dim = x.size()\n",
    "        return x.view(batch_size, seq_length, self.embedding_dim)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        # Weights linear pass (Random inicialization) + Split heads\n",
    "        Q = self.split_heads(self.W_q(x))\n",
    "        K = self.split_heads(self.W_k(x))\n",
    "        V = self.split_heads(self.W_v(x))\n",
    "        # Multihead attention\n",
    "        attn = self.dot_prd_attn(Q, K, V, mask)                 # scaled_dot_product_attention\n",
    "        attn = self.combine_heads(attn)                         # Concat heads\n",
    "        attn = self.linear(attn)                                # Linear pass\n",
    "        return attn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.5 Transformer model (Passage Ranking)</h3></center>\n",
    "<center>Source papers:</center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Attention Is All You Need</ph></a></center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Text and Code Embeddings by Contrastive Pre-Training</ph></a></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Encoder:</center>\n",
    "<center><img src=../Images/encoder.png alt=\"drawing\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class based \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                vocab_size,                            # Size of vocabulary\n",
    "                embedding_dim,                         # Embedding dimension\n",
    "                n_head,                                # Number of heads  in the multihead attention model\n",
    "                hidden_dim,                            # Hiden dims for the feed forward pass\n",
    "                dropout = 0.5):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)                  # Embeding layer\n",
    "        self.pos_encoder = positionalEmbeding(embedding_dim, dropout)           # Positional embeding\n",
    "        self.multihead = MultiHeadAttention(embedding_dim, n_head)              # Multihead attention layer\n",
    "        self.normalization = nn.LayerNorm(embedding_dim)                        # Normalization layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Feed forward pass\n",
    "        self.feed_forward = nn.Sequential()\n",
    "        self.feed_forward.add_module('fc1', nn.Linear(embedding_dim, hidden_dim))\n",
    "        self.feed_forward.add_module('relu', nn.ReLU())\n",
    "        self.feed_forward.add_module('fc2', nn.Linear(hidden_dim, embedding_dim))\n",
    "\n",
    "    def forward(self, text):\n",
    "        encoder = self.encoder(text) * math.sqrt(self.embedding_dim)            # Encode imput text [batch_size, text_length, embedding_dim]\n",
    "        pos_enc = self.pos_encoder(encoder)                                     # Reurn pos encoder [batch_size, text_length, embedding_dim]\n",
    "        attn = self.multihead(pos_enc)                                          # Multihead encoder\n",
    "        normal = self.normalization(text.unsqueeze(2) + self.dropout(attn))     # Add & Normalize pass #1  UNSQUEEZE\n",
    "        forward = self.feed_forward(normal)                                     # Feed Forward pass\n",
    "        encoded = self.normalization(normal + self.dropout(forward))            # Add & Normalize pass #2\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "n_head = 2\n",
    "dropout = 0.5\n",
    "hidden_dim = 2048\n",
    "model = Encoder(vocab_size, embedding_dim, n_head, hidden_dim, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 44, 300])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = model(next(iter(tr_dl))[0])\n",
    "enc.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.9594e-01, -2.4603e-01,  7.7858e-02,  ..., -1.0842e+00,\n",
       "           1.8080e+00,  2.2548e-01],\n",
       "         [ 2.8391e-01,  9.3860e-01, -8.9123e-02,  ..., -8.7096e-01,\n",
       "           4.7969e-02, -7.5383e-01],\n",
       "         [ 7.4855e-02, -5.8413e-01, -1.3787e+00,  ..., -6.4854e-01,\n",
       "          -5.2938e-01, -2.9025e-02],\n",
       "         ...,\n",
       "         [-2.6756e+00,  8.2335e-01,  1.9251e+00,  ..., -6.6846e-01,\n",
       "           2.4282e+00, -9.7128e-01],\n",
       "         [-2.9915e+00,  1.0594e+00,  5.9534e-01,  ...,  2.8972e+00,\n",
       "           1.3887e-01,  6.4214e-02],\n",
       "         [-1.5426e+00,  1.6817e+00,  1.9284e+00,  ..., -4.2212e-01,\n",
       "          -5.3812e-02, -8.2272e-01]],\n",
       "\n",
       "        [[ 7.6182e-01, -4.7167e-01,  1.1669e+00,  ..., -1.3793e+00,\n",
       "          -6.3860e-02, -3.8378e-01],\n",
       "         [ 7.1473e-01, -1.2070e+00,  2.3097e-01,  ..., -6.4373e-02,\n",
       "          -6.4373e-02,  4.0369e-01],\n",
       "         [-2.7365e-01, -1.7996e+00, -3.7576e-02,  ..., -1.1863e+00,\n",
       "          -1.3729e+00,  1.3109e+00],\n",
       "         ...,\n",
       "         [-1.1603e-01,  5.8230e-01, -1.1603e-01,  ..., -7.3647e-01,\n",
       "           8.6153e-01, -2.9677e-01],\n",
       "         [ 2.3734e+00, -1.6376e-01, -1.0421e+00,  ..., -3.5133e-01,\n",
       "           1.9042e-02, -7.3846e-01],\n",
       "         [ 3.4107e+00, -2.0800e-01,  8.2781e-02,  ..., -5.4542e-01,\n",
       "           2.7172e+00, -2.6299e-01]],\n",
       "\n",
       "        [[-3.9123e-02,  8.2006e-01, -9.6211e-01,  ..., -1.5415e+00,\n",
       "           3.7746e-01, -4.3473e-01],\n",
       "         [ 3.5410e-01, -5.8358e-02,  1.6614e-02,  ..., -4.7876e-01,\n",
       "          -3.4546e-01, -7.0246e-01],\n",
       "         [-1.4915e+00, -5.7857e-02,  1.2466e+00,  ..., -5.7857e-02,\n",
       "          -5.7857e-02, -5.7857e-02],\n",
       "         ...,\n",
       "         [ 6.6277e-02, -2.9744e-01,  4.2701e-01,  ..., -9.6865e-01,\n",
       "           8.9024e-02, -1.3549e+00],\n",
       "         [-4.6752e-01, -2.2520e-02,  3.8284e-02,  ..., -1.1354e-03,\n",
       "           3.8284e-02, -2.3316e-02],\n",
       "         [ 2.6603e-01, -1.0571e+00, -5.5608e-02,  ..., -1.6452e+00,\n",
       "           5.8118e-02,  4.3945e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-9.7703e-01, -2.3805e+00,  2.6031e+00,  ..., -3.7913e-01,\n",
       "           1.5205e+00,  2.3384e-03],\n",
       "         [ 6.7156e-01, -5.4207e-01,  5.2906e-01,  ..., -1.1570e-02,\n",
       "          -1.1570e-02,  9.9313e-02],\n",
       "         [ 1.9171e+00, -5.6162e-01, -3.1997e-01,  ..., -6.4769e-01,\n",
       "          -4.1590e-01, -4.3799e-01],\n",
       "         ...,\n",
       "         [-2.6540e-01,  3.3783e-01, -2.9611e-02,  ...,  1.0685e+00,\n",
       "          -1.8707e+00,  6.3951e-02],\n",
       "         [ 3.3407e-01, -9.7515e-02, -1.5756e-01,  ...,  5.8204e-01,\n",
       "           1.9062e+00, -4.6653e-01],\n",
       "         [ 1.7136e-02, -6.7858e-02,  1.2542e+00,  ..., -6.7858e-02,\n",
       "           3.0578e-01,  1.2543e+00]],\n",
       "\n",
       "        [[-4.9494e-01,  2.4507e-01,  1.2501e+00,  ...,  1.9230e+00,\n",
       "           3.3174e-02,  3.3174e-02],\n",
       "         [ 3.0819e-03,  1.2842e+00, -2.7960e-02,  ...,  1.5711e+00,\n",
       "          -2.1452e-01,  6.3873e-01],\n",
       "         [-4.2052e-01, -1.9802e-02, -1.9802e-02,  ..., -1.9994e+00,\n",
       "           1.2027e+00, -5.8700e-01],\n",
       "         ...,\n",
       "         [-3.3870e-01, -2.6969e-02, -2.6969e-02,  ...,  1.7397e+00,\n",
       "           8.0008e-01, -3.6572e-01],\n",
       "         [-1.6097e+00, -3.7837e-01,  3.4709e-01,  ..., -5.3378e-01,\n",
       "          -2.9283e-02, -2.9283e-02],\n",
       "         [-2.1776e-01,  1.0395e+00,  3.1393e-02,  ...,  3.1393e-02,\n",
       "           3.1393e-02, -5.1353e-01]],\n",
       "\n",
       "        [[ 1.0973e-01,  4.8132e-01,  8.7673e-01,  ..., -6.2890e-01,\n",
       "           1.5125e+00,  6.4560e-01],\n",
       "         [ 2.7737e-01,  2.5384e-02,  4.2773e-01,  ...,  6.3955e-01,\n",
       "          -2.3121e+00, -2.9269e-01],\n",
       "         [ 1.3862e+00,  2.7010e-01, -9.0901e-02,  ..., -9.0901e-02,\n",
       "          -9.0901e-02, -2.2862e-01],\n",
       "         ...,\n",
       "         [-1.3844e+00,  5.1956e-02,  2.1730e+00,  ...,  2.5994e+00,\n",
       "           2.1756e+00, -1.7703e+00],\n",
       "         [ 7.5947e-01,  4.9229e-01,  4.5614e-01,  ..., -3.3301e-01,\n",
       "           6.7602e-01, -1.9210e+00],\n",
       "         [ 5.9989e-01, -9.5040e-01,  8.9922e-01,  ...,  2.3671e-01,\n",
       "          -3.5684e-01, -1.8284e+00]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
