{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas nltk numpy torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read evidence\n",
    "with open('../data/evidence.json', 'r') as f:\n",
    "    evidence = json.load(f)\n",
    "eviden = pd.DataFrame.from_dict(evidence, orient='index', columns=['evidence'])\n",
    "ev_txt = eviden['evidence'].values\n",
    "max_len = max([len(j.split()) for i,j in evidence.items()])\n",
    "\n",
    "# Read train claims\n",
    "with open('../data/train-claims.json', 'r') as f:\n",
    "    df_train = pd.DataFrame(json.load(f)).transpose()\n",
    "\n",
    "# Read dev claims\n",
    "with open('../data/dev-claims.json', 'r') as f:\n",
    "    df_dev = pd.DataFrame(json.load(f)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evidence len: 1208827 * dev len 1228 = 1484439556\n"
     ]
    }
   ],
   "source": [
    "print(f'evidence len: {len(ev_txt)} * dev len {df_train.shape[0]} = {len(ev_txt)*df_train.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to prepare datasets\n",
    "def prepare_df(df):\n",
    "    # Support labels\n",
    "    df_1 = df.explode(\"evidences\")\n",
    "    df_1['evidences_text'] = [evidence[item] for item in df_1['evidences']]\n",
    "    df_1['label'] = 1\n",
    "    # Refuse labels\n",
    "    df_2 = df_1[['claim_text']].copy()\n",
    "    df_2['evidences_text'] = [random.choice(ev_txt) for i in range(df_2.shape[0])]\n",
    "    df_2['label'] = 0\n",
    "    df = pd.concat([df_1[['claim_text' , 'evidences_text', 'label']], df_2]).sample(frac=1)\n",
    "    return df\n",
    "\n",
    "# Select columns to work on and retrieve tokenized and preprocesed vectors \n",
    "def feature_selection(df):\n",
    "    # Prepare df\n",
    "    df_ = prepare_df(df)\n",
    "    # Set words to lower and tokenize\n",
    "    tok_evidence = [word_tokenize(i.lower()) for i in df_['evidences_text']]\n",
    "    tok_claim = [word_tokenize(i.lower()) for i in df_['claim_text']]\n",
    "    # Drop unknown characters (This may be modified depending model performance)\n",
    "    tok_evidence = [' '.join([w for w in seq if re.match('^[\\w\\d]+$', w)]) for seq in tok_evidence]\n",
    "    tok_claim = [' '.join([w for w in seq if re.match('^[\\w\\d]+$', w)]) for seq in tok_claim]\n",
    "    # Class label\n",
    "    label = df_['label']\n",
    "    return tok_claim, tok_evidence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data features\n",
    "claim_train, evid_train, y_train = feature_selection(df_train)\n",
    "claim_dev, evid_dev, y_dev = feature_selection(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer class ( Can be improved)\n",
    "class token:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"[PAD]\": 0, \"[CLS]\": 1, \"[SEP]\": 2, \"[MASK]\": 3}\n",
    "        self.index2word = {0: \"[PAD]\", 1: \"[CLS]\", 2: \"[SEP]\", 3: \"[MASK]\"}\n",
    "        self.n_words = 4  # Count CLS and SEP\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "# tokenizer function\n",
    "def tok(corpus):\n",
    "    [tokenizer.addSentence(i) for i in corpus]\n",
    "    \n",
    "# Add tokens to idx dict\n",
    "tokenizer = token()\n",
    "\n",
    "# Create dicts\n",
    "tok(claim_train)\n",
    "tok(evid_train)\n",
    "tok(claim_dev)\n",
    "tok(evid_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data to tensor batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.1 Bert embeding</h3></center>\n",
    "\n",
    "<center><img src=../Images/BERT_emb.png alt=\"drawing\" width=\"500\"></center>\n",
    "<center><img src=../Images/BERT_emb_example.png alt=\"drawing\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=max_len):\n",
    "        self.text = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        # Step 1: get text tokens\n",
    "        sent = [self.tokenizer.word2index[i] for i in self.text[idx].split()]\n",
    "        \n",
    "        # Step 2: replace random words in sentence with mask / random words\n",
    "        sent_mask, labels = self.masking(sent)\n",
    "\n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentence\n",
    "        # Adding PAD token for labels\n",
    "        sent = [self.tokenizer.word2index['[CLS]']] + sent_mask + [self.tokenizer.word2index['[SEP]']]\n",
    "        labels = [self.tokenizer.word2index['[PAD]']] + labels + [self.tokenizer.word2index['[PAD]']]\n",
    "\n",
    "        # Step 4: Add PAD tokens to make the sentence same length as seq_len\n",
    "        padding = [self.tokenizer.word2index['[PAD]'] for empty in range(self.seq_len - len(sent))]\n",
    "        sent.extend(padding)\n",
    "        labels.extend(padding)\n",
    "        return np.array(sent), np.array(labels)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------\n",
    "    # Function to mask/randomize tokens\n",
    "    def masking(self, tokens, to_replace = 0.15):\n",
    "        # tokens = input.split()\n",
    "        output = []\n",
    "        label = []\n",
    "        for token in tokens:\n",
    "            prob = random.random()\n",
    "            # 15% of the tokens would be replaced\n",
    "            if prob <= to_replace:\n",
    "                # 10% chance change token to current token\n",
    "                if prob < to_replace*.1:\n",
    "                    output.append(token)\n",
    "                # 10% chance change token to random\n",
    "                elif prob < to_replace*.1*2:\n",
    "                    output.append(random.choice(list(self.tokenizer.word2index.values())))\n",
    "                # 10% chance change token to random\n",
    "                else:\n",
    "                    output.append(self.tokenizer.word2index[\"[MASK]\"])\n",
    "                label.append(token)\n",
    "            else:\n",
    "                output.append(token)\n",
    "                label.append(0)\n",
    "        return output, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collate (pre_process) function\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = torch.from_numpy(np.array(texts)).to(device)\n",
    "    labels = torch.from_numpy(np.array(labels)).to(device)\n",
    "    return texts, labels\n",
    "\n",
    "# Instanciate DataLoader\n",
    "bs = 32\n",
    "\n",
    "# ______________________________Traing data______________________________\n",
    "# Datasets\n",
    "tr_ev_ds = Dataset(evid_train, tokenizer)\n",
    "tr_cl_ds = Dataset(claim_train, tokenizer)\n",
    "\n",
    "# Dataloaders\n",
    "tr_ev_dl = DataLoader(tr_ev_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "tr_cl_dl = DataLoader(tr_cl_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "tr_y_dl = DataLoader(y_train, batch_size=bs)\n",
    "\n",
    "# ______________________________Test data______________________________\n",
    "# Datasets\n",
    "dv_ev_ds = Dataset(evid_dev, tokenizer)\n",
    "dv_cl_ds = Dataset(claim_dev, tokenizer)\n",
    "\n",
    "# Dataloaders\n",
    "dv_ev_dl = DataLoader(dv_ev_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "dv_cl_dl = DataLoader(dv_cl_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "dv_y_dl = DataLoader(y_dev, batch_size=bs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.1 Positional encoding to embed the data</h3></center>\n",
    "\n",
    "<center><img src=../Images/pos_encoder.png alt=\"drawing\" width=\"300\"></center>\n",
    "\n",
    "<center>Details on:</center>\n",
    "<center><a href=\"https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\"><ph>A Gentle Introduction to Positional Encoding in Transformer Models</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Positional embeding function\n",
    "class positionalEmbeding(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop = 0.2, max_len = max_len):\n",
    "        # Inputs:\n",
    "        # embedding_dim: Length of input embeding\n",
    "        # max_len: Max number of tokens in an input sentence\n",
    "        # Return: Positional Embeding Matrix\n",
    "        super(positionalEmbeding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=drop)                                                                           # Dropout layer\n",
    "        \n",
    "        # Positional embeding matrix \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)                                         # Positional increasing vector [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))      # Division term for the sin/cos functions\n",
    "        pe = torch.zeros(max_len, embedding_dim).float()                                                            # Matrix of 0's [max_len, embedding_dim]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)                                                                # 0::2 means starting with index 0, step = 2\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)                                                                # 1::2 means starting with index 1, step = 2\n",
    "        pe = pe.unsqueeze(0)                                                                                        # Resize pos encoder [1, max_len, embedding_dim]\n",
    "        self.register_buffer('pe', pe)                                                                              # Adds pos encoder to the model state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input:\n",
    "        # x: Embeding matrix [batch_size, text_length, embedding_dim]\n",
    "        x = x + self.pe.requires_grad_(False)                      # Sum the position embeding\n",
    "        return self.dropout(x)                                     # Apply dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.2 Multihead attention</h3></center>\n",
    "<center><img src=../Images/attention.png alt=\"drawing\" width=\"600\"></center>\n",
    "\n",
    "<center>Details on:</center>\n",
    "<center><a href=\"https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\"><ph>Build your own Transformer from scratch using Pytorch</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert embedding_dim % num_heads == 0, \"in_size must be divisible by num_heads\"\n",
    "\n",
    "        self.embedding_dim = embedding_dim                      # Embeding input size\n",
    "        self.num_heads = num_heads                              # Num heads of multihead attention model\n",
    "        self.head_dim = embedding_dim // num_heads              # Embedding parameters for each head\n",
    "        \n",
    "        # Instanciate weights\n",
    "        self.W_q = nn.Linear(embedding_dim, embedding_dim)      # Query weights\n",
    "        self.W_k = nn.Linear(embedding_dim, embedding_dim)      # Key weights\n",
    "        self.W_v = nn.Linear(embedding_dim, embedding_dim)      # Values weights\n",
    "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # scaled_dot_product_attention\n",
    "    def dot_prd_attn(self, Q, K, V, mask):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)   # MatMult (Q*K)\n",
    "\n",
    "        # Fill 0 mask with super small number so it wont affect the softmax weight\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)     \n",
    "\n",
    "        # softmax to put attention weight for all non-pad tokens\n",
    "        attn_probs = self.dropout(torch.softmax(attn_scores, dim=-1))                   # Softmax\n",
    "        context = torch.matmul(attn_probs, V)                                           # MatMult (Probs*V)\n",
    "        return context\n",
    "    \n",
    "    # Function to split attention heads\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, embedding_dim = x.size()\n",
    "        return x.view(batch_size, self.num_heads, seq_length, self.head_dim)\n",
    "    # Function to join attention heads\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, head_dim = x.size()\n",
    "        return x.view(batch_size, seq_length, self.embedding_dim)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # Weights linear pass (Random inicialization) + Split heads\n",
    "        Q = self.split_heads(self.W_q(x))\n",
    "        K = self.split_heads(self.W_k(x))\n",
    "        V = self.split_heads(self.W_v(x))\n",
    "        # Multihead attention\n",
    "        attn = self.dot_prd_attn(Q, K, V, mask)                 # scaled_dot_product_attention\n",
    "        attn = self.combine_heads(attn)                         # Concat heads\n",
    "        attn = self.linear(attn)                                # Linear pass\n",
    "        return attn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.3 Encoder model (Passage Ranking)</h3></center>\n",
    "<center>Source papers:</center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Attention Is All You Need</ph></a></center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Text and Code Embeddings by Contrastive Pre-Training</ph></a></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Encoder:</center>\n",
    "<center><img src=../Images/encoder.png alt=\"drawing\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class based \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                vocab_size,                            # Size of vocabulary\n",
    "                embedding_dim,                         # Embedding dimension\n",
    "                n_head,                                # Number of heads  in the multihead attention model\n",
    "                hidden_dim = 300,                      # Hiden dims for the feed forward pass\n",
    "                dropout = 0.5):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.multihead = MultiHeadAttention(embedding_dim, n_head)              # Multihead attention layer\n",
    "        self.normalization = nn.LayerNorm(embedding_dim)                        # Normalization layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(embedding_dim, 1)                               # Output layer\n",
    "\n",
    "        # Feed forward pass\n",
    "        self.feed_forward = nn.Sequential().to(device)\n",
    "        self.feed_forward.add_module('fc1', nn.Linear(embedding_dim, hidden_dim))\n",
    "        self.feed_forward.add_module('relu', nn.GELU())\n",
    "        self.feed_forward.add_module('fc2', nn.Linear(hidden_dim, embedding_dim))\n",
    "\n",
    "    def forward(self, embeding, mask):\n",
    "        attn = self.dropout(self.multihead(embeding, mask))                      # Multihead attention\n",
    "        normal = self.normalization(embeding + attn)                             # Add & Normalize pass\n",
    "        forward = self.dropout(self.feed_forward(normal))                       # Feed Forward pass\n",
    "        encoded = self.normalization(normal + forward)                          # Add & Normalize pass #2\n",
    "        return encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.4 BERT model</h3></center>\n",
    "<center><img src=../Images/BERT_enc.png alt=\"drawing\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert model\n",
    "class BERT(nn.Module):\n",
    "    # Encoder is a stack of N encoder layers. \n",
    "    def __init__(self, vocab_size, d_model, num_layers, n_head, dropout):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = num_layers\n",
    "        self.heads = n_head\n",
    "\n",
    "        # paper noted they used 4 * hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = d_model * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional and token embeddings (No sentence since it is a SBERT)\n",
    "        self.encoder = nn.Embedding(vocab_size, d_model, padding_idx=0)   # Embeding layer\n",
    "        self.pos_encoder = positionalEmbeding(d_model, dropout)           # Positional embeding\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.encoder_blocks = torch.nn.ModuleList(\n",
    "            [EncoderLayer(vocab_size = vocab_size, embedding_dim = d_model, n_head = n_head, hidden_dim = 500, dropout = 0.5)\\\n",
    "                .to(device) for _ in range(num_layers)])\n",
    "        \n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        mask = (text > 0).unsqueeze(1).repeat(1, text.size(1), 1).unsqueeze(1)  # Redim mask [batch_size, 1, 1, max_len]\n",
    "        encoder = self.encoder(text) * math.sqrt(self.d_model)                  # Text embeding imput\n",
    "        pos_enc = self.pos_encoder(encoder)                                     # Positional embeding + Text embeding\n",
    "        # running over multiple transformer blocks\n",
    "        for layer in self.encoder_blocks:\n",
    "            output = layer(pos_enc, mask)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.5 SBERT model</h3></center>\n",
    "\n",
    "<center><img src=../Images/SBERT.png alt=\"drawing\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = len(tokenizer.word2index)+1\n",
    "d_model = 300\n",
    "n_head = 1\n",
    "dropout = 0.1\n",
    "hidden_dim = 2048\n",
    "num_layers = 3\n",
    "# Instanciate model\n",
    "model = BERT(vocab_size, d_model, num_layers, n_head, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss fn\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())    # lr=2eâˆ’5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SBERT model\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model():\n",
    "    # Cosine similarity function\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    # Iterate dataloader\n",
    "    for t1, t2, y in tqdm(zip(tr_ev_dl, tr_cl_dl, tr_y_dl)):\n",
    "        # Set parameters\n",
    "        sent_a, m1 = t1\n",
    "        sent_b, m2 = t2\n",
    "        y = y.float().to(device)\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encoder layer\n",
    "        enc_a = model(sent_a, m1)\n",
    "        enc_b = model(sent_b, m2)\n",
    "\n",
    "        # Pooling layer mean\n",
    "        u = torch.mean(enc_a, 1) \n",
    "        v = torch.mean(enc_b, 1) \n",
    "\n",
    "        # Similarity metric\n",
    "        similarity = cos(u, v)\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(similarity, y)\n",
    "        acc = torch.sum((similarity>=0.5).float() == y)\n",
    "        total = y.size()[0]\n",
    "\n",
    "        # Metrics\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()             # Backpropagation\n",
    "        optimizer.step()            # Update parameters\n",
    "\n",
    "    # Print results\n",
    "    d_acc = (acc)/(total)\n",
    "    loss = train_loss/len(tr_y_dl)\n",
    "\n",
    "    tqdm.write(\n",
    "        f'Train Accuracy: {d_acc:.3f}\\\n",
    "        Train Loss: {loss:.3f}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    test_loss = 0\n",
    "    # Iterate dataloader\n",
    "    for t1, t2, y in tqdm(zip(dv_ev_dl, dv_cl_dl, dv_y_dl)):\n",
    "        # Set parameters\n",
    "        sent_a, m1 = t1\n",
    "        sent_b, m2 = t2\n",
    "        y = y.float().to(device)\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        # Encoder layer\n",
    "        enc_a = model(sent_a, m1)\n",
    "        enc_b = model(sent_b, m2)\n",
    "    \n",
    "        # Pooling layer mean\n",
    "        u = torch.mean(enc_a, 1) \n",
    "        v = torch.mean(enc_b, 1) \n",
    "    \n",
    "        # Similarity metric\n",
    "        similarity = cos(u, v)\n",
    "    \n",
    "        # Loss\n",
    "        loss = loss_fn((similarity>=0.5).float(), y)\n",
    "        acc = torch.sum((similarity>=0.5).float() == y)\n",
    "        total = y.size()[0]\n",
    "    \n",
    "        # Metrics\n",
    "        test_loss += loss.item()\n",
    "    \n",
    "    # Print results\n",
    "    d_acc = (acc)/(total)\n",
    "    loss = test_loss/len(dv_y_dl)\n",
    "    \n",
    "    tqdm.write(\n",
    "        f'Test Accuracy: {d_acc:.3f}\\\n",
    "        Test Loss: {loss:.3f}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SBERT model!\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "258it [00:21, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.800        Train Loss: 53.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:01, 17.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.818        Test Loss: 52.885\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "258it [00:24, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.800        Train Loss: 51.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:01, 16.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.909        Test Loss: 51.949\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "258it [00:24, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.900        Train Loss: 50.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:01, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.955        Test Loss: 51.999\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "258it [00:24, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.900        Train Loss: 50.290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:01, 16.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.955        Test Loss: 52.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm, tqdm_notebook # show progress bar\n",
    "\n",
    "# Epochs\n",
    "epochs = 4\n",
    "print(\"Training SBERT model!\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: %d'% (epoch))\n",
    "    train_model()\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 3 Predict on test data</h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test claims\n",
    "with open('../data/test-claims-unlabelled.json', 'r') as f:\n",
    "    df_test = pd.DataFrame(json.load(f)).transpose()\n",
    "\n",
    "# Prepare\n",
    "claim_test = [word_tokenize(i.lower()) for i in df_test['claim_text']]\n",
    "claim_test = [' '.join([w for w in seq if re.match('^[\\w\\d]+$', w)]) for seq in claim_test]\n",
    "\n",
    "# Add to dict\n",
    "tok(claim_test)\n",
    "\n",
    "# Load Dataloader\n",
    "ts_cl_ds = Dataset(claim_test, tokenizer)\n",
    "ts_cl_dl = DataLoader(ts_cl_ds, batch_size=bs, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The model is approaching wrong the dev data, it is necessary to treat the dev set as we are going to treat the test set, that is:\n",
    "a. Compare the model claims to all the evidence claims and retrieve the ones that are the most similar\n",
    "\n",
    "To do so, it is necessary to:\n",
    "1. Preprocess the evidence texts and save the processed ev.\n",
    "2. Train the model evaluating the performance over the whole dev-ev relation:\n",
    "    evidence len (1.208.827) * dev len (154) = 186.159.358 tuples to compare\n",
    "3. If the baseline trained model does not perform properly, enlarge the model including all the evidence data\n",
    "    evidence len (1.208.827) * train len (1.227) = 1.484.439.556 tuples to compare\n",
    "This would give the closest possible results without any changes on architecture parameters\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
