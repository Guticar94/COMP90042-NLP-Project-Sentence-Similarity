{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read evidence\n",
    "with open('../data/evidence.json', 'r') as f:\n",
    "    evidence = json.load(f)\n",
    "eviden = pd.DataFrame.from_dict(evidence, orient='index', columns=['evidence'])\n",
    "ev_txt = eviden['evidence'].values\n",
    "max_len = max([len(j.split()) for i,j in evidence.items()])\n",
    "\n",
    "# Read train claims\n",
    "with open('../data/train-claims.json', 'r') as f:\n",
    "    df_train = pd.DataFrame(json.load(f)).transpose()\n",
    "df_train = df_train.explode(\"evidences\")\n",
    "df_train['evidences_text'] = [evidence[item] for item in df_train['evidences']]\n",
    "df_train['label'] = 1\n",
    "df_train_ = df_train[['claim_text']].copy()\n",
    "df_train_['evidences_text'] = [random.choice(ev_txt) for i in range(df_train_.shape[0])]\n",
    "df_train_['label'] = 0\n",
    "df_train = pd.concat([df_train[['claim_text' , 'evidences_text', 'label']], df_train_]).sample(frac=1)\n",
    "\n",
    "# Read dev claims\n",
    "with open('../data/dev-claims.json', 'r') as f:\n",
    "    df_dev = pd.DataFrame(json.load(f)).transpose()\n",
    "df_dev['split'] = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set words to lower and tokenize\n",
    "tok_ev = [word_tokenize(i.lower()) for i in df_train['evidences_text']]\n",
    "tok_cl = [word_tokenize(i.lower()) for i in df_train['claim_text']]\n",
    "# Drop unknown characters (This may be modified depending model performance)\n",
    "tok_ev = [' '.join([w for w in seq if re.match('^[\\w\\d]+$', w)]) for seq in tok_ev]\n",
    "tok_cl = [' '.join([w for w in seq if re.match('^[\\w\\d]+$', w)]) for seq in tok_cl]\n",
    "# Class label\n",
    "y = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer class ( Can be improved)\n",
    "class tokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"[PAD]\": 0, \"[CLS]\": 1, \"[SEP]\": 2, \"[MASK]\": 3}\n",
    "        self.index2word = {0: \"[PAD]\", 1: \"[CLS]\", 2: \"[SEP]\", 3: \"[MASK]\"}\n",
    "        self.n_words = 4  # Count CLS and SEP\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "# Add tokens to idx dict\n",
    "tokenizer = tokenizer()\n",
    "for i, j in zip(tok_cl, tok_ev):\n",
    "    tokenizer.addSentence(i)\n",
    "    tokenizer.addSentence(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data to tensor batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.1 Bert embeding</h3></center>\n",
    "\n",
    "<center><img src=../Images/BERT_emb.png alt=\"drawing\" width=\"500\"></center>\n",
    "<center><img src=../Images/BERT_emb_example.png alt=\"drawing\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=max_len):\n",
    "        self.text = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        # Step 1: get text tokens\n",
    "        sent = [self.tokenizer.word2index[i] for i in self.text[idx].split()]\n",
    "        \n",
    "        # Step 2: replace random words in sentence with mask / random words\n",
    "        sent_mask, labels = self.masking(sent)\n",
    "\n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentence\n",
    "        # Adding PAD token for labels\n",
    "        sent = [self.tokenizer.word2index['[CLS]']] + sent_mask + [self.tokenizer.word2index['[SEP]']]\n",
    "        labels = [self.tokenizer.word2index['[PAD]']] + labels + [self.tokenizer.word2index['[PAD]']]\n",
    "\n",
    "        # Step 4: Add PAD tokens to make the sentence same length as seq_len\n",
    "        padding = [self.tokenizer.word2index['[PAD]'] for empty in range(self.seq_len - len(sent))]\n",
    "        sent.extend(padding)\n",
    "        labels.extend(padding)\n",
    "        return np.array(sent), np.array(labels)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------\n",
    "    # Function to mask/randomize tokens\n",
    "    def masking(self, tokens, to_replace = 0.15):\n",
    "        # tokens = input.split()\n",
    "        output = []\n",
    "        label = []\n",
    "        for token in tokens:\n",
    "            prob = random.random()\n",
    "            # 15% of the tokens would be replaced\n",
    "            if prob <= to_replace:\n",
    "                # 10% chance change token to current token\n",
    "                if prob < to_replace*.1:\n",
    "                    output.append(token)\n",
    "                # 10% chance change token to random\n",
    "                elif prob < to_replace*.1*2:\n",
    "                    output.append(random.choice(list(self.tokenizer.word2index.values())))\n",
    "                # 10% chance change token to random\n",
    "                else:\n",
    "                    output.append(self.tokenizer.word2index[\"[MASK]\"])\n",
    "                label.append(token)\n",
    "            else:\n",
    "                output.append(token)\n",
    "                label.append(0)\n",
    "        return output, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collate (pre_process) function\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = torch.from_numpy(np.array(texts)).to(device)\n",
    "    labels = torch.from_numpy(np.array(labels)).to(device)\n",
    "    return texts, labels\n",
    "\n",
    "# Instanciate DataLoader\n",
    "bs = 32\n",
    "\n",
    "# Datasets\n",
    "tr_ev_ds = Dataset(tok_ev, tokenizer)\n",
    "tr_cl_ds = Dataset(tok_cl, tokenizer)\n",
    "\n",
    "# Dataloaders\n",
    "tr_ev_dl = DataLoader(tr_ev_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "tr_cl_dl = DataLoader(tr_cl_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "y_dl = DataLoader(y, batch_size=bs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.1 Positional encoding to embed the data</h3></center>\n",
    "\n",
    "<center><img src=../Images/pos_encoder.png alt=\"drawing\" width=\"300\"></center>\n",
    "\n",
    "<center>Details on:</center>\n",
    "<center><a href=\"https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\"><ph>A Gentle Introduction to Positional Encoding in Transformer Models</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Positional embeding function\n",
    "class positionalEmbeding(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop = 0.2, max_len = max_len):\n",
    "        # Inputs:\n",
    "        # embedding_dim: Length of input embeding\n",
    "        # max_len: Max number of tokens in an input sentence\n",
    "        # Return: Positional Embeding Matrix\n",
    "        super(positionalEmbeding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=drop)                                                                           # Dropout layer\n",
    "        \n",
    "        # Positional embeding matrix \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)                                         # Positional increasing vector [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))      # Division term for the sin/cos functions\n",
    "        pe = torch.zeros(max_len, embedding_dim).float()                                                            # Matrix of 0's [max_len, embedding_dim]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)                                                                # 0::2 means starting with index 0, step = 2\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)                                                                # 1::2 means starting with index 1, step = 2\n",
    "        pe = pe.unsqueeze(0)                                                                                        # Resize pos encoder [1, max_len, embedding_dim]\n",
    "        self.register_buffer('pe', pe)                                                                              # Adds pos encoder to the model state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input:\n",
    "        # x: Embeding matrix [batch_size, text_length, embedding_dim]\n",
    "        x = x + self.pe.requires_grad_(False)                      # Sum the position embeding\n",
    "        return self.dropout(x)                                     # Apply dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.2 Multihead attention</h3></center>\n",
    "<center><img src=../Images/attention.png alt=\"drawing\" width=\"600\"></center>\n",
    "\n",
    "<center>Details on:</center>\n",
    "<center><a href=\"https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\"><ph>Build your own Transformer from scratch using Pytorch</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert embedding_dim % num_heads == 0, \"in_size must be divisible by num_heads\"\n",
    "\n",
    "        self.embedding_dim = embedding_dim                      # Embeding input size\n",
    "        self.num_heads = num_heads                              # Num heads of multihead attention model\n",
    "        self.head_dim = embedding_dim // num_heads              # Embedding parameters for each head\n",
    "        \n",
    "        # Instanciate weights\n",
    "        self.W_q = nn.Linear(embedding_dim, embedding_dim)      # Query weights\n",
    "        self.W_k = nn.Linear(embedding_dim, embedding_dim)      # Key weights\n",
    "        self.W_v = nn.Linear(embedding_dim, embedding_dim)      # Values weights\n",
    "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # scaled_dot_product_attention\n",
    "    def dot_prd_attn(self, Q, K, V, mask):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)   # MatMult (Q*K)\n",
    "\n",
    "        # Fill 0 mask with super small number so it wont affect the softmax weight\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)     \n",
    "\n",
    "        # softmax to put attention weight for all non-pad tokens\n",
    "        attn_probs = self.dropout(torch.softmax(attn_scores, dim=-1))                   # Softmax\n",
    "        context = torch.matmul(attn_probs, V)                                           # MatMult (Probs*V)\n",
    "        return context\n",
    "    \n",
    "    # Function to split attention heads\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, embedding_dim = x.size()\n",
    "        return x.view(batch_size, self.num_heads, seq_length, self.head_dim)\n",
    "    # Function to join attention heads\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, head_dim = x.size()\n",
    "        return x.view(batch_size, seq_length, self.embedding_dim)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # Weights linear pass (Random inicialization) + Split heads\n",
    "        Q = self.split_heads(self.W_q(x))\n",
    "        K = self.split_heads(self.W_k(x))\n",
    "        V = self.split_heads(self.W_v(x))\n",
    "        # Multihead attention\n",
    "        attn = self.dot_prd_attn(Q, K, V, mask)                 # scaled_dot_product_attention\n",
    "        attn = self.combine_heads(attn)                         # Concat heads\n",
    "        attn = self.linear(attn)                                # Linear pass\n",
    "        return attn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.3 Encoder model (Passage Ranking)</h3></center>\n",
    "<center>Source papers:</center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Attention Is All You Need</ph></a></center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Text and Code Embeddings by Contrastive Pre-Training</ph></a></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Encoder:</center>\n",
    "<center><img src=../Images/encoder.png alt=\"drawing\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class based \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                vocab_size,                            # Size of vocabulary\n",
    "                embedding_dim,                         # Embedding dimension\n",
    "                n_head,                                # Number of heads  in the multihead attention model\n",
    "                hidden_dim = 300,                      # Hiden dims for the feed forward pass\n",
    "                dropout = 0.5):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.multihead = MultiHeadAttention(embedding_dim, n_head)              # Multihead attention layer\n",
    "        self.normalization = nn.LayerNorm(embedding_dim)                        # Normalization layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(embedding_dim, 1)                               # Output layer\n",
    "\n",
    "        # Feed forward pass\n",
    "        self.feed_forward = nn.Sequential().to(device)\n",
    "        self.feed_forward.add_module('fc1', nn.Linear(embedding_dim, hidden_dim))\n",
    "        self.feed_forward.add_module('relu', nn.GELU())\n",
    "        self.feed_forward.add_module('fc2', nn.Linear(hidden_dim, embedding_dim))\n",
    "\n",
    "    def forward(self, embeding, mask):\n",
    "        attn = self.dropout(self.multihead(embeding, mask))                      # Multihead attention\n",
    "        normal = self.normalization(embeding + attn)                             # Add & Normalize pass\n",
    "        forward = self.dropout(self.feed_forward(normal))                       # Feed Forward pass\n",
    "        encoded = self.normalization(normal + forward)                          # Add & Normalize pass #2\n",
    "        return encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.4 BERT model</h3></center>\n",
    "<center><img src=../Images/BERT_enc.png alt=\"drawing\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert model\n",
    "class BERT(nn.Module):\n",
    "    # Encoder is a stack of N encoder layers. \n",
    "    def __init__(self, vocab_size, d_model, num_layers, n_head, dropout):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = num_layers\n",
    "        self.heads = n_head\n",
    "\n",
    "        # paper noted they used 4 * hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = d_model * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional and token embeddings (No sentence since it is a SBERT)\n",
    "        self.encoder = nn.Embedding(vocab_size, d_model, padding_idx=0)   # Embeding layer\n",
    "        self.pos_encoder = positionalEmbeding(d_model, dropout)           # Positional embeding\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.encoder_blocks = torch.nn.ModuleList(\n",
    "            [EncoderLayer(vocab_size = vocab_size, embedding_dim = d_model, n_head = n_head, hidden_dim = 500, dropout = 0.5)\\\n",
    "                .to(device) for _ in range(num_layers)])\n",
    "        \n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        mask = (text > 0).unsqueeze(1).repeat(1, text.size(1), 1).unsqueeze(1)  # Redim mask [batch_size, 1, 1, max_len]\n",
    "        encoder = self.encoder(text) * math.sqrt(self.d_model)                  # Text embeding imput\n",
    "        pos_enc = self.pos_encoder(encoder)                                     # Positional embeding + Text embeding\n",
    "        # running over multiple transformer blocks\n",
    "        for layer in self.encoder_blocks:\n",
    "            output = layer(pos_enc, mask)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.4 SBERT model</h3></center>\n",
    "\n",
    "<center><img src=../Images/SBERT.png alt=\"drawing\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = len(tokenizer.word2index)+1\n",
    "d_model = 300\n",
    "n_head = 1\n",
    "dropout = 0.1\n",
    "hidden_dim = 2048\n",
    "num_layers = 3\n",
    "# Instanciate model\n",
    "model = BERT(vocab_size, d_model, num_layers, n_head, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss fn\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())    # lr=2e−5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "258it [05:36,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain_Acc: 0.750    Loss: 53.429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train SBERT model\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model():\n",
    "    # Cosine similarity function\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    # Iterate dataloader\n",
    "    for t1, t2, y in tqdm(zip(tr_ev_dl, tr_cl_dl, y_dl)):\n",
    "        # Set parameters\n",
    "        sent_a, m1 = t1\n",
    "        sent_b, m2 = t2\n",
    "        y = y.float()\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encoder layer\n",
    "        enc_a = model(sent_a, m1)\n",
    "        enc_b = model(sent_b, m2)\n",
    "\n",
    "        # Pooling layer mean\n",
    "        u = torch.mean(enc_a, 1) \n",
    "        v = torch.mean(enc_b, 1) \n",
    "\n",
    "        # Similarity metric\n",
    "        similarity = cos(u, v)\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(similarity, y)\n",
    "        acc = torch.sum((similarity>=0.5).float() == y)\n",
    "        total = y.size()[0]\n",
    "\n",
    "        # Metrics\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()             # Backpropagation\n",
    "        optimizer.step()            # Update parameters\n",
    "\n",
    "    # Print results\n",
    "    d_acc = (acc)/(total)\n",
    "    loss = train_loss/len(y_dl)\n",
    "\n",
    "    tqdm.write(\n",
    "        f'Domain_Acc: {d_acc:.3f}\\\n",
    "        Loss: {loss:.3f}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 479, 300])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/129 [00:02<01:24,  1.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m txt, mask \u001b[39min\u001b[39;00m tqdm(tr_ev_dl):\n\u001b[0;32m----> 3\u001b[0m     a \u001b[39m=\u001b[39m bert(txt, mask)\n",
      "File \u001b[0;32m~/Documents/Semester_3/Natural Language Processing/COMP90042_NLP_Project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Semester_3/Natural Language Processing/COMP90042_NLP_Project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[106], line 29\u001b[0m, in \u001b[0;36mBERT.forward\u001b[0;34m(self, text, mask)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m# running over multiple transformer blocks\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_blocks:\n\u001b[0;32m---> 29\u001b[0m     output \u001b[39m=\u001b[39m layer(pos_enc, mask)\n\u001b[1;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Documents/Semester_3/Natural Language Processing/COMP90042_NLP_Project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Semester_3/Natural Language Processing/COMP90042_NLP_Project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[97], line 27\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, embeding, mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultihead(embeding, mask))                      \u001b[39m# Multihead attention\u001b[39;00m\n\u001b[1;32m     26\u001b[0m normal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalization(embeding \u001b[39m+\u001b[39m attn)                             \u001b[39m# Add & Normalize pass\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m forward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward(normal))                       \u001b[39m# Feed Forward pass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalization(normal \u001b[39m+\u001b[39m forward)                          \u001b[39m# Add & Normalize pass #2\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m encoded\n",
      "File \u001b[0;32m~/Documents/Semester_3/Natural Language Processing/COMP90042_NLP_Project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Semester_3/Natural Language Processing/COMP90042_NLP_Project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Semester_3/Natural Language Processing/COMP90042_NLP_Project/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Semester_3/Natural Language Processing/COMP90042_NLP_Project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Semester_3/Natural Language Processing/COMP90042_NLP_Project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Semester_3/Natural Language Processing/COMP90042_NLP_Project/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dv_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m norm\n\u001b[1;32m      3\u001b[0m cos_sim \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m y \u001b[39m=\u001b[39m encoder_layer(\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dv_dl))[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m x, _ \u001b[39min\u001b[39;00m tqdm(tr_dl):\n\u001b[1;32m      6\u001b[0m     enc \u001b[39m=\u001b[39m encoder_layer(x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dv_dl' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from numpy.linalg import norm\n",
    "cos_sim = []\n",
    "y = encoder_layer(next(iter(dv_dl))[0].unsqueeze(1)).reshape(-1).detach().numpy()\n",
    "for x, _ in tqdm(tr_dl):\n",
    "    enc = encoder_layer(x)\n",
    "    for line in enc:\n",
    "        X = line.reshape(-1).detach().numpy()\n",
    "        cos_sim.append(np.dot(X[:len(y)], y[:len(X)])/(norm(X)*norm(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cos_sim, columns=['similarity'])\n",
    "df['Evidence'] = ev[:15000]\n",
    "df.sort_values('similarity', ascending=False, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['claim_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index == 12171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(dv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['evidences'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(list(sequence.word2index.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# def reject(df, eviden):\n",
    "#     reject = []\n",
    "#     for row in tqdm(df.index):\n",
    "#         ev = df.loc[row, 'evidences']\n",
    "#         rej = []\n",
    "#         samp = True\n",
    "#         while samp:\n",
    "#             sp = eviden.sample(len(ev))['evidence'].values\n",
    "#             for val in sp:\n",
    "#                 if val not in ev:\n",
    "#                     rej.append(sp)\n",
    "#                     samp = False\n",
    "#         reject.append(rej[0])\n",
    "#     return reject\n",
    "\n",
    "# df_train_ref = df_train[['claim_text','claim_label']].copy()\n",
    "# df_train_ref['evidences'] = reject(df_train, eviden)\n",
    "# df_train_ref['split'] = 'train'\n",
    "# df_train_ref['label'] = 'disengagement'\n",
    "\n",
    "# df_train['evidences'] = [[evidence[ev] for ev in val] for val in df_train['evidences']]\n",
    "\n",
    "# df = pd.concat([df_train, df_train_ref])[['claim_text', 'evidences', 'label', 'split']]\n",
    "# df = df.explode('evidences')\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
