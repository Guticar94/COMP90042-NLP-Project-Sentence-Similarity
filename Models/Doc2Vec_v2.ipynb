{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%pip install gensim nltk pandas sklearn torch rank_bm25\n",
        "\n",
        "Code based on https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "import pickle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "import collections\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Garbage Collector to fix memory issues\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read train claims\n",
        "with open('../data/train-claims.json', 'r') as f:\n",
        "    claims = json.load(f)\n",
        "\n",
        "# Read dev claims\n",
        "with open('../data/dev-claims.json', 'r') as f:\n",
        "    dev_claims = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lowercasing the 'claim_text' field for each claim\n",
        "for claim_id, claim_info in claims.items():\n",
        "    claim_info['claim_text'] = claim_info['claim_text'].lower()\n",
        "\n",
        "for claim_id, claim_info in dev_claims.items():\n",
        "    claim_info['claim_text'] = claim_info['claim_text'].lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read evidence\n",
        "with open('../data/evidence.json', 'r') as f:\n",
        "    evidences = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "evidences = {i: str.lower(j) for i,j in evidences.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of claims for training = 1228\n",
            "Number of claims for development = 154\n",
            "Number of evidences = 1208827\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of claims for training = {0}\".format(len(claims)))\n",
        "print(\"Number of claims for development = {0}\".format(len(dev_claims)))\n",
        "print(\"Number of evidences = {0}\".format(len(evidences)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Second approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from claims\n",
        "corpus = {}\n",
        "\n",
        "for id, claim in claims.items():\n",
        "    text2 = claim['claim_text']\n",
        "    # Create pairs claim + evidence\n",
        "    for evidence in claim['evidences']:\n",
        "        text = claim['claim_text'] + \" \" + evidences[evidence]\n",
        "        corpus[id + ' - ' + evidence] = (str.strip(text),id)\n",
        "        text2 = text2 + \" \" + evidences[evidence]\n",
        "    # Create pairs claim + all_evidence\n",
        "    corpus[id] = (str.strip(text2),claim['claim_label'])\n",
        "\n",
        "for id, evidence in evidences.items():\n",
        "    corpus[id] = (str.strip(evidence),id) # Add evidence text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### First approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model in claims and evidences\n",
        "# Collect all texts from claims\n",
        "#corpus = {}\n",
        "#for id, claim in claims.items():\n",
        "#    corpus[id] = str.strip(claim['claim_text'])  # Add claim text\n",
        "#for id, evidence in evidences.items():\n",
        "#    corpus[id] = str.strip(evidence) # Add evidence text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \n",
        "def tokenize_text(df, column):\n",
        "    df['tokens'] = df[column].apply(lambda x: [token for token in word_tokenize(x) if token.isalnum()])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row(row, index):\n",
        "    return TaggedDocument(row['tokens'], tags=[row[index]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1214177 entries, claim-1937 - evidence-442946 to evidence-1208826\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count    Dtype \n",
            "---  ------  --------------    ----- \n",
            " 0   text    1214177 non-null  object\n",
            " 1   label   1214177 non-null  object\n",
            " 2   tokens  1214177 non-null  object\n",
            " 3   tagged  1214177 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 46.3+ MB\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tagged</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937 - evidence-442946</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>claim-1937</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>([not, only, is, there, no, scientific, eviden...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1937 - evidence-1194317</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>claim-1937</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>([not, only, is, there, no, scientific, eviden...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1937 - evidence-12171</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>claim-1937</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>([not, only, is, there, no, scientific, eviden...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>([not, only, is, there, no, scientific, eviden...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126 - evidence-338219</th>\n",
              "      <td>el niño drove record highs in global temperatu...</td>\n",
              "      <td>claim-126</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "      <td>([el, niño, drove, record, highs, in, global, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                            text  \\\n",
              "claim-1937 - evidence-442946   not only is there no scientific evidence that ...   \n",
              "claim-1937 - evidence-1194317  not only is there no scientific evidence that ...   \n",
              "claim-1937 - evidence-12171    not only is there no scientific evidence that ...   \n",
              "claim-1937                     not only is there no scientific evidence that ...   \n",
              "claim-126 - evidence-338219    el niño drove record highs in global temperatu...   \n",
              "\n",
              "                                    label  \\\n",
              "claim-1937 - evidence-442946   claim-1937   \n",
              "claim-1937 - evidence-1194317  claim-1937   \n",
              "claim-1937 - evidence-12171    claim-1937   \n",
              "claim-1937                       DISPUTED   \n",
              "claim-126 - evidence-338219     claim-126   \n",
              "\n",
              "                                                                          tokens  \\\n",
              "claim-1937 - evidence-442946   [not, only, is, there, no, scientific, evidenc...   \n",
              "claim-1937 - evidence-1194317  [not, only, is, there, no, scientific, evidenc...   \n",
              "claim-1937 - evidence-12171    [not, only, is, there, no, scientific, evidenc...   \n",
              "claim-1937                     [not, only, is, there, no, scientific, evidenc...   \n",
              "claim-126 - evidence-338219    [el, niño, drove, record, highs, in, global, t...   \n",
              "\n",
              "                                                                          tagged  \n",
              "claim-1937 - evidence-442946   ([not, only, is, there, no, scientific, eviden...  \n",
              "claim-1937 - evidence-1194317  ([not, only, is, there, no, scientific, eviden...  \n",
              "claim-1937 - evidence-12171    ([not, only, is, there, no, scientific, eviden...  \n",
              "claim-1937                     ([not, only, is, there, no, scientific, eviden...  \n",
              "claim-126 - evidence-338219    ([el, niño, drove, record, highs, in, global, ...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the list of documents into a pandas DataFrame\n",
        "df = pd.DataFrame.from_dict(corpus, orient='index', columns=['text','label'])\n",
        "df = tokenize_text(df,'text')\n",
        "df['tagged'] = df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "df.info()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([TaggedDocument(words=['not', 'only', 'is', 'there', 'no', 'scientific', 'evidence', 'that', 'co2', 'is', 'a', 'pollutant', 'higher', 'co2', 'concentrations', 'actually', 'help', 'ecosystems', 'support', 'more', 'plant', 'and', 'animal', 'life', 'at', 'very', 'high', 'concentrations', '100', 'times', 'atmospheric', 'concentration', 'or', 'greater', 'carbon', 'dioxide', 'can', 'be', 'toxic', 'to', 'animal', 'life', 'so', 'raising', 'the', 'concentration', 'to', 'ppm', '1', 'or', 'higher', 'for', 'several', 'hours', 'will', 'eliminate', 'pests', 'such', 'as', 'whiteflies', 'and', 'spider', 'mites', 'in', 'a', 'greenhouse'], tags=['claim-1937']),\n",
              "       TaggedDocument(words=['not', 'only', 'is', 'there', 'no', 'scientific', 'evidence', 'that', 'co2', 'is', 'a', 'pollutant', 'higher', 'co2', 'concentrations', 'actually', 'help', 'ecosystems', 'support', 'more', 'plant', 'and', 'animal', 'life', 'plants', 'can', 'grow', 'as', 'much', 'as', '50', 'percent', 'faster', 'in', 'concentrations', 'of', 'ppm', 'co', '2', 'when', 'compared', 'with', 'ambient', 'conditions', 'though', 'this', 'assumes', 'no', 'change', 'in', 'climate', 'and', 'no', 'limitation', 'on', 'other', 'nutrients'], tags=['claim-1937']),\n",
              "       TaggedDocument(words=['not', 'only', 'is', 'there', 'no', 'scientific', 'evidence', 'that', 'co2', 'is', 'a', 'pollutant', 'higher', 'co2', 'concentrations', 'actually', 'help', 'ecosystems', 'support', 'more', 'plant', 'and', 'animal', 'life', 'higher', 'carbon', 'dioxide', 'concentrations', 'will', 'favourably', 'affect', 'plant', 'growth', 'and', 'demand', 'for', 'water'], tags=['claim-1937']),\n",
              "       ...,\n",
              "       TaggedDocument(words=['dragon', 'storm', 'game', 'a', 'game', 'and', 'collectible', 'card', 'game'], tags=['evidence-1208824']),\n",
              "       TaggedDocument(words=['it', 'states', 'that', 'the', 'zeriuani', 'which', 'is', 'so', 'great', 'a', 'realm', 'that', 'from', 'it', 'as', 'their', 'tradition', 'relates', 'all', 'the', 'tribes', 'of', 'the', 'slavs', 'are', 'sprung', 'and', 'trace', 'their', 'origin', 'zeriuani', 'tantum', 'est', 'reguum', 'utex', 'eo', 'cunctae', 'gentes', 'sclavorum', 'exortae', 'sint', 'et', 'originem', 'sicut', 'affirmant', 'ducant'], tags=['evidence-1208825']),\n",
              "       TaggedDocument(words=['the', 'storyline', 'revolves', 'around', 'a', 'giant', 'plesiosaur', 'akin', 'to', 'the', 'loch', 'ness', 'monster', 'which', 'appears', 'in', 'crater', 'lake', 'in', 'northern', 'california', 'near', 'susanville', 'not', 'to', 'be', 'confused', 'with', 'the', 'much', 'more', 'famous', 'crater', 'lake', 'in', 'oregon'], tags=['evidence-1208826'])],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_corpus = df.tagged.values\n",
        "del df\n",
        "gc.collect()\n",
        "train_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from dev claims\n",
        "dv_corpus = {}\n",
        "\n",
        "for id, claim in dev_claims.items():\n",
        "    text2 = claim['claim_text']\n",
        "    # Create pairs claim + evidence\n",
        "    for evidence in claim['evidences']:\n",
        "        text = claim['claim_text'] + \" \" + evidences[evidence]\n",
        "        dv_corpus[id + ' - ' + evidence] = (str.strip(text),id)\n",
        "        text2 = text2 + \" \" + evidences[evidence]\n",
        "    # Create pairs claim + all_evidence\n",
        "    dv_corpus[id] = (str.strip(text2),claim['claim_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 645 entries, claim-752 - evidence-67732 to claim-1021\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    645 non-null    object\n",
            " 1   label   645 non-null    object\n",
            " 2   tokens  645 non-null    object\n",
            " 3   tagged  645 non-null    object\n",
            "dtypes: object(4)\n",
            "memory usage: 25.2+ KB\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tagged</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-752 - evidence-67732</th>\n",
              "      <td>[south australia] has the most expensive elect...</td>\n",
              "      <td>claim-752</td>\n",
              "      <td>[south, australia, has, the, most, expensive, ...</td>\n",
              "      <td>([south, australia, has, the, most, expensive,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-752 - evidence-572512</th>\n",
              "      <td>[south australia] has the most expensive elect...</td>\n",
              "      <td>claim-752</td>\n",
              "      <td>[south, australia, has, the, most, expensive, ...</td>\n",
              "      <td>([south, australia, has, the, most, expensive,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-752</th>\n",
              "      <td>[south australia] has the most expensive elect...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[south, australia, has, the, most, expensive, ...</td>\n",
              "      <td>([south, australia, has, the, most, expensive,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-375 - evidence-996421</th>\n",
              "      <td>when 3 per cent of total annual global emissio...</td>\n",
              "      <td>claim-375</td>\n",
              "      <td>[when, 3, per, cent, of, total, annual, global...</td>\n",
              "      <td>([when, 3, per, cent, of, total, annual, globa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-375 - evidence-1080858</th>\n",
              "      <td>when 3 per cent of total annual global emissio...</td>\n",
              "      <td>claim-375</td>\n",
              "      <td>[when, 3, per, cent, of, total, annual, global...</td>\n",
              "      <td>([when, 3, per, cent, of, total, annual, globa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                           text  \\\n",
              "claim-752 - evidence-67732    [south australia] has the most expensive elect...   \n",
              "claim-752 - evidence-572512   [south australia] has the most expensive elect...   \n",
              "claim-752                     [south australia] has the most expensive elect...   \n",
              "claim-375 - evidence-996421   when 3 per cent of total annual global emissio...   \n",
              "claim-375 - evidence-1080858  when 3 per cent of total annual global emissio...   \n",
              "\n",
              "                                  label  \\\n",
              "claim-752 - evidence-67732    claim-752   \n",
              "claim-752 - evidence-572512   claim-752   \n",
              "claim-752                      SUPPORTS   \n",
              "claim-375 - evidence-996421   claim-375   \n",
              "claim-375 - evidence-1080858  claim-375   \n",
              "\n",
              "                                                                         tokens  \\\n",
              "claim-752 - evidence-67732    [south, australia, has, the, most, expensive, ...   \n",
              "claim-752 - evidence-572512   [south, australia, has, the, most, expensive, ...   \n",
              "claim-752                     [south, australia, has, the, most, expensive, ...   \n",
              "claim-375 - evidence-996421   [when, 3, per, cent, of, total, annual, global...   \n",
              "claim-375 - evidence-1080858  [when, 3, per, cent, of, total, annual, global...   \n",
              "\n",
              "                                                                         tagged  \n",
              "claim-752 - evidence-67732    ([south, australia, has, the, most, expensive,...  \n",
              "claim-752 - evidence-572512   ([south, australia, has, the, most, expensive,...  \n",
              "claim-752                     ([south, australia, has, the, most, expensive,...  \n",
              "claim-375 - evidence-996421   ([when, 3, per, cent, of, total, annual, globa...  \n",
              "claim-375 - evidence-1080858  ([when, 3, per, cent, of, total, annual, globa...  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Collect all texts from dev claims\n",
        "dev_df = pd.DataFrame.from_dict(dv_corpus, orient='index', columns=['text','label'])\n",
        "dev_df = tokenize_text(dev_df,'text')\n",
        "dev_df['tagged'] = dev_df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "dev_df.info()\n",
        "dev_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_corpus = dev_df.tagged.values\n",
        "dev_corpus[0:5]\n",
        "del dev_df\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model\n",
        "https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "cores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Doc2Vec model\n",
        "model = Doc2Vec(dm=1, vector_size=100, window=5, min_count=1, workers=cores, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.build_vocab(train_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "class EpochLogger(CallbackAny2Vec):\n",
        "    '''Callback to log information about training'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "        self.last_signal = datetime.now()\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        t = datetime.now() - self.last_signal\n",
        "        print(\"Epoch #{} - Duration: {}\".format(self.epoch, t))\n",
        "        self.epoch += 1\n",
        "        self.last_signal = datetime.now()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #0 - Duration: 0:01:25.614273\n",
            "Epoch #1 - Duration: 0:01:25.618853\n",
            "Epoch #2 - Duration: 0:01:27.527903\n",
            "Epoch #3 - Duration: 0:01:26.822355\n",
            "Epoch #4 - Duration: 0:01:26.571568\n",
            "Epoch #5 - Duration: 0:01:27.340967\n",
            "Epoch #6 - Duration: 0:01:27.417400\n",
            "Epoch #7 - Duration: 0:01:26.667165\n",
            "Epoch #8 - Duration: 0:01:27.247633\n",
            "Epoch #9 - Duration: 0:01:27.723882\n",
            "Epoch #10 - Duration: 0:01:28.603977\n",
            "Epoch #11 - Duration: 0:01:27.043769\n",
            "Epoch #12 - Duration: 0:01:27.309179\n",
            "Epoch #13 - Duration: 0:01:28.594344\n",
            "Epoch #14 - Duration: 0:01:31.987251\n",
            "Epoch #15 - Duration: 0:01:27.626736\n",
            "Epoch #16 - Duration: 0:01:27.835790\n",
            "Epoch #17 - Duration: 0:01:27.636705\n",
            "Epoch #18 - Duration: 0:01:27.210743\n",
            "Epoch #19 - Duration: 0:01:27.216718\n",
            "Epoch #20 - Duration: 0:01:26.876791\n",
            "Epoch #21 - Duration: 0:01:29.553064\n",
            "Epoch #22 - Duration: 0:01:36.572004\n",
            "Epoch #23 - Duration: 0:01:36.051439\n",
            "Epoch #24 - Duration: 0:01:33.348528\n",
            "Epoch #25 - Duration: 0:01:31.431308\n",
            "Epoch #26 - Duration: 0:01:29.021999\n",
            "Epoch #27 - Duration: 0:01:27.863234\n",
            "Epoch #28 - Duration: 0:01:28.429784\n",
            "Epoch #29 - Duration: 0:01:27.800112\n",
            "Epoch #30 - Duration: 0:01:28.470556\n",
            "Epoch #31 - Duration: 0:01:27.419515\n",
            "Epoch #32 - Duration: 0:01:28.252957\n",
            "Epoch #33 - Duration: 0:01:27.666289\n",
            "Epoch #34 - Duration: 0:01:28.084750\n",
            "Epoch #35 - Duration: 0:01:27.734218\n",
            "Epoch #36 - Duration: 0:01:27.954901\n",
            "Epoch #37 - Duration: 0:01:27.108070\n",
            "Epoch #38 - Duration: 0:01:27.757975\n",
            "Epoch #39 - Duration: 0:01:26.819092\n",
            "Epoch #40 - Duration: 0:01:28.397021\n",
            "Epoch #41 - Duration: 0:01:27.448014\n",
            "Epoch #42 - Duration: 0:01:27.656501\n",
            "Epoch #43 - Duration: 0:01:27.551700\n",
            "Epoch #44 - Duration: 0:01:27.940013\n"
          ]
        }
      ],
      "source": [
        "epoch_logger = EpochLogger()\n",
        "model.train(train_corpus, total_examples=model.corpus_count,  epochs=model.epochs, callbacks=[epoch_logger])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(\"Doc2Vec.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assesing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evidences_df = pd.DataFrame.from_dict(evidences, orient='index', columns=['text'])\n",
        "evidences_df = tokenize_text(evidences_df,'text')\n",
        "evidences_df['inferred'] = evidences_df['tokens'].apply(lambda x: model.infer_vector(x))\n",
        "evidences_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evidences_df.to_pickle('evidences_df.pkl')\n",
        "#evidences_df = pd.read_pickle('evidences_df.pkl')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "claims_df = pd.DataFrame.from_dict(claims, orient='index')\n",
        "claims_df = tokenize_text(claims_df,'claim_text')\n",
        "claims_df['inferred'] = claims_df['tokens'].apply(lambda x: model.infer_vector(x))\n",
        "claims_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "claims_df.to_pickle('claims_df.pkl')\n",
        "#claims_df = pd.read_pickle('claims_df.pkl')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implement BM25 Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize BM25 model\n",
        "bm25 = BM25Okapi(evidences_df['tokens'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate BM25 scores for each claim\n",
        "def calculate_bm25_scores(query_tokens):\n",
        "    return bm25.get_scores(query_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating the BM25 scores\")\n",
        "# Compute BM25 scores\n",
        "bm25_scores = claims_df['tokens'].apply(calculate_bm25_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_scores.to_pickle('bm25_scores.pkl')\n",
        "#bm25_scores = pd.read_pickle('bm25_scores.pkl')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract lists\n",
        "claim_vectors = claims_df['inferred'].to_list()\n",
        "evidence_vectors = evidences_df['inferred'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating the similarities\")\n",
        "# Calculate Doc2Vec similarities\n",
        "doc2vec_similarities = cosine_similarity(claim_vectors, evidence_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('doc2vec_similarities.pkl','wb') as f: pickle.dump(doc2vec_similarities, f)\n",
        "#with open('doc2vec_similarities.pkl','rb') as f: doc2vec_similarities = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize the lengths\n",
        "def normalize(scores):\n",
        "    return (scores - np.min(scores)) / (np.max(scores) - np.min(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "document_lengths = [len(doc) for doc in evidences_df['tokens']]\n",
        "normalized_lengths = normalize(document_lengths)\n",
        "p_normalized_lengths = np.array(0.2 * normalized_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_bm25_scores = [normalize(doc) for doc in bm25_scores]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_doc2vec_similarities = normalize(doc2vec_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_bm25_scores = np.array([0.4 * doc for doc in n_bm25_scores])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_doc2vec_similarities = np.array([0.4 * doc for doc in n_doc2vec_similarities])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del document_lengths, normalized_lengths\n",
        "del bm25_scores, n_bm25_scores\n",
        "del doc2vec_similarities, n_doc2vec_similarities\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_combined_scores(bm25,similarities,lengths):\n",
        "    # Initialize an array to store the sum results\n",
        "    scores = []\n",
        "\n",
        "    # Perform the element-wise addition\n",
        "    for i in range(len(similarities)):\n",
        "        sum_result = bm25[i] + similarities[i] + lengths\n",
        "        scores.append(sum_result)\n",
        "    \n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dev claims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize an array to store the sum results\n",
        "combined_scores = get_combined_scores(p_bm25_scores, p_doc2vec_similarities, p_normalized_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del p_bm25_scores, p_doc2vec_similarities\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rank evidences\n",
        "ranked_index = np.argsort(combined_scores, axis=1)[:, ::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('ranked_index.pkl','wb') as f: pickle.dump(ranked_index, f)\n",
        "#with open('ranked_index.pkl','rb') as f: ranked_index = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dev_df = pd.DataFrame.from_dict(dev_claims, orient='index')\n",
        "dev_df = tokenize_text(dev_df,'claim_text')\n",
        "dev_df['inferred'] = dev_df['tokens'].apply(lambda x: model.infer_vector(x))\n",
        "dev_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating the dev BM25 scores\")\n",
        "# Compute dev BM25 scores\n",
        "dev_bm25_scores = dev_df['tokens'].apply(calculate_bm25_scores)\n",
        "n_dev_bm25_scores = [normalize(doc) for doc in dev_bm25_scores]\n",
        "p_dev_bm25_scores = np.array([0.4 * doc for doc in n_dev_bm25_scores])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dev_bm25_scores.to_pickle('dev_bm25_scores.pkl')\n",
        "#dev_bm25_scores = pd.read_pickle('dev_bm25_scores.pkl')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract lists\n",
        "claim_dev_vectors = dev_df['inferred'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating the dev similarities\")\n",
        "# Calculate Doc2Vec similarities\n",
        "dev_doc2vec_similarities = cosine_similarity(claim_dev_vectors, evidence_vectors)\n",
        "n_dev_doc2vec_similarities = normalize(dev_doc2vec_similarities)\n",
        "p_dev_doc2vec_similarities = np.array([0.4 * doc for doc in n_dev_doc2vec_similarities])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('dev_doc2vec_similarities.pkl','wb') as f: pickle.dump(dev_doc2vec_similarities, f)\n",
        "#with open('dev_doc2vec_similarities.pkl','rb') as f: dev_doc2vec_similarities = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize an array to store the sum results\n",
        "dev_combined_scores = get_combined_scores(p_dev_bm25_scores, p_dev_doc2vec_similarities, p_normalized_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del p_dev_bm25_scores, p_dev_doc2vec_similarities\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rank evidences\n",
        "dev_ranked_index = np.argsort(dev_combined_scores, axis=1)[:, ::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the list of documents into a pandas DataFrame\n",
        "df = pd.DataFrame.from_dict(corpus, orient='index', columns=['text','label'])\n",
        "df = tokenize_text(df,'text')\n",
        "df['tagged'] = df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "df = df[~df['label'].str.startswith('claim')]\n",
        "\n",
        "# Filter rows where the label does not start with 'claim'\n",
        "train_corpus = df.tagged.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from dev claims\n",
        "dev_df = pd.DataFrame.from_dict(dv_corpus, orient='index', columns=['text','label'])\n",
        "dev_df = tokenize_text(dev_df,'text')\n",
        "dev_df['tagged'] = dev_df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "dev_df = dev_df[~dev_df['label'].str.startswith('claim')]\n",
        "\n",
        "# Filter rows where the label does not start with 'claim'\n",
        "dev_corpus = dev_df.tagged.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label Distribution\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16,6))\n",
        "\n",
        "axs[0].set_title(\"Train\")\n",
        "axs[1].set_title(\"Validation\")\n",
        "tlabel = axs[0].hist(sorted([l for l in df['label']]))\n",
        "vlabel = axs[1].hist(sorted([l for l in dev_df['label']]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vec_for_learning(model, sents):\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
        "    return targets, regressors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg = LogisticRegression(n_jobs=cores, C=1e5, max_iter=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train, X_train = vec_for_learning(model, train_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_dev, X_dev = vec_for_learning(model, dev_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = logreg.predict(X_dev)\n",
        "print('Testing accuracy %s' % accuracy_score(y_dev, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_dev, y_pred, average='weighted')))\n",
        "report = classification_report(y_dev, y_pred)\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_pred = logreg.predict(X_train)\n",
        "print('Testing accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_train, y_train_pred, average='weighted')))\n",
        "report = classification_report(y_train, y_train_pred)\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = {}\n",
        "for i in range(len(claims_df)):\n",
        "    ev_list = ['evidence-'+ str(num) for num in ranked_index[i][:5] ]\n",
        "    predictions[claims_df.index[i]] = {}\n",
        "    predictions[claims_df.index[i]][\"claim_text\"] = claims_df.claim_text[i]\n",
        "    predictions[claims_df.index[i]][\"claim_label\"] = y_train_pred[i]\n",
        "    predictions[claims_df.index[i]][\"evidences\"] = ev_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the DataFrame to a JSON file\n",
        "train_df_doc2vec = pd.DataFrame.from_dict(predictions, orient='index') \n",
        "train_df_doc2vec.to_json('../data/train_claims_doc2vec.json', orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dev_predictions = {}\n",
        "for i in range(len(dev_df)):\n",
        "    ev_list = ['evidence-'+ str(num) for num in ranked_index[i][:5] ]\n",
        "    dev_predictions[dev_df.index[i]] = {}\n",
        "    dev_predictions[dev_df.index[i]][\"claim_text\"] = dev_df.text[i]\n",
        "    dev_predictions[dev_df.index[i]][\"claim_label\"] = y_pred[i]\n",
        "    dev_predictions[dev_df.index[i]][\"evidences\"] = ev_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the DataFrame to a JSON file\n",
        "dev_df_doc2vec = pd.DataFrame.from_dict(dev_predictions, orient='index') \n",
        "dev_df_doc2vec.to_json('../data/dev_claims_doc2vec.json', orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Information Retrieval Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load information retrieval\n",
        "ir_dev_df = pd.read_pickle('dev-trained.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from dev claims\n",
        "ir_dev_corpus = {}\n",
        "\n",
        "for i in range(len(ir_dev_df)):\n",
        "    text2 = ir_dev_df.iloc[i].claim_text\n",
        "    # Create pairs claim + evidence\n",
        "    for evidence in ir_dev_df.iloc[i].evidences:\n",
        "        text2 = text2 + \" \" + evidences[evidence]\n",
        "    # Create pairs claim + all_evidence\n",
        "    ir_dev_corpus[ir_dev_df.iloc[i].name] = (str.strip(text2),ir_dev_df.iloc[i].claim_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from dev claims\n",
        "ir_dev_df = pd.DataFrame.from_dict(ir_dev_corpus, orient='index', columns=['text','label'])\n",
        "ir_dev_df = tokenize_text(ir_dev_df,'text')\n",
        "ir_dev_df['tagged'] = ir_dev_df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "\n",
        "ir_dev_corpus = ir_dev_df.tagged.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_ir_dev, X_ir_dev = vec_for_learning(model, ir_dev_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = logreg.predict(X_ir_dev)\n",
        "print('Testing accuracy %s' % accuracy_score(y_ir_dev, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_ir_dev, y_pred, average='weighted')))\n",
        "report = classification_report(y_ir_dev, y_pred)\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
