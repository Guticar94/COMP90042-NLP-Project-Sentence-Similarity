{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "pip install gensim\n",
        "\n",
        "pip install nltk\n",
        "\n",
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import collections\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read train claims\n",
        "with open('../data/train-claims.json', 'r') as f:\n",
        "    claims = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lowercasing the 'claim_text' field for each claim\n",
        "for claim_id, claim_info in claims.items():\n",
        "    claim_info['claim_text'] = claim_info['claim_text'].lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read evidence\n",
        "with open('../data/evidence.json', 'r') as f:\n",
        "    evidences = json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "evidences = {i: str.lower(j) for i,j in evidences.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from claims\n",
        "corpus = {}\n",
        "for id, claim in claims.items():\n",
        "    corpus[id] = str.strip(claim['claim_text'])  # Add claim text\n",
        "\n",
        "for id, evidence in evidences.items():\n",
        "    corpus[id] = str.strip(evidence) # Add evidence text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_text(df):\n",
        "    df['tokens'] = df['text'].apply(lambda x: [token for token in word_tokenize(x) if token.isalpha()])\n",
        "    df[\"length\"] = df.tokens.apply(len)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the list of documents into a pandas DataFrame\n",
        "df = pd.DataFrame.from_dict(corpus, orient='index', columns=['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>el niño drove record highs in global temperatu...</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>in 1946, pdo switched to a cool phase.</td>\n",
              "      <td>[in, pdo, switched, to, a, cool, phase]</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>weather channel co-founder john coleman provid...</td>\n",
              "      <td>[weather, channel, john, coleman, provided, ev...</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"january 2008 capped a 12 month period of glob...</td>\n",
              "      <td>[january, capped, a, month, period, of, global...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208822</th>\n",
              "      <td>also on the property is a contributing garage ...</td>\n",
              "      <td>[also, on, the, property, is, a, contributing,...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208823</th>\n",
              "      <td>| class = ``fn org'' | fyrde | | | | 6110 | | ...</td>\n",
              "      <td>[class, fn, org, fyrde, volda]</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208824</th>\n",
              "      <td>dragon storm (game), a role-playing game and c...</td>\n",
              "      <td>[dragon, storm, game, a, game, and, collectibl...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208825</th>\n",
              "      <td>it states that the zeriuani ``which is so grea...</td>\n",
              "      <td>[it, states, that, the, zeriuani, which, is, s...</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208826</th>\n",
              "      <td>the storyline revolves around a giant plesiosa...</td>\n",
              "      <td>[the, storyline, revolves, around, a, giant, p...</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1210055 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                               text  \\\n",
              "claim-1937        not only is there no scientific evidence that ...   \n",
              "claim-126         el niño drove record highs in global temperatu...   \n",
              "claim-2510                   in 1946, pdo switched to a cool phase.   \n",
              "claim-2021        weather channel co-founder john coleman provid...   \n",
              "claim-2449        \"january 2008 capped a 12 month period of glob...   \n",
              "...                                                             ...   \n",
              "evidence-1208822  also on the property is a contributing garage ...   \n",
              "evidence-1208823  | class = ``fn org'' | fyrde | | | | 6110 | | ...   \n",
              "evidence-1208824  dragon storm (game), a role-playing game and c...   \n",
              "evidence-1208825  it states that the zeriuani ``which is so grea...   \n",
              "evidence-1208826  the storyline revolves around a giant plesiosa...   \n",
              "\n",
              "                                                             tokens  length  \n",
              "claim-1937        [not, only, is, there, no, scientific, evidenc...      22  \n",
              "claim-126         [el, niño, drove, record, highs, in, global, t...      16  \n",
              "claim-2510                  [in, pdo, switched, to, a, cool, phase]       7  \n",
              "claim-2021        [weather, channel, john, coleman, provided, ev...      15  \n",
              "claim-2449        [january, capped, a, month, period, of, global...      17  \n",
              "...                                                             ...     ...  \n",
              "evidence-1208822  [also, on, the, property, is, a, contributing,...       9  \n",
              "evidence-1208823                     [class, fn, org, fyrde, volda]       5  \n",
              "evidence-1208824  [dragon, storm, game, a, game, and, collectibl...       9  \n",
              "evidence-1208825  [it, states, that, the, zeriuani, which, is, s...      46  \n",
              "evidence-1208826  [the, storyline, revolves, around, a, giant, p...      36  \n",
              "\n",
              "[1210055 rows x 3 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = tokenize_text(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "      <th>length</th>\n",
              "      <th>tagged</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>22</td>\n",
              "      <td>([not, only, is, there, no, scientific, eviden...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>el niño drove record highs in global temperatu...</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "      <td>16</td>\n",
              "      <td>([el, niño, drove, record, highs, in, global, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>in 1946, pdo switched to a cool phase.</td>\n",
              "      <td>[in, pdo, switched, to, a, cool, phase]</td>\n",
              "      <td>7</td>\n",
              "      <td>([in, pdo, switched, to, a, cool, phase], [cla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>weather channel co-founder john coleman provid...</td>\n",
              "      <td>[weather, channel, john, coleman, provided, ev...</td>\n",
              "      <td>15</td>\n",
              "      <td>([weather, channel, john, coleman, provided, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"january 2008 capped a 12 month period of glob...</td>\n",
              "      <td>[january, capped, a, month, period, of, global...</td>\n",
              "      <td>17</td>\n",
              "      <td>([january, capped, a, month, period, of, globa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208822</th>\n",
              "      <td>also on the property is a contributing garage ...</td>\n",
              "      <td>[also, on, the, property, is, a, contributing,...</td>\n",
              "      <td>9</td>\n",
              "      <td>([also, on, the, property, is, a, contributing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208823</th>\n",
              "      <td>| class = ``fn org'' | fyrde | | | | 6110 | | ...</td>\n",
              "      <td>[class, fn, org, fyrde, volda]</td>\n",
              "      <td>5</td>\n",
              "      <td>([class, fn, org, fyrde, volda], [evidence-120...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208824</th>\n",
              "      <td>dragon storm (game), a role-playing game and c...</td>\n",
              "      <td>[dragon, storm, game, a, game, and, collectibl...</td>\n",
              "      <td>9</td>\n",
              "      <td>([dragon, storm, game, a, game, and, collectib...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208825</th>\n",
              "      <td>it states that the zeriuani ``which is so grea...</td>\n",
              "      <td>[it, states, that, the, zeriuani, which, is, s...</td>\n",
              "      <td>46</td>\n",
              "      <td>([it, states, that, the, zeriuani, which, is, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208826</th>\n",
              "      <td>the storyline revolves around a giant plesiosa...</td>\n",
              "      <td>[the, storyline, revolves, around, a, giant, p...</td>\n",
              "      <td>36</td>\n",
              "      <td>([the, storyline, revolves, around, a, giant, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1210055 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                               text  \\\n",
              "claim-1937        not only is there no scientific evidence that ...   \n",
              "claim-126         el niño drove record highs in global temperatu...   \n",
              "claim-2510                   in 1946, pdo switched to a cool phase.   \n",
              "claim-2021        weather channel co-founder john coleman provid...   \n",
              "claim-2449        \"january 2008 capped a 12 month period of glob...   \n",
              "...                                                             ...   \n",
              "evidence-1208822  also on the property is a contributing garage ...   \n",
              "evidence-1208823  | class = ``fn org'' | fyrde | | | | 6110 | | ...   \n",
              "evidence-1208824  dragon storm (game), a role-playing game and c...   \n",
              "evidence-1208825  it states that the zeriuani ``which is so grea...   \n",
              "evidence-1208826  the storyline revolves around a giant plesiosa...   \n",
              "\n",
              "                                                             tokens  length  \\\n",
              "claim-1937        [not, only, is, there, no, scientific, evidenc...      22   \n",
              "claim-126         [el, niño, drove, record, highs, in, global, t...      16   \n",
              "claim-2510                  [in, pdo, switched, to, a, cool, phase]       7   \n",
              "claim-2021        [weather, channel, john, coleman, provided, ev...      15   \n",
              "claim-2449        [january, capped, a, month, period, of, global...      17   \n",
              "...                                                             ...     ...   \n",
              "evidence-1208822  [also, on, the, property, is, a, contributing,...       9   \n",
              "evidence-1208823                     [class, fn, org, fyrde, volda]       5   \n",
              "evidence-1208824  [dragon, storm, game, a, game, and, collectibl...       9   \n",
              "evidence-1208825  [it, states, that, the, zeriuani, which, is, s...      46   \n",
              "evidence-1208826  [the, storyline, revolves, around, a, giant, p...      36   \n",
              "\n",
              "                                                             tagged  \n",
              "claim-1937        ([not, only, is, there, no, scientific, eviden...  \n",
              "claim-126         ([el, niño, drove, record, highs, in, global, ...  \n",
              "claim-2510        ([in, pdo, switched, to, a, cool, phase], [cla...  \n",
              "claim-2021        ([weather, channel, john, coleman, provided, e...  \n",
              "claim-2449        ([january, capped, a, month, period, of, globa...  \n",
              "...                                                             ...  \n",
              "evidence-1208822  ([also, on, the, property, is, a, contributing...  \n",
              "evidence-1208823  ([class, fn, org, fyrde, volda], [evidence-120...  \n",
              "evidence-1208824  ([dragon, storm, game, a, game, and, collectib...  \n",
              "evidence-1208825  ([it, states, that, the, zeriuani, which, is, ...  \n",
              "evidence-1208826  ([the, storyline, revolves, around, a, giant, ...  \n",
              "\n",
              "[1210055 rows x 4 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a function to be applied to each row\n",
        "def process_row(row, index):\n",
        "    return TaggedDocument(row['tokens'], tags=[index])\n",
        "\n",
        "df['tagged'] = df.apply(lambda row: process_row(row, row.name), axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_corpus = df.tagged.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model\n",
        "https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Doc2Vec(vector_size=50, min_count=2, epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.build_vocab(train_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(\"Doc2Vec.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assesing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_corpus)):\n\u001b[0;32m      4\u001b[0m     inferred_vector \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minfer_vector(train_corpus[doc_id]\u001b[38;5;241m.\u001b[39mwords)\n\u001b[1;32m----> 5\u001b[0m     sims \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minferred_vector\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     inferred_vectors[doc_id] \u001b[38;5;241m=\u001b[39m sims[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m10\u001b[39m]\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\comp90042Project\\lib\\site-packages\\gensim\\models\\keyedvectors.py:854\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    852\u001b[0m best \u001b[38;5;241m=\u001b[39m matutils\u001b[38;5;241m.\u001b[39margsort(dists, topn\u001b[38;5;241m=\u001b[39mtopn \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_keys), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# ignore (don't return) keys from the input\u001b[39;00m\n\u001b[1;32m--> 854\u001b[0m result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    855\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_key[sim \u001b[38;5;241m+\u001b[39m clip_start], \u001b[38;5;28mfloat\u001b[39m(dists[sim]))\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sim \u001b[38;5;129;01min\u001b[39;00m best \u001b[38;5;28;01mif\u001b[39;00m (sim \u001b[38;5;241m+\u001b[39m clip_start) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_keys\n\u001b[0;32m    857\u001b[0m ]\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[:topn]\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\comp90042Project\\lib\\site-packages\\gensim\\models\\keyedvectors.py:855\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    852\u001b[0m best \u001b[38;5;241m=\u001b[39m matutils\u001b[38;5;241m.\u001b[39margsort(dists, topn\u001b[38;5;241m=\u001b[39mtopn \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_keys), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# ignore (don't return) keys from the input\u001b[39;00m\n\u001b[0;32m    854\u001b[0m result \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 855\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_key[sim \u001b[38;5;241m+\u001b[39m \u001b[43mclip_start\u001b[49m], \u001b[38;5;28mfloat\u001b[39m(dists[sim]))\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sim \u001b[38;5;129;01min\u001b[39;00m best \u001b[38;5;28;01mif\u001b[39;00m (sim \u001b[38;5;241m+\u001b[39m clip_start) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_keys\n\u001b[0;32m    857\u001b[0m ]\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[:topn]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "inferred_vectors = {}\n",
        "\n",
        "for doc_id in range(len(train_corpus)):\n",
        "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "    inferred_vectors[doc_id] = sims[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top10_rank(doc_id):\n",
        "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "    similarity_vector = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "    return similarity_vector[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save DataFrame as a Pickle file\n",
        "df.to_pickle('dfWord2Vec.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restore the DataFrame from the Pickle file\n",
        "df_restored = pd.read_pickle('dfWord2Vec.pkl')\n",
        "\n",
        "# Verify the restored DataFrame\n",
        "print(df_restored)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "#existing_document_vector = model.dv['doc1']\n",
        "\n",
        "\n",
        "\n",
        "df['infered'] = df.apply(lambda row: process_row(row, row.name), axis=1)\n",
        "df['infered'] = df['text'].apply(lambda x: [token for token in word_tokenize(x) if token.isalpha()])\n",
        "df['infered'] = df['text'].apply(lambda x: get_top10_rank(x))\n",
        "df.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "inferred_vectors = {}\n",
        "sims = {}\n",
        "\n",
        "for doc_id in range(len(train_corpus)):\n",
        "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "    inferred_vectors[doc_id] = inferred_vector\n",
        "    #sim = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "    #sims[doc_id] = sim\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for key, values in inferred_vectors:\n",
        "    print(key, values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ranks = []\n",
        "second_ranks = []\n",
        "\n",
        "rank = [docid for docid, sim in sims].index(doc_id)\n",
        "ranks.append(rank)\n",
        "\n",
        "second_ranks.append(sims[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "counter = collections.Counter(ranks)\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
