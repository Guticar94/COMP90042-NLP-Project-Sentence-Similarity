{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%pip install gensim nltk pandas sklearn torch rank_bm25\n",
        "\n",
        "Code based on https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "import pickle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "import collections\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Garbage Collector to fix memory issues\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read train claims\n",
        "with open('../data/train-claims.json', 'r') as f:\n",
        "    claims = json.load(f)\n",
        "\n",
        "# Read dev claims\n",
        "with open('../data/dev-claims.json', 'r') as f:\n",
        "    dev_claims = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lowercasing the 'claim_text' field for each claim\n",
        "for claim_id, claim_info in claims.items():\n",
        "    claim_info['claim_text'] = claim_info['claim_text'].lower()\n",
        "\n",
        "for claim_id, claim_info in dev_claims.items():\n",
        "    claim_info['claim_text'] = claim_info['claim_text'].lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read evidence\n",
        "with open('../data/evidence.json', 'r') as f:\n",
        "    evidences = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "evidences = {i: str.lower(j) for i,j in evidences.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of claims for training = 1228\n",
            "Number of claims for development = 154\n",
            "Number of evidences = 1208827\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of claims for training = {0}\".format(len(claims)))\n",
        "print(\"Number of claims for development = {0}\".format(len(dev_claims)))\n",
        "print(\"Number of evidences = {0}\".format(len(evidences)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Second approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from claims\n",
        "corpus = {}\n",
        "\n",
        "for id, claim in claims.items():\n",
        "    text2 = claim['claim_text']\n",
        "    # Create pairs claim + evidence\n",
        "    for evidence in claim['evidences']:\n",
        "        text = claim['claim_text'] + \" \" + evidences[evidence]\n",
        "        corpus[id + ' - ' + evidence] = (str.strip(text),id)\n",
        "        text2 = text2 + \" \" + evidences[evidence]\n",
        "    # Create pairs claim + all_evidence\n",
        "    corpus[id] = (str.strip(text2),claim['claim_label'])\n",
        "\n",
        "for id, evidence in evidences.items():\n",
        "    corpus[id] = (str.strip(evidence),id) # Add evidence text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### First approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model in claims and evidences\n",
        "# Collect all texts from claims\n",
        "#corpus = {}\n",
        "#for id, claim in claims.items():\n",
        "#    corpus[id] = str.strip(claim['claim_text'])  # Add claim text\n",
        "#for id, evidence in evidences.items():\n",
        "#    corpus[id] = str.strip(evidence) # Add evidence text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \n",
        "def tokenize_text(df, column):\n",
        "    df['tokens'] = df[column].apply(lambda x: [token for token in word_tokenize(x) if token.isalnum()])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row(row, index):\n",
        "    return TaggedDocument(row['tokens'], tags=[row[index]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the list of documents into a pandas DataFrame\n",
        "df = pd.DataFrame.from_dict(corpus, orient='index', columns=['text','label'])\n",
        "df = tokenize_text(df,'text')\n",
        "df['tagged'] = df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "df.info()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_corpus = df.tagged.values\n",
        "del df\n",
        "gc.collect()\n",
        "train_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from dev claims\n",
        "dv_corpus = {}\n",
        "\n",
        "for id, claim in dev_claims.items():\n",
        "    text2 = claim['claim_text']\n",
        "    # Create pairs claim + evidence\n",
        "    for evidence in claim['evidences']:\n",
        "        text = claim['claim_text'] + \" \" + evidences[evidence]\n",
        "        dv_corpus[id + ' - ' + evidence] = (str.strip(text),id)\n",
        "        text2 = text2 + \" \" + evidences[evidence]\n",
        "    # Create pairs claim + all_evidence\n",
        "    dv_corpus[id] = (str.strip(text2),claim['claim_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from dev claims\n",
        "dev_df = pd.DataFrame.from_dict(dv_corpus, orient='index', columns=['text','label'])\n",
        "dev_df = tokenize_text(dev_df,'text')\n",
        "dev_df['tagged'] = dev_df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "dev_df.info()\n",
        "dev_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dev_corpus = dev_df.tagged.values\n",
        "dev_corpus[0:5]\n",
        "del dev_df\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model\n",
        "https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "cores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Doc2Vec model\n",
        "model = Doc2Vec(dm=1, vector_size=100, window=5, min_count=1, workers=cores, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.build_vocab(train_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "class EpochLogger(CallbackAny2Vec):\n",
        "    '''Callback to log information about training'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "        self.last_signal = datetime.now()\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        t = datetime.now() - self.last_signal\n",
        "        print(\"Epoch #{} - Duration: {}\".format(self.epoch, t))\n",
        "        self.epoch += 1\n",
        "        self.last_signal = datetime.now()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_logger = EpochLogger()\n",
        "model.train(train_corpus, total_examples=model.corpus_count,  epochs=model.epochs, callbacks=[epoch_logger])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(\"Doc2Vec.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assesing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evidences_df = pd.DataFrame.from_dict(evidences, orient='index', columns=['text'])\n",
        "evidences_df = tokenize_text(evidences_df,'text')\n",
        "evidences_df['inferred'] = evidences_df['tokens'].apply(lambda x: model.infer_vector(x))\n",
        "evidences_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evidences_df.to_pickle('evidences_df.pkl')\n",
        "#evidences_df = pd.read_pickle('evidences_df.pkl')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "claims_df = pd.DataFrame.from_dict(claims, orient='index')\n",
        "claims_df = tokenize_text(claims_df,'claim_text')\n",
        "claims_df['inferred'] = claims_df['tokens'].apply(lambda x: model.infer_vector(x))\n",
        "claims_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "claims_df.to_pickle('claims_df.pkl')\n",
        "#claims_df = pd.read_pickle('claims_df.pkl')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implement BM25 Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize BM25 model\n",
        "bm25 = BM25Okapi(evidences_df['tokens'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate BM25 scores for each claim\n",
        "def calculate_bm25_scores(query_tokens):\n",
        "    return bm25.get_scores(query_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating the BM25 scores\")\n",
        "# Compute BM25 scores\n",
        "bm25_scores = claims_df['tokens'].apply(calculate_bm25_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_scores.to_pickle('bm25_scores.pkl')\n",
        "#bm25_scores = pd.read_pickle('bm25_scores.pkl')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract lists\n",
        "claim_vectors = claims_df['inferred'].to_list()\n",
        "evidence_vectors = evidences_df['inferred'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating the similarities\")\n",
        "# Calculate Doc2Vec similarities\n",
        "doc2vec_similarities = cosine_similarity(claim_vectors, evidence_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('doc2vec_similarities.pkl','wb') as f: pickle.dump(doc2vec_similarities, f)\n",
        "#with open('doc2vec_similarities.pkl','rb') as f: doc2vec_similarities = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize the lengths\n",
        "def normalize(scores):\n",
        "    return (scores - np.min(scores)) / (np.max(scores) - np.min(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "document_lengths = [len(doc) for doc in evidences_df['tokens']]\n",
        "normalized_lengths = normalize(document_lengths)\n",
        "p_normalized_lengths = np.array(0.2 * normalized_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_bm25_scores = [normalize(doc) for doc in bm25_scores]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_doc2vec_similarities = normalize(doc2vec_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_bm25_scores = np.array([0.4 * doc for doc in n_bm25_scores])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_doc2vec_similarities = np.array([0.4 * doc for doc in n_doc2vec_similarities])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del document_lengths, normalized_lengths\n",
        "del bm25_scores, n_bm25_scores\n",
        "del doc2vec_similarities, n_doc2vec_similarities\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_combined_scores(bm25,similarities,lengths):\n",
        "    # Initialize an array to store the sum results\n",
        "    scores = []\n",
        "\n",
        "    # Perform the element-wise addition\n",
        "    for i in range(len(similarities)):\n",
        "        sum_result = bm25[i] + similarities[i] + lengths\n",
        "        scores.append(sum_result)\n",
        "    \n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dev claims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize an array to store the sum results\n",
        "combined_scores = get_combined_scores(p_bm25_scores, p_doc2vec_similarities, p_normalized_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del p_bm25_scores, p_doc2vec_similarities\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rank evidences\n",
        "ranked_index = np.argsort(combined_scores, axis=1)[:, ::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('ranked_index.pkl','wb') as f: pickle.dump(ranked_index, f)\n",
        "#with open('ranked_index.pkl','rb') as f: ranked_index = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dev_df = pd.DataFrame.from_dict(dev_claims, orient='index')\n",
        "dev_df = tokenize_text(dev_df,'claim_text')\n",
        "dev_df['inferred'] = dev_df['tokens'].apply(lambda x: model.infer_vector(x))\n",
        "dev_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating the dev BM25 scores\")\n",
        "# Compute dev BM25 scores\n",
        "dev_bm25_scores = dev_df['tokens'].apply(calculate_bm25_scores)\n",
        "n_dev_bm25_scores = [normalize(doc) for doc in dev_bm25_scores]\n",
        "p_dev_bm25_scores = np.array([0.4 * doc for doc in n_dev_bm25_scores])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dev_bm25_scores.to_pickle('dev_bm25_scores.pkl')\n",
        "#dev_bm25_scores = pd.read_pickle('dev_bm25_scores.pkl')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract lists\n",
        "claim_dev_vectors = dev_df['inferred'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating the dev similarities\")\n",
        "# Calculate Doc2Vec similarities\n",
        "dev_doc2vec_similarities = cosine_similarity(claim_dev_vectors, evidence_vectors)\n",
        "n_dev_doc2vec_similarities = normalize(dev_doc2vec_similarities)\n",
        "p_dev_doc2vec_similarities = np.array([0.4 * doc for doc in n_dev_doc2vec_similarities])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('dev_doc2vec_similarities.pkl','wb') as f: pickle.dump(dev_doc2vec_similarities, f)\n",
        "#with open('dev_doc2vec_similarities.pkl','rb') as f: dev_doc2vec_similarities = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize an array to store the sum results\n",
        "dev_combined_scores = get_combined_scores(p_dev_bm25_scores, p_dev_doc2vec_similarities, p_normalized_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del p_dev_bm25_scores, p_dev_doc2vec_similarities\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rank evidences\n",
        "dev_ranked_index = np.argsort(dev_combined_scores, axis=1)[:, ::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the list of documents into a pandas DataFrame\n",
        "df = pd.DataFrame.from_dict(corpus, orient='index', columns=['text','label'])\n",
        "df = tokenize_text(df,'text')\n",
        "df['tagged'] = df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "df = df[~df['label'].str.startswith('claim')]\n",
        "\n",
        "# Filter rows where the label does not start with 'claim'\n",
        "train_corpus = df.tagged.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from dev claims\n",
        "dev_df = pd.DataFrame.from_dict(dv_corpus, orient='index', columns=['text','label'])\n",
        "dev_df = tokenize_text(dev_df,'text')\n",
        "dev_df['tagged'] = dev_df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "dev_df = dev_df[~dev_df['label'].str.startswith('claim')]\n",
        "\n",
        "# Filter rows where the label does not start with 'claim'\n",
        "dev_corpus = dev_df.tagged.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label Distribution\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16,6))\n",
        "\n",
        "axs[0].set_title(\"Train\")\n",
        "axs[1].set_title(\"Validation\")\n",
        "tlabel = axs[0].hist(sorted([l for l in df['label']]))\n",
        "vlabel = axs[1].hist(sorted([l for l in dev_df['label']]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vec_for_learning(model, sents):\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
        "    return targets, regressors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg = LogisticRegression(n_jobs=cores, C=1e5, max_iter=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train, X_train = vec_for_learning(model, train_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_dev, X_dev = vec_for_learning(model, dev_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = logreg.predict(X_dev)\n",
        "print('Testing accuracy %s' % accuracy_score(y_dev, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_dev, y_pred, average='weighted')))\n",
        "report = classification_report(y_dev, y_pred)\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_pred = logreg.predict(X_train)\n",
        "print('Testing accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_train, y_train_pred, average='weighted')))\n",
        "report = classification_report(y_train, y_train_pred)\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = {}\n",
        "for i in range(len(claims_df)):\n",
        "    ev_list = ['evidence-'+ str(num) for num in ranked_index[i][:5] ]\n",
        "    predictions[claims_df.index[i]] = {}\n",
        "    predictions[claims_df.index[i]][\"claim_text\"] = claims_df.claim_text[i]\n",
        "    predictions[claims_df.index[i]][\"claim_label\"] = y_train_pred[i]\n",
        "    predictions[claims_df.index[i]][\"evidences\"] = ev_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the DataFrame to a JSON file\n",
        "train_df_doc2vec = pd.DataFrame.from_dict(predictions, orient='index') \n",
        "train_df_doc2vec.to_json('../data/train_claims_doc2vec.json', orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dev_predictions = {}\n",
        "for i in range(len(dev_df)):\n",
        "    ev_list = ['evidence-'+ str(num) for num in ranked_index[i][:5] ]\n",
        "    dev_predictions[dev_df.index[i]] = {}\n",
        "    dev_predictions[dev_df.index[i]][\"claim_text\"] = dev_df.text[i]\n",
        "    dev_predictions[dev_df.index[i]][\"claim_label\"] = y_pred[i]\n",
        "    dev_predictions[dev_df.index[i]][\"evidences\"] = ev_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the DataFrame to a JSON file\n",
        "dev_df_doc2vec = pd.DataFrame.from_dict(dev_predictions, orient='index') \n",
        "dev_df_doc2vec.to_json('../data/dev_claims_doc2vec.json', orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Information Retrieval Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load information retrieval\n",
        "ir_dev_df = pd.read_pickle('dev-trained.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from dev claims\n",
        "ir_dev_corpus = {}\n",
        "\n",
        "for i in range(len(ir_dev_df)):\n",
        "    text2 = ir_dev_df.iloc[i].claim_text\n",
        "    # Create pairs claim + evidence\n",
        "    for evidence in ir_dev_df.iloc[i].evidences:\n",
        "        text2 = text2 + \" \" + evidences[evidence]\n",
        "    # Create pairs claim + all_evidence\n",
        "    ir_dev_corpus[ir_dev_df.iloc[i].name] = (str.strip(text2),ir_dev_df.iloc[i].claim_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from dev claims\n",
        "ir_dev_df = pd.DataFrame.from_dict(ir_dev_corpus, orient='index', columns=['text','label'])\n",
        "ir_dev_df = tokenize_text(ir_dev_df,'text')\n",
        "ir_dev_df['tagged'] = ir_dev_df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "\n",
        "ir_dev_corpus = ir_dev_df.tagged.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_ir_dev, X_ir_dev = vec_for_learning(model, ir_dev_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = logreg.predict(X_ir_dev)\n",
        "print('Testing accuracy %s' % accuracy_score(y_ir_dev, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_ir_dev, y_pred, average='weighted')))\n",
        "report = classification_report(y_ir_dev, y_pred)\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
