{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%pip install gensim nltk pandas sklearn torch rank_bm25\n",
        "\n",
        "Code based on https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "import pickle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "import collections\n",
        "\n",
        "from rank_bm25 import BM25Okapi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read train claims\n",
        "with open('../data/train-claims.json', 'r') as f:\n",
        "    claims = json.load(f)\n",
        "\n",
        "# Read dev claims\n",
        "with open('../data/dev-claims.json', 'r') as f:\n",
        "    dev_claims = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lowercasing the 'claim_text' field for each claim\n",
        "for claim_id, claim_info in claims.items():\n",
        "    claim_info['claim_text'] = claim_info['claim_text'].lower()\n",
        "\n",
        "for claim_id, claim_info in dev_claims.items():\n",
        "    claim_info['claim_text'] = claim_info['claim_text'].lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read evidence\n",
        "with open('../data/evidence.json', 'r') as f:\n",
        "    evidences = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "evidences = {i: str.lower(j) for i,j in evidences.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of claims for training = 1228\n",
            "Number of claims for development = 154\n",
            "Number of evidences = 1208827\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of claims for training = {0}\".format(len(claims)))\n",
        "print(\"Number of claims for development = {0}\".format(len(dev_claims)))\n",
        "print(\"Number of evidences = {0}\".format(len(evidences)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Second approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from claims\n",
        "corpus = {}\n",
        "\n",
        "for id, claim in claims.items():\n",
        "    # Create pairs claim + evidence\n",
        "    for evidence in claim['evidences']:\n",
        "        text = claim['claim_text'] + \" \" + evidences[evidence]\n",
        "        corpus[id + ' - ' + evidence] = (str.strip(text),claim['claim_label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### First approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model in claims and evidences\n",
        "# Collect all texts from claims\n",
        "#corpus = {}\n",
        "#for id, claim in claims.items():\n",
        "#    corpus[id] = str.strip(claim['claim_text'])  # Add claim text\n",
        "#for id, evidence in evidences.items():\n",
        "#    corpus[id] = str.strip(evidence) # Add evidence text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_text(df, column):\n",
        "    df['tokens'] = df[column].apply(lambda x: [token for token in word_tokenize(x) if token.isalnum()])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row(row, index):\n",
        "    return TaggedDocument(row['tokens'], tags=[row[index]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 4122 entries, claim-1937 - evidence-442946 to claim-3093 - evidence-883158\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    4122 non-null   object\n",
            " 1   label   4122 non-null   object\n",
            " 2   tokens  4122 non-null   object\n",
            " 3   tagged  4122 non-null   object\n",
            "dtypes: object(4)\n",
            "memory usage: 161.0+ KB\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tagged</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937 - evidence-442946</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>([not, only, is, there, no, scientific, eviden...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1937 - evidence-1194317</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>([not, only, is, there, no, scientific, eviden...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1937 - evidence-12171</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>([not, only, is, there, no, scientific, eviden...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126 - evidence-338219</th>\n",
              "      <td>el niño drove record highs in global temperatu...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "      <td>([el, niño, drove, record, highs, in, global, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126 - evidence-1127398</th>\n",
              "      <td>el niño drove record highs in global temperatu...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "      <td>([el, niño, drove, record, highs, in, global, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                            text  \\\n",
              "claim-1937 - evidence-442946   not only is there no scientific evidence that ...   \n",
              "claim-1937 - evidence-1194317  not only is there no scientific evidence that ...   \n",
              "claim-1937 - evidence-12171    not only is there no scientific evidence that ...   \n",
              "claim-126 - evidence-338219    el niño drove record highs in global temperatu...   \n",
              "claim-126 - evidence-1127398   el niño drove record highs in global temperatu...   \n",
              "\n",
              "                                  label  \\\n",
              "claim-1937 - evidence-442946   DISPUTED   \n",
              "claim-1937 - evidence-1194317  DISPUTED   \n",
              "claim-1937 - evidence-12171    DISPUTED   \n",
              "claim-126 - evidence-338219     REFUTES   \n",
              "claim-126 - evidence-1127398    REFUTES   \n",
              "\n",
              "                                                                          tokens  \\\n",
              "claim-1937 - evidence-442946   [not, only, is, there, no, scientific, evidenc...   \n",
              "claim-1937 - evidence-1194317  [not, only, is, there, no, scientific, evidenc...   \n",
              "claim-1937 - evidence-12171    [not, only, is, there, no, scientific, evidenc...   \n",
              "claim-126 - evidence-338219    [el, niño, drove, record, highs, in, global, t...   \n",
              "claim-126 - evidence-1127398   [el, niño, drove, record, highs, in, global, t...   \n",
              "\n",
              "                                                                          tagged  \n",
              "claim-1937 - evidence-442946   ([not, only, is, there, no, scientific, eviden...  \n",
              "claim-1937 - evidence-1194317  ([not, only, is, there, no, scientific, eviden...  \n",
              "claim-1937 - evidence-12171    ([not, only, is, there, no, scientific, eviden...  \n",
              "claim-126 - evidence-338219    ([el, niño, drove, record, highs, in, global, ...  \n",
              "claim-126 - evidence-1127398   ([el, niño, drove, record, highs, in, global, ...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the list of documents into a pandas DataFrame\n",
        "df = pd.DataFrame.from_dict(corpus, orient='index', columns=['text','label'])\n",
        "df = tokenize_text(df,'text')\n",
        "df['tagged'] = df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "df.info()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([TaggedDocument(words=['not', 'only', 'is', 'there', 'no', 'scientific', 'evidence', 'that', 'co2', 'is', 'a', 'pollutant', 'higher', 'co2', 'concentrations', 'actually', 'help', 'ecosystems', 'support', 'more', 'plant', 'and', 'animal', 'life', 'at', 'very', 'high', 'concentrations', '100', 'times', 'atmospheric', 'concentration', 'or', 'greater', 'carbon', 'dioxide', 'can', 'be', 'toxic', 'to', 'animal', 'life', 'so', 'raising', 'the', 'concentration', 'to', 'ppm', '1', 'or', 'higher', 'for', 'several', 'hours', 'will', 'eliminate', 'pests', 'such', 'as', 'whiteflies', 'and', 'spider', 'mites', 'in', 'a', 'greenhouse'], tags=['DISPUTED']),\n",
              "       TaggedDocument(words=['not', 'only', 'is', 'there', 'no', 'scientific', 'evidence', 'that', 'co2', 'is', 'a', 'pollutant', 'higher', 'co2', 'concentrations', 'actually', 'help', 'ecosystems', 'support', 'more', 'plant', 'and', 'animal', 'life', 'plants', 'can', 'grow', 'as', 'much', 'as', '50', 'percent', 'faster', 'in', 'concentrations', 'of', 'ppm', 'co', '2', 'when', 'compared', 'with', 'ambient', 'conditions', 'though', 'this', 'assumes', 'no', 'change', 'in', 'climate', 'and', 'no', 'limitation', 'on', 'other', 'nutrients'], tags=['DISPUTED']),\n",
              "       TaggedDocument(words=['not', 'only', 'is', 'there', 'no', 'scientific', 'evidence', 'that', 'co2', 'is', 'a', 'pollutant', 'higher', 'co2', 'concentrations', 'actually', 'help', 'ecosystems', 'support', 'more', 'plant', 'and', 'animal', 'life', 'higher', 'carbon', 'dioxide', 'concentrations', 'will', 'favourably', 'affect', 'plant', 'growth', 'and', 'demand', 'for', 'water'], tags=['DISPUTED']),\n",
              "       ...,\n",
              "       TaggedDocument(words=['sending', 'oscillating', 'microwaves', 'from', 'an', 'antenna', 'inside', 'a', 'vacuum', 'through', 'an', 'electromagnetic', 'field', 'through', 'a', 'dielectric', 'material', 'such', 'as', 'water', 'creates', 'radio', 'frequency', 'heating', 'at', 'the', 'molecular', 'level', 'an', 'example', 'is', 'absorption', 'or', 'emission', 'of', 'radio', 'waves', 'by', 'antennas', 'or', 'absorption', 'of', 'microwaves', 'by', 'water', 'or', 'other', 'molecules', 'with', 'an', 'electric', 'dipole', 'moment', 'as', 'for', 'example', 'inside', 'a', 'microwave', 'oven'], tags=['SUPPORTS']),\n",
              "       TaggedDocument(words=['sending', 'oscillating', 'microwaves', 'from', 'an', 'antenna', 'inside', 'a', 'vacuum', 'through', 'an', 'electromagnetic', 'field', 'through', 'a', 'dielectric', 'material', 'such', 'as', 'water', 'creates', 'radio', 'frequency', 'heating', 'at', 'the', 'molecular', 'level', 'water', 'fat', 'and', 'other', 'substances', 'in', 'the', 'food', 'absorb', 'energy', 'from', 'the', 'microwaves', 'in', 'a', 'process', 'called', 'dielectric', 'heating'], tags=['SUPPORTS']),\n",
              "       TaggedDocument(words=['sending', 'oscillating', 'microwaves', 'from', 'an', 'antenna', 'inside', 'a', 'vacuum', 'through', 'an', 'electromagnetic', 'field', 'through', 'a', 'dielectric', 'material', 'such', 'as', 'water', 'creates', 'radio', 'frequency', 'heating', 'at', 'the', 'molecular', 'level', 'a', 'microwave', 'oven', 'passes', 'microwave', 'radiation', 'at', 'a', 'frequency', 'near', 'ghz', '12', 'cm', 'through', 'food', 'causing', 'dielectric', 'heating', 'primarily', 'by', 'absorption', 'of', 'the', 'energy', 'in', 'water'], tags=['SUPPORTS'])],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_corpus = df.tagged.values\n",
        "del df\n",
        "train_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from claims\n",
        "corpus = {}\n",
        "\n",
        "for id, claim in dev_claims.items():\n",
        "    # Create pairs claim + evidence\n",
        "    for evidence in claim['evidences']:\n",
        "        text = claim['claim_text'] + \" \" + evidences[evidence]\n",
        "        corpus[id + ' - ' + evidence] = (str.strip(text),claim['claim_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 491 entries, claim-752 - evidence-67732 to claim-1021 - evidence-1175280\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    491 non-null    object\n",
            " 1   label   491 non-null    object\n",
            " 2   tokens  491 non-null    object\n",
            " 3   tagged  491 non-null    object\n",
            "dtypes: object(4)\n",
            "memory usage: 19.2+ KB\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tagged</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-752 - evidence-67732</th>\n",
              "      <td>[south australia] has the most expensive elect...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[south, australia, has, the, most, expensive, ...</td>\n",
              "      <td>([south, australia, has, the, most, expensive,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-752 - evidence-572512</th>\n",
              "      <td>[south australia] has the most expensive elect...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[south, australia, has, the, most, expensive, ...</td>\n",
              "      <td>([south, australia, has, the, most, expensive,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-375 - evidence-996421</th>\n",
              "      <td>when 3 per cent of total annual global emissio...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[when, 3, per, cent, of, total, annual, global...</td>\n",
              "      <td>([when, 3, per, cent, of, total, annual, globa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-375 - evidence-1080858</th>\n",
              "      <td>when 3 per cent of total annual global emissio...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[when, 3, per, cent, of, total, annual, global...</td>\n",
              "      <td>([when, 3, per, cent, of, total, annual, globa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-375 - evidence-208053</th>\n",
              "      <td>when 3 per cent of total annual global emissio...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[when, 3, per, cent, of, total, annual, global...</td>\n",
              "      <td>([when, 3, per, cent, of, total, annual, globa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                           text  \\\n",
              "claim-752 - evidence-67732    [south australia] has the most expensive elect...   \n",
              "claim-752 - evidence-572512   [south australia] has the most expensive elect...   \n",
              "claim-375 - evidence-996421   when 3 per cent of total annual global emissio...   \n",
              "claim-375 - evidence-1080858  when 3 per cent of total annual global emissio...   \n",
              "claim-375 - evidence-208053   when 3 per cent of total annual global emissio...   \n",
              "\n",
              "                                        label  \\\n",
              "claim-752 - evidence-67732           SUPPORTS   \n",
              "claim-752 - evidence-572512          SUPPORTS   \n",
              "claim-375 - evidence-996421   NOT_ENOUGH_INFO   \n",
              "claim-375 - evidence-1080858  NOT_ENOUGH_INFO   \n",
              "claim-375 - evidence-208053   NOT_ENOUGH_INFO   \n",
              "\n",
              "                                                                         tokens  \\\n",
              "claim-752 - evidence-67732    [south, australia, has, the, most, expensive, ...   \n",
              "claim-752 - evidence-572512   [south, australia, has, the, most, expensive, ...   \n",
              "claim-375 - evidence-996421   [when, 3, per, cent, of, total, annual, global...   \n",
              "claim-375 - evidence-1080858  [when, 3, per, cent, of, total, annual, global...   \n",
              "claim-375 - evidence-208053   [when, 3, per, cent, of, total, annual, global...   \n",
              "\n",
              "                                                                         tagged  \n",
              "claim-752 - evidence-67732    ([south, australia, has, the, most, expensive,...  \n",
              "claim-752 - evidence-572512   ([south, australia, has, the, most, expensive,...  \n",
              "claim-375 - evidence-996421   ([when, 3, per, cent, of, total, annual, globa...  \n",
              "claim-375 - evidence-1080858  ([when, 3, per, cent, of, total, annual, globa...  \n",
              "claim-375 - evidence-208053   ([when, 3, per, cent, of, total, annual, globa...  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Collect all texts from dev claims\n",
        "dev_df = pd.DataFrame.from_dict(corpus, orient='index', columns=['text','label'])\n",
        "dev_df = tokenize_text(dev_df,'text')\n",
        "dev_df['tagged'] = dev_df.apply(lambda row: process_row(row, 'label'), axis=1)\n",
        "dev_df.info()\n",
        "dev_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([TaggedDocument(words=['south', 'australia', 'has', 'the', 'most', 'expensive', 'electricity', 'in', 'the', 'world', 'citation', 'needed', 'south', 'australia', 'has', 'the', 'highest', 'retail', 'price', 'for', 'electricity', 'in', 'the', 'country'], tags=['SUPPORTS']),\n",
              "       TaggedDocument(words=['south', 'australia', 'has', 'the', 'most', 'expensive', 'electricity', 'in', 'the', 'world', 'south', 'australia', 'has', 'the', 'highest', 'power', 'prices', 'in', 'the', 'world'], tags=['SUPPORTS']),\n",
              "       TaggedDocument(words=['when', '3', 'per', 'cent', 'of', 'total', 'annual', 'global', 'emissions', 'of', 'carbon', 'dioxide', 'are', 'from', 'humans', 'and', 'australia', 'per', 'cent', 'of', 'this', '3', 'per', 'cent', 'then', 'no', 'amount', 'of', 'emissions', 'here', 'will', 'have', 'any', 'effect', 'on', 'global', 'climate', 'the', '2011', 'unep', 'green', 'economy', 'report', 'states', 'that', 'a', 'agricultural', 'operations', 'excluding', 'land', 'use', 'changes', 'produce', 'approximately', '13', 'per', 'cent', 'of', 'anthropogenic', 'global', 'ghg', 'emissions'], tags=['NOT_ENOUGH_INFO']),\n",
              "       TaggedDocument(words=['when', '3', 'per', 'cent', 'of', 'total', 'annual', 'global', 'emissions', 'of', 'carbon', 'dioxide', 'are', 'from', 'humans', 'and', 'australia', 'per', 'cent', 'of', 'this', '3', 'per', 'cent', 'then', 'no', 'amount', 'of', 'emissions', 'here', 'will', 'have', 'any', 'effect', 'on', 'global', 'climate', 'with', 'a', 'market', 'share', 'of', '30', 'and', 'potentially', 'clean', 'electricity', 'heat', 'pumps', 'could', 'reduce', 'global', 'co', '2', 'emissions', 'by', '8', 'annually'], tags=['NOT_ENOUGH_INFO']),\n",
              "       TaggedDocument(words=['when', '3', 'per', 'cent', 'of', 'total', 'annual', 'global', 'emissions', 'of', 'carbon', 'dioxide', 'are', 'from', 'humans', 'and', 'australia', 'per', 'cent', 'of', 'this', '3', 'per', 'cent', 'then', 'no', 'amount', 'of', 'emissions', 'here', 'will', 'have', 'any', 'effect', 'on', 'global', 'climate', 'in', 'the', 'modern', 'era', 'emissions', 'to', 'the', 'atmosphere', 'from', 'volcanoes', 'are', 'approximately', 'billion', 'tonnes', 'of', 'co', '2', 'per', 'year', 'whereas', 'humans', 'contribute', '29', 'billion', 'tonnes', 'of', 'co', '2', 'each', 'year'], tags=['NOT_ENOUGH_INFO'])],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_corpus = dev_df.tagged.values\n",
        "del dev_df\n",
        "dev_corpus[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model\n",
        "https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "cores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Doc2Vec model\n",
        "model = Doc2Vec(dm=1, vector_size=50, window=5, min_count=1, workers=cores, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.build_vocab(train_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "class EpochLogger(CallbackAny2Vec):\n",
        "    '''Callback to log information about training'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "        self.last_signal = datetime.now()\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        t = datetime.now() - self.last_signal\n",
        "        print(\"Epoch #{} - Duration: {}\".format(self.epoch, t))\n",
        "        self.epoch += 1\n",
        "        self.last_signal = datetime.now()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #0 - Duration: 0:00:00.423078\n",
            "Epoch #1 - Duration: 0:00:00.455902\n",
            "Epoch #2 - Duration: 0:00:00.433977\n",
            "Epoch #3 - Duration: 0:00:00.436833\n",
            "Epoch #4 - Duration: 0:00:00.417160\n",
            "Epoch #5 - Duration: 0:00:00.422769\n",
            "Epoch #6 - Duration: 0:00:00.423467\n",
            "Epoch #7 - Duration: 0:00:00.426159\n",
            "Epoch #8 - Duration: 0:00:00.417526\n",
            "Epoch #9 - Duration: 0:00:00.421000\n",
            "Epoch #10 - Duration: 0:00:00.431445\n",
            "Epoch #11 - Duration: 0:00:00.412789\n",
            "Epoch #12 - Duration: 0:00:00.427250\n",
            "Epoch #13 - Duration: 0:00:00.440512\n",
            "Epoch #14 - Duration: 0:00:00.430008\n",
            "Epoch #15 - Duration: 0:00:00.423950\n",
            "Epoch #16 - Duration: 0:00:00.423063\n",
            "Epoch #17 - Duration: 0:00:00.425558\n",
            "Epoch #18 - Duration: 0:00:00.423047\n",
            "Epoch #19 - Duration: 0:00:00.482626\n",
            "Epoch #20 - Duration: 0:00:00.414231\n",
            "Epoch #21 - Duration: 0:00:00.413847\n",
            "Epoch #22 - Duration: 0:00:00.431119\n",
            "Epoch #23 - Duration: 0:00:00.415751\n",
            "Epoch #24 - Duration: 0:00:00.411826\n",
            "Epoch #25 - Duration: 0:00:00.418572\n",
            "Epoch #26 - Duration: 0:00:00.419590\n",
            "Epoch #27 - Duration: 0:00:00.419787\n",
            "Epoch #28 - Duration: 0:00:00.426189\n",
            "Epoch #29 - Duration: 0:00:00.419093\n",
            "Epoch #30 - Duration: 0:00:00.399184\n",
            "Epoch #31 - Duration: 0:00:00.414229\n",
            "Epoch #32 - Duration: 0:00:00.418574\n",
            "Epoch #33 - Duration: 0:00:00.414761\n",
            "Epoch #34 - Duration: 0:00:00.413072\n",
            "Epoch #35 - Duration: 0:00:00.417299\n",
            "Epoch #36 - Duration: 0:00:00.448165\n",
            "Epoch #37 - Duration: 0:00:00.408243\n",
            "Epoch #38 - Duration: 0:00:00.413488\n",
            "Epoch #39 - Duration: 0:00:00.394819\n",
            "Epoch #40 - Duration: 0:00:00.399758\n",
            "Epoch #41 - Duration: 0:00:00.417375\n",
            "Epoch #42 - Duration: 0:00:00.408031\n",
            "Epoch #43 - Duration: 0:00:00.413403\n",
            "Epoch #44 - Duration: 0:00:00.399796\n",
            "Epoch #45 - Duration: 0:00:00.412899\n",
            "Epoch #46 - Duration: 0:00:00.416528\n",
            "Epoch #47 - Duration: 0:00:00.408325\n",
            "Epoch #48 - Duration: 0:00:00.415510\n",
            "Epoch #49 - Duration: 0:00:00.410494\n"
          ]
        }
      ],
      "source": [
        "epoch_logger = EpochLogger()\n",
        "model.train(train_corpus, total_examples=model.corpus_count,  epochs=model.epochs, callbacks=[epoch_logger])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(\"Doc2Vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "#model.load(\"Doc2Vec.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assesing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "      <th>inferred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>evidence-0</th>\n",
              "      <td>john bennet lawes, english entrepreneur and ag...</td>\n",
              "      <td>[john, bennet, lawes, english, entrepreneur, a...</td>\n",
              "      <td>[-0.23991504, 0.51219636, 0.05094586, 0.195069...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1</th>\n",
              "      <td>lindberg began his professional career at the ...</td>\n",
              "      <td>[lindberg, began, his, professional, career, a...</td>\n",
              "      <td>[-1.3084338, -0.12524424, -0.95123047, 0.55854...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-2</th>\n",
              "      <td>``boston (ladies of cambridge)'' by vampire we...</td>\n",
              "      <td>[boston, ladies, of, cambridge, by, vampire, w...</td>\n",
              "      <td>[-0.2850507, -0.10411729, -0.35949567, 0.23621...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-3</th>\n",
              "      <td>gerald francis goyer (born october 20, 1936) w...</td>\n",
              "      <td>[gerald, francis, goyer, born, october, 20, 19...</td>\n",
              "      <td>[-0.5594862, -0.7934478, 0.57019496, -0.384542...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-4</th>\n",
              "      <td>he detected abnormalities of oxytocinergic fun...</td>\n",
              "      <td>[he, detected, abnormalities, of, oxytocinergi...</td>\n",
              "      <td>[-0.04946132, 0.110815756, -0.44829205, -0.211...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208822</th>\n",
              "      <td>also on the property is a contributing garage ...</td>\n",
              "      <td>[also, on, the, property, is, a, contributing,...</td>\n",
              "      <td>[0.0019220194, 0.084965706, -0.4861797, 0.3197...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208823</th>\n",
              "      <td>| class = ``fn org'' | fyrde | | | | 6110 | | ...</td>\n",
              "      <td>[class, fn, org, fyrde, 6110, volda]</td>\n",
              "      <td>[0.0010043998, -0.026967091, -0.020551557, 0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208824</th>\n",
              "      <td>dragon storm (game), a role-playing game and c...</td>\n",
              "      <td>[dragon, storm, game, a, game, and, collectibl...</td>\n",
              "      <td>[-0.3721467, 0.26059905, 0.22694564, -0.447193...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208825</th>\n",
              "      <td>it states that the zeriuani ``which is so grea...</td>\n",
              "      <td>[it, states, that, the, zeriuani, which, is, s...</td>\n",
              "      <td>[0.24094175, -0.4339357, 0.10777598, 0.4715772...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208826</th>\n",
              "      <td>the storyline revolves around a giant plesiosa...</td>\n",
              "      <td>[the, storyline, revolves, around, a, giant, p...</td>\n",
              "      <td>[1.6479698, 0.08338774, 0.46844044, 0.72938865...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1208827 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                               text  \\\n",
              "evidence-0        john bennet lawes, english entrepreneur and ag...   \n",
              "evidence-1        lindberg began his professional career at the ...   \n",
              "evidence-2        ``boston (ladies of cambridge)'' by vampire we...   \n",
              "evidence-3        gerald francis goyer (born october 20, 1936) w...   \n",
              "evidence-4        he detected abnormalities of oxytocinergic fun...   \n",
              "...                                                             ...   \n",
              "evidence-1208822  also on the property is a contributing garage ...   \n",
              "evidence-1208823  | class = ``fn org'' | fyrde | | | | 6110 | | ...   \n",
              "evidence-1208824  dragon storm (game), a role-playing game and c...   \n",
              "evidence-1208825  it states that the zeriuani ``which is so grea...   \n",
              "evidence-1208826  the storyline revolves around a giant plesiosa...   \n",
              "\n",
              "                                                             tokens  \\\n",
              "evidence-0        [john, bennet, lawes, english, entrepreneur, a...   \n",
              "evidence-1        [lindberg, began, his, professional, career, a...   \n",
              "evidence-2        [boston, ladies, of, cambridge, by, vampire, w...   \n",
              "evidence-3        [gerald, francis, goyer, born, october, 20, 19...   \n",
              "evidence-4        [he, detected, abnormalities, of, oxytocinergi...   \n",
              "...                                                             ...   \n",
              "evidence-1208822  [also, on, the, property, is, a, contributing,...   \n",
              "evidence-1208823               [class, fn, org, fyrde, 6110, volda]   \n",
              "evidence-1208824  [dragon, storm, game, a, game, and, collectibl...   \n",
              "evidence-1208825  [it, states, that, the, zeriuani, which, is, s...   \n",
              "evidence-1208826  [the, storyline, revolves, around, a, giant, p...   \n",
              "\n",
              "                                                           inferred  \n",
              "evidence-0        [-0.23991504, 0.51219636, 0.05094586, 0.195069...  \n",
              "evidence-1        [-1.3084338, -0.12524424, -0.95123047, 0.55854...  \n",
              "evidence-2        [-0.2850507, -0.10411729, -0.35949567, 0.23621...  \n",
              "evidence-3        [-0.5594862, -0.7934478, 0.57019496, -0.384542...  \n",
              "evidence-4        [-0.04946132, 0.110815756, -0.44829205, -0.211...  \n",
              "...                                                             ...  \n",
              "evidence-1208822  [0.0019220194, 0.084965706, -0.4861797, 0.3197...  \n",
              "evidence-1208823  [0.0010043998, -0.026967091, -0.020551557, 0.1...  \n",
              "evidence-1208824  [-0.3721467, 0.26059905, 0.22694564, -0.447193...  \n",
              "evidence-1208825  [0.24094175, -0.4339357, 0.10777598, 0.4715772...  \n",
              "evidence-1208826  [1.6479698, 0.08338774, 0.46844044, 0.72938865...  \n",
              "\n",
              "[1208827 rows x 3 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evidences_df = pd.DataFrame.from_dict(evidences, orient='index', columns=['text'])\n",
        "evidences_df = tokenize_text(evidences_df,'text')\n",
        "evidences_df['inferred'] = evidences_df['tokens'].apply(lambda x: model.infer_vector(x))\n",
        "evidences_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "evidences_df.to_pickle('evidences_df.pkl')\n",
        "#evidences_df = pd.read_pickle('evidences_df.pkl')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_label</th>\n",
              "      <th>evidences</th>\n",
              "      <th>tokens</th>\n",
              "      <th>inferred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-442946, evidence-1194317, evidence-1...</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>[-0.80932164, -0.08572763, -0.3182288, -0.2329...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>el niño drove record highs in global temperatu...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[evidence-338219, evidence-1127398]</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "      <td>[-0.3487168, -0.09100804, -0.65324616, -0.2439...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>in 1946, pdo switched to a cool phase.</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-530063, evidence-984887]</td>\n",
              "      <td>[in, 1946, pdo, switched, to, a, cool, phase]</td>\n",
              "      <td>[-0.11134084, -0.29519156, -0.43432373, 0.0635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>weather channel co-founder john coleman provid...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-1177431, evidence-782448, evidence-5...</td>\n",
              "      <td>[weather, channel, john, coleman, provided, ev...</td>\n",
              "      <td>[-0.12073334, -0.11993397, -0.32204187, 0.2162...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"january 2008 capped a 12 month period of glob...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-1010750, evidence-91661, evidence-72...</td>\n",
              "      <td>[january, 2008, capped, a, 12, month, period, ...</td>\n",
              "      <td>[-0.8894794, 0.098310165, -0.9383252, 0.193941...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1504</th>\n",
              "      <td>climate scientists say that aspects of the cas...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-1055682, evidence-1047356, evidence-...</td>\n",
              "      <td>[climate, scientists, say, that, aspects, of, ...</td>\n",
              "      <td>[-0.3415583, -0.087080225, -0.3509223, 0.34327...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-243</th>\n",
              "      <td>in its 5th assessment report in 2013, the ipcc...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-916755]</td>\n",
              "      <td>[in, its, 5th, assessment, report, in, 2013, t...</td>\n",
              "      <td>[0.21890192, 0.038463153, -1.2138172, -0.93325...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2302</th>\n",
              "      <td>since the mid 1970s, global temperatures have ...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-403673, evidence-889933, evidence-11...</td>\n",
              "      <td>[since, the, mid, 1970s, global, temperatures,...</td>\n",
              "      <td>[-0.8260765, -0.086758286, -0.24591607, 0.3731...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-502</th>\n",
              "      <td>but abnormal temperature spikes in february an...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-97375, evidence-562427, evidence-521...</td>\n",
              "      <td>[but, abnormal, temperature, spikes, in, febru...</td>\n",
              "      <td>[-1.3430239, -0.19148764, -0.87164366, 0.09012...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-3093</th>\n",
              "      <td>sending oscillating microwaves from an antenna...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-971105, evidence-457769, evidence-29...</td>\n",
              "      <td>[sending, oscillating, microwaves, from, an, a...</td>\n",
              "      <td>[-0.6231345, -0.6301942, -1.0201, 0.15725291, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1228 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-1937  not only is there no scientific evidence that ...   \n",
              "claim-126   el niño drove record highs in global temperatu...   \n",
              "claim-2510             in 1946, pdo switched to a cool phase.   \n",
              "claim-2021  weather channel co-founder john coleman provid...   \n",
              "claim-2449  \"january 2008 capped a 12 month period of glob...   \n",
              "...                                                       ...   \n",
              "claim-1504  climate scientists say that aspects of the cas...   \n",
              "claim-243   in its 5th assessment report in 2013, the ipcc...   \n",
              "claim-2302  since the mid 1970s, global temperatures have ...   \n",
              "claim-502   but abnormal temperature spikes in february an...   \n",
              "claim-3093  sending oscillating microwaves from an antenna...   \n",
              "\n",
              "                claim_label  \\\n",
              "claim-1937         DISPUTED   \n",
              "claim-126           REFUTES   \n",
              "claim-2510         SUPPORTS   \n",
              "claim-2021         DISPUTED   \n",
              "claim-2449  NOT_ENOUGH_INFO   \n",
              "...                     ...   \n",
              "claim-1504         SUPPORTS   \n",
              "claim-243          SUPPORTS   \n",
              "claim-2302  NOT_ENOUGH_INFO   \n",
              "claim-502   NOT_ENOUGH_INFO   \n",
              "claim-3093         SUPPORTS   \n",
              "\n",
              "                                                    evidences  \\\n",
              "claim-1937  [evidence-442946, evidence-1194317, evidence-1...   \n",
              "claim-126                 [evidence-338219, evidence-1127398]   \n",
              "claim-2510                 [evidence-530063, evidence-984887]   \n",
              "claim-2021  [evidence-1177431, evidence-782448, evidence-5...   \n",
              "claim-2449  [evidence-1010750, evidence-91661, evidence-72...   \n",
              "...                                                       ...   \n",
              "claim-1504  [evidence-1055682, evidence-1047356, evidence-...   \n",
              "claim-243                                   [evidence-916755]   \n",
              "claim-2302  [evidence-403673, evidence-889933, evidence-11...   \n",
              "claim-502   [evidence-97375, evidence-562427, evidence-521...   \n",
              "claim-3093  [evidence-971105, evidence-457769, evidence-29...   \n",
              "\n",
              "                                                       tokens  \\\n",
              "claim-1937  [not, only, is, there, no, scientific, evidenc...   \n",
              "claim-126   [el, niño, drove, record, highs, in, global, t...   \n",
              "claim-2510      [in, 1946, pdo, switched, to, a, cool, phase]   \n",
              "claim-2021  [weather, channel, john, coleman, provided, ev...   \n",
              "claim-2449  [january, 2008, capped, a, 12, month, period, ...   \n",
              "...                                                       ...   \n",
              "claim-1504  [climate, scientists, say, that, aspects, of, ...   \n",
              "claim-243   [in, its, 5th, assessment, report, in, 2013, t...   \n",
              "claim-2302  [since, the, mid, 1970s, global, temperatures,...   \n",
              "claim-502   [but, abnormal, temperature, spikes, in, febru...   \n",
              "claim-3093  [sending, oscillating, microwaves, from, an, a...   \n",
              "\n",
              "                                                     inferred  \n",
              "claim-1937  [-0.80932164, -0.08572763, -0.3182288, -0.2329...  \n",
              "claim-126   [-0.3487168, -0.09100804, -0.65324616, -0.2439...  \n",
              "claim-2510  [-0.11134084, -0.29519156, -0.43432373, 0.0635...  \n",
              "claim-2021  [-0.12073334, -0.11993397, -0.32204187, 0.2162...  \n",
              "claim-2449  [-0.8894794, 0.098310165, -0.9383252, 0.193941...  \n",
              "...                                                       ...  \n",
              "claim-1504  [-0.3415583, -0.087080225, -0.3509223, 0.34327...  \n",
              "claim-243   [0.21890192, 0.038463153, -1.2138172, -0.93325...  \n",
              "claim-2302  [-0.8260765, -0.086758286, -0.24591607, 0.3731...  \n",
              "claim-502   [-1.3430239, -0.19148764, -0.87164366, 0.09012...  \n",
              "claim-3093  [-0.6231345, -0.6301942, -1.0201, 0.15725291, ...  \n",
              "\n",
              "[1228 rows x 5 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "claims_df = pd.DataFrame.from_dict(claims, orient='index')\n",
        "claims_df = tokenize_text(claims_df,'claim_text')\n",
        "claims_df['inferred'] = claims_df['tokens'].apply(lambda x: model.infer_vector(x))\n",
        "claims_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "claims_df.to_pickle('claims_df.pkl')\n",
        "#claims_df = pd.read_pickle('claims_df.pkl')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implement BM25 Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize BM25 model\n",
        "bm25 = BM25Okapi(evidences_df['tokens'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate BM25 scores for each claim\n",
        "def calculate_bm25_scores(query_tokens):\n",
        "    return bm25.get_scores(query_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the BM25 scores\n"
          ]
        }
      ],
      "source": [
        "print(\"Generating the BM25 scores\")\n",
        "# Compute BM25 scores\n",
        "#bm25_scores = claims_df['tokens'].apply(calculate_bm25_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating the similarities\n"
          ]
        }
      ],
      "source": [
        "# Extract lists\n",
        "claim_vectors = claims_df['inferred'].to_list()\n",
        "evidence_vectors = evidences_df['inferred'].to_list()\n",
        "\n",
        "print(\"Generating the similarities\")\n",
        "# Calculate Doc2Vec similarities\n",
        "doc2vec_similarities = cosine_similarity(claim_vectors, evidence_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#bm25_scores.to_pickle('bm25_scores.pkl')\n",
        "bm25_scores = pd.read_pickle('bm25_scores.pkl')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('doc2vec_similarities.pkl','wb') as f: pickle.dump(doc2vec_similarities, f)\n",
        "#with open('doc2vec_similarities.pkl','rb') as f: doc2vec_similarities = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize the lengths\n",
        "def normalize(scores):\n",
        "    return (scores - np.min(scores)) / (np.max(scores) - np.min(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "document_lengths = [len(doc) for doc in evidences_df['tokens']]\n",
        "normalized_lengths = normalize(document_lengths)\n",
        "p_normalized_lengths = 0.2 * normalized_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_bm25_scores = [normalize(doc) for doc in bm25_scores]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_doc2vec_similarities = normalize(doc2vec_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_bm25_scores = [0.4 * doc for doc in n_bm25_scores]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_doc2vec_similarities = [0.4 * doc for doc in n_doc2vec_similarities]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del document_lengths, normalized_lengths\n",
        "del bm25, n_bm25_scores\n",
        "del doc2vec_similarities, n_doc2vec_similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(p_normalized_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_similarities = [(sim + p_normalized_lengths) for sim in p_doc2vec_similarities]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine scores with weights\n",
        "combined_scores = p_bm25_scores + p_doc2vec_similarities + p_normalized_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rank evidences\n",
        "ranked_indices = np.argsort(combined_scores)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Prepare to collect top values, their indices, names\n",
        "top_values_per_column = []\n",
        "top_indices_per_column = []\n",
        "top_evidence_names_per_column = []\n",
        "\n",
        "\n",
        "for i in range(similarity_matrix.shape[1]):  # Iterate over columns\n",
        "    column = similarity_matrix[:, i]\n",
        "    indices = np.argpartition(column, -5)[-5:]\n",
        "    sorted_indices = indices[np.argsort(column[indices])][::-1]  \n",
        "    sorted_indices = sorted_indices.astype(int)\n",
        "    evidences = []\n",
        "    for index in sorted_indices:\n",
        "        evidences.append(evidences_df.index[index])\n",
        "    \n",
        "    top_indices_per_column.append(sorted_indices)\n",
        "    top_values_per_column.append(column[sorted_indices])\n",
        "    top_evidence_names_per_column.append(evidences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the results\n",
        "predictions = {}\n",
        "\n",
        "for i, (values, indices, names) in enumerate(zip(top_values_per_column, top_indices_per_column, top_evidence_names_per_column)):\n",
        "    predictions[claims_df.index[i]] = names\n",
        "\n",
        "doc2vec = pd.DataFrame.from_dict(predictions, orient='index')\n",
        "doc2vec['predictions'] = doc2vec[[0, 1, 2, 3, 4]].agg(list, axis=1)\n",
        "\n",
        "train_df_doc2vec = claims_df.copy()\n",
        "train_df_doc2vec = train_df_doc2vec.drop('evidences', axis=1)\n",
        "train_df_doc2vec = train_df_doc2vec.drop('tokens', axis=1)\n",
        "train_df_doc2vec = train_df_doc2vec.drop('inferred', axis=1)\n",
        "train_df_doc2vec['predictions'] = doc2vec['predictions']\n",
        "train_df_doc2vec = train_df_doc2vec.rename(columns={'predictions': 'evidences'})\n",
        "\n",
        "# Save the DataFrame to a Pickle file\n",
        "train_df_doc2vec.to_pickle('dfDoc2VecPredictions.pkl')\n",
        "train_df_doc2vec.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the DataFrame to a JSON file\n",
        "train_df_doc2vec.to_json('../data/train_claims_doc2vec.json', orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dev_df = pd.DataFrame.from_dict(dev_claims, orient='index')\n",
        "dev_df = tokenize_text(dev_df,'claim_text')\n",
        "dev_df['inferred'] = dev_df['tokens'].apply(lambda x: model.infer_vector(x))\n",
        "dev_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "claim_dev_vectors = dev_df['inferred'].to_list()\n",
        "similarity_dev_matrix = cosine_similarity(evidence_vectors, claim_dev_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "similarity_dev_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare to collect top values, their indices, names\n",
        "dev_top_values_per_column = []\n",
        "dev_top_indices_per_column = []\n",
        "dev_top_evidence_names_per_column = []\n",
        "\n",
        "\n",
        "for i in range(similarity_dev_matrix.shape[1]):  # Iterate over columns\n",
        "    column = similarity_dev_matrix[:, i]\n",
        "    indices = np.argpartition(column, -5)[-5:]\n",
        "    sorted_indices = indices[np.argsort(column[indices])][::-1]  \n",
        "    sorted_indices = sorted_indices.astype(int)\n",
        "    evidences = []\n",
        "    for index in sorted_indices:\n",
        "        evidences.append(evidences_df.index[index])\n",
        "    \n",
        "    dev_top_indices_per_column.append(sorted_indices)\n",
        "    dev_top_values_per_column.append(column[sorted_indices])\n",
        "    dev_top_evidence_names_per_column.append(evidences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the results\n",
        "dev_predictions = {}\n",
        "\n",
        "for i, (values, indices, names) in enumerate(zip(dev_top_values_per_column, dev_top_indices_per_column, dev_top_evidence_names_per_column)):\n",
        "    dev_predictions[dev_df.index[i]] = names\n",
        "\n",
        "dev_doc2vec = pd.DataFrame.from_dict(dev_predictions, orient='index')\n",
        "dev_doc2vec['predictions'] = dev_doc2vec[[0, 1, 2, 3, 4]].agg(list, axis=1)\n",
        "\n",
        "dev_df_doc2vec = dev_df.copy()\n",
        "dev_df_doc2vec = dev_df_doc2vec.drop('tokens', axis=1)\n",
        "dev_df_doc2vec = dev_df_doc2vec.drop('inferred', axis=1)\n",
        "dev_df_doc2vec = dev_df_doc2vec.drop('evidences', axis=1)\n",
        "dev_df_doc2vec['predictions'] = dev_doc2vec['predictions']\n",
        "dev_df_doc2vec = dev_df_doc2vec.rename(columns={'predictions': 'evidences'})\n",
        "\n",
        "# Save the DataFrame to a Pickle file\n",
        "dev_df_doc2vec.to_pickle('dfdevDoc2VecDevPredictions.pkl')\n",
        "dev_df_doc2vec.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export the DataFrame to a JSON file\n",
        "dev_df_doc2vec.to_json('../data/dev_claims_doc2vec.json', orient='index')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vec_for_learning(model, sents):\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
        "    return targets, regressors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg = LogisticRegression(n_jobs=cores, C=1e5, max_iter=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train, X_train = vec_for_learning(model, train_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_dev, X_dev = vec_for_learning(model, dev_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_dev)\n",
        "print('Testing accuracy %s' % accuracy_score(y_dev, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_dev, y_pred, average='weighted')))\n",
        "report = classification_report(y_dev, y_pred)\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read dev claims\n",
        "with open('../data/dev-claims.json', 'r') as f:\n",
        "    dev_claims = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
