{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
        "    return e_x / e_x.sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Doc2Vec:    \n",
        "    def __init__(self, num_docs, vocab_size, vector_size, learning_rate=0.01):\n",
        "        self.num_docs = num_docs\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vector_size = vector_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Random initialization of weights\n",
        "        self.word_vectors = np.random.rand(vocab_size, vector_size)\n",
        "        self.doc_vectors = np.random.rand(num_docs, vector_size)  # Vector for each document\n",
        "    \n",
        "    def train(self, dataset, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            for doc_id, target, context in dataset:\n",
        "                # Forward pass: Calculate combined vector and predict probabilities\n",
        "                context_vector = np.mean(self.word_vectors[context], axis=0)\n",
        "                combined_vector = (context_vector + self.doc_vectors[doc_id]) / 2\n",
        "                \n",
        "                scores = np.dot(self.word_vectors, combined_vector)\n",
        "                probs = softmax(scores)\n",
        "                \n",
        "                # Compute cross-entropy loss\n",
        "                loss = -np.log(probs[target])\n",
        "                if epoch % 100 == 0 and doc_id == 0:  # Example printout for monitoring, from the first document\n",
        "                    print(f\"Epoch {epoch}, Document {doc_id}, Loss: {loss}\")\n",
        "                \n",
        "                # Backward pass: Compute gradients\n",
        "                dL_dy = probs\n",
        "                dL_dy[target] -= 1  # dL/dy where y is the softmax output\n",
        "\n",
        "                dL_dv = np.outer(dL_dy, combined_vector)  # Gradient w.r.t. word vectors\n",
        "\n",
        "                # Update word vectors\n",
        "                self.word_vectors -= self.learning_rate * dL_dv\n",
        "                \n",
        "                # Gradient w.r.t. combined vector\n",
        "                dL_dcombined_vector = np.dot(self.word_vectors.T, dL_dy)\n",
        "\n",
        "                # Update context words\n",
        "                for idx in context:\n",
        "                    self.word_vectors[idx] -= self.learning_rate * dL_dcombined_vector / len(context)\n",
        "\n",
        "                # Update document vector for specific document\n",
        "                self.doc_vectors[doc_id] -= self.learning_rate * dL_dcombined_vector\n",
        "\n",
        "    def get_document_vector(self, doc_id):\n",
        "        return self.doc_vectors[doc_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "num_docs = 3\n",
        "vocab_size = 100\n",
        "vector_size = 50\n",
        "\n",
        "model = Doc2Vec(num_docs, vocab_size, vector_size)\n",
        "\n",
        "# Example data: list of tuples (doc_id, target_word_index, context_indices)\n",
        "dataset = [\n",
        "    (0, 10, [1, 2, 3]),\n",
        "    (1, 20, [21, 22, 23]),\n",
        "    (2, 30, [31, 32, 33])\n",
        "]\n",
        "\n",
        "model.train(dataset, epochs=1000)\n",
        "\n",
        "for i in range(num_docs):\n",
        "    print(f\"Document {i} vector:\", model.get_document_vector(i))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
