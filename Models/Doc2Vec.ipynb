{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://github.com/cbowdon/doc2vec-pytorch/blob/master/doc2vec.ipynb\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam  # ilenic uses Adam, but gensim uses plain SGD\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import altair as alt\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read train claims\n",
        "with open('../data/train-claims.json', 'r') as f:\n",
        "    claims = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1228"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(claims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read evidence\n",
        "with open('../data/evidence.json', 'r') as f:\n",
        "    evidences = json.load(f)\n",
        "evidences = {i: str.lower(j) for i,j in evidences.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1208827"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(evidences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'at very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evidences['evidence-442946']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all texts from claims\n",
        "corpus = {}\n",
        "for id, claim in claims.items():\n",
        "    corpus[id] = str.strip(str.lower(claim['claim_text']))  # Add claim text\n",
        "\n",
        "for id, evidence in evidences.items():\n",
        "    corpus[id] = str.strip(evidence)  # Add evidence text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_text(df):\n",
        "    #df[\"tokens\"] = df.text.apply(lambda x: [token.text.strip() for token in word_tokenize(x) if token.text.isalnum()])\n",
        "    #df['tokens'] = df['text'].apply(lambda x: [token for token in word_tokenize(x) if token.isalnum()])\n",
        "    df['tokens'] = df['text'].apply(lambda x: [token for token in word_tokenize(x) if token.isalpha()])\n",
        "    df[\"length\"] = df.tokens.apply(len)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the list of documents into a pandas DataFrame\n",
        "df = pd.DataFrame.from_dict(corpus, orient='index', columns=['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>el niño drove record highs in global temperatu...</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>in 1946, pdo switched to a cool phase.</td>\n",
              "      <td>[in, pdo, switched, to, a, cool, phase]</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>weather channel co-founder john coleman provid...</td>\n",
              "      <td>[weather, channel, john, coleman, provided, ev...</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"january 2008 capped a 12 month period of glob...</td>\n",
              "      <td>[january, capped, a, month, period, of, global...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208822</th>\n",
              "      <td>also on the property is a contributing garage ...</td>\n",
              "      <td>[also, on, the, property, is, a, contributing,...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208823</th>\n",
              "      <td>| class = ``fn org'' | fyrde | | | | 6110 | | ...</td>\n",
              "      <td>[class, fn, org, fyrde, volda]</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208824</th>\n",
              "      <td>dragon storm (game), a role-playing game and c...</td>\n",
              "      <td>[dragon, storm, game, a, game, and, collectibl...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208825</th>\n",
              "      <td>it states that the zeriuani ``which is so grea...</td>\n",
              "      <td>[it, states, that, the, zeriuani, which, is, s...</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidence-1208826</th>\n",
              "      <td>the storyline revolves around a giant plesiosa...</td>\n",
              "      <td>[the, storyline, revolves, around, a, giant, p...</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1210055 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                               text  \\\n",
              "claim-1937        not only is there no scientific evidence that ...   \n",
              "claim-126         el niño drove record highs in global temperatu...   \n",
              "claim-2510                   in 1946, pdo switched to a cool phase.   \n",
              "claim-2021        weather channel co-founder john coleman provid...   \n",
              "claim-2449        \"january 2008 capped a 12 month period of glob...   \n",
              "...                                                             ...   \n",
              "evidence-1208822  also on the property is a contributing garage ...   \n",
              "evidence-1208823  | class = ``fn org'' | fyrde | | | | 6110 | | ...   \n",
              "evidence-1208824  dragon storm (game), a role-playing game and c...   \n",
              "evidence-1208825  it states that the zeriuani ``which is so grea...   \n",
              "evidence-1208826  the storyline revolves around a giant plesiosa...   \n",
              "\n",
              "                                                             tokens  length  \n",
              "claim-1937        [not, only, is, there, no, scientific, evidenc...      22  \n",
              "claim-126         [el, niño, drove, record, highs, in, global, t...      16  \n",
              "claim-2510                  [in, pdo, switched, to, a, cool, phase]       7  \n",
              "claim-2021        [weather, channel, john, coleman, provided, ev...      15  \n",
              "claim-2449        [january, capped, a, month, period, of, global...      17  \n",
              "...                                                             ...     ...  \n",
              "evidence-1208822  [also, on, the, property, is, a, contributing,...       9  \n",
              "evidence-1208823                     [class, fn, org, fyrde, volda]       5  \n",
              "evidence-1208824  [dragon, storm, game, a, game, and, collectibl...       9  \n",
              "evidence-1208825  [it, states, that, the, zeriuani, which, is, s...      46  \n",
              "evidence-1208826  [the, storyline, revolves, around, a, giant, p...      36  \n",
              "\n",
              "[1210055 rows x 3 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = tokenize_text(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, all_tokens, min_count=2):\n",
        "        self.min_count = min_count\n",
        "        # Only includes the word when has more than 1 ocurrence\n",
        "        self.freqs = {t:n for t, n in Counter(all_tokens).items() if n >= min_count}\n",
        "        self.words = sorted(self.freqs.keys())\n",
        "        self.word2idx = {w: i for i, w in enumerate(self.words)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset comprises 1210055 documents and 232159 unique words (over the limit of 2 occurrences)\n"
          ]
        }
      ],
      "source": [
        "# Create the vocabulary\n",
        "vocab = Vocab([tok for tokens in df.tokens for tok in tokens], min_count=2)\n",
        "print(f\"Dataset comprises {len(df)} documents and {len(vocab.words)} unique words (over the limit of {vocab.min_count} occurrences)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_tokens(df, vocab):\n",
        "    df[\"length\"] = df.tokens.apply(len)\n",
        "    df[\"clean_tokens\"] = df.tokens.apply(lambda x: [t for t in x if t in vocab.freqs.keys()])\n",
        "    df[\"clean_length\"] = df.clean_tokens.apply(len)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "      <th>length</th>\n",
              "      <th>clean_tokens</th>\n",
              "      <th>clean_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>22</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>el niño drove record highs in global temperatu...</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "      <td>16</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>in 1946, pdo switched to a cool phase.</td>\n",
              "      <td>[in, pdo, switched, to, a, cool, phase]</td>\n",
              "      <td>7</td>\n",
              "      <td>[in, pdo, switched, to, a, cool, phase]</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>weather channel co-founder john coleman provid...</td>\n",
              "      <td>[weather, channel, john, coleman, provided, ev...</td>\n",
              "      <td>15</td>\n",
              "      <td>[weather, channel, john, coleman, provided, ev...</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"january 2008 capped a 12 month period of glob...</td>\n",
              "      <td>[january, capped, a, month, period, of, global...</td>\n",
              "      <td>17</td>\n",
              "      <td>[january, capped, a, month, period, of, global...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         text  \\\n",
              "claim-1937  not only is there no scientific evidence that ...   \n",
              "claim-126   el niño drove record highs in global temperatu...   \n",
              "claim-2510             in 1946, pdo switched to a cool phase.   \n",
              "claim-2021  weather channel co-founder john coleman provid...   \n",
              "claim-2449  \"january 2008 capped a 12 month period of glob...   \n",
              "\n",
              "                                                       tokens  length  \\\n",
              "claim-1937  [not, only, is, there, no, scientific, evidenc...      22   \n",
              "claim-126   [el, niño, drove, record, highs, in, global, t...      16   \n",
              "claim-2510            [in, pdo, switched, to, a, cool, phase]       7   \n",
              "claim-2021  [weather, channel, john, coleman, provided, ev...      15   \n",
              "claim-2449  [january, capped, a, month, period, of, global...      17   \n",
              "\n",
              "                                                 clean_tokens  clean_length  \n",
              "claim-1937  [not, only, is, there, no, scientific, evidenc...            22  \n",
              "claim-126   [el, niño, drove, record, highs, in, global, t...            16  \n",
              "claim-2510            [in, pdo, switched, to, a, cool, phase]             7  \n",
              "claim-2021  [weather, channel, john, coleman, provided, ev...            15  \n",
              "claim-2449  [january, capped, a, month, period, of, global...            17  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = clean_tokens(df, vocab)\n",
        "df[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Doc2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NegativeSampling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NegativeSampling, self).__init__()\n",
        "        self.log_sigmoid = nn.LogSigmoid()\n",
        "\n",
        "    def forward(self, scores):\n",
        "        batch_size = scores.shape[0]\n",
        "        positive = self.log_sigmoid(scores[:,0])\n",
        "        negatives = torch.sum(self.log_sigmoid(-scores[:,1:]), dim=1)\n",
        "        return -torch.sum(positive + negatives) / batch_size  # average for batch\n",
        "\n",
        "loss = NegativeSampling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NoiseDistribution:\n",
        "    def __init__(self, vocab):\n",
        "        self.probs = np.array([vocab.freqs[w] for w in vocab.words])\n",
        "        # A unigram distribution raised to the 3/4rd power, as proposed by T. Mikolov et al. in Distributed Representations of Words and Phrases and their Compositionality\n",
        "        self.probs = np.power(self.probs, 0.75)\n",
        "        self.probs /= np.sum(self.probs)\n",
        "    def sample(self, n):\n",
        "        \"Returns the indices of n words randomly sampled from the vocabulary.\"\n",
        "        return np.random.choice(a=self.probs.shape[0], size=n, p=self.probs)\n",
        "        \n",
        "noise = NoiseDistribution(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_size=5\n",
        "n_negative_samples=5\n",
        "\n",
        "doc = df[:1]\n",
        "for i in range(context_size, len(doc.clean_tokens) - context_size):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def example_generator(df, context_size, noise, n_negative_samples, vocab):\n",
        "    for doc_id, doc in df.iterrows():\n",
        "        for i in range(context_size, len(doc.clean_tokens) - context_size):\n",
        "            positive_sample = vocab.word2idx[doc.clean_tokens[i]]\n",
        "            sample_ids = noise.sample(n_negative_samples).tolist()\n",
        "            # Fix a wee bug - ensure negative samples don't accidentally include the positive\n",
        "            sample_ids = [sample_id if sample_id != positive_sample else -1 for sample_id in sample_ids]\n",
        "            sample_ids.insert(0, positive_sample)\n",
        "            context = doc.clean_tokens[i - context_size:i] + doc.clean_tokens[i + 1:i + context_size + 1]\n",
        "            context_ids = [vocab.word2idx[w] for w in context]\n",
        "            yield {\"doc_ids\": torch.tensor(doc_id),  # we use plural here because it will be batched\n",
        "                   \"sample_ids\": torch.tensor(sample_ids), \n",
        "                   \"context_ids\": torch.tensor(context_ids)}\n",
        "            \n",
        "examples = example_generator(df, context_size=5, noise=noise, n_negative_samples=5, vocab=vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "new(): invalid data type 'str'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[57], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples[index]\n\u001b[1;32m----> 9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNCEDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "Cell \u001b[1;32mIn[57], line 3\u001b[0m, in \u001b[0;36mNCEDataset.__init__\u001b[1;34m(self, examples)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, examples):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[55], line 11\u001b[0m, in \u001b[0;36mexample_generator\u001b[1;34m(df, context_size, noise, n_negative_samples, vocab)\u001b[0m\n\u001b[0;32m      9\u001b[0m context \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mclean_tokens[i \u001b[38;5;241m-\u001b[39m context_size:i] \u001b[38;5;241m+\u001b[39m doc\u001b[38;5;241m.\u001b[39mclean_tokens[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:i \u001b[38;5;241m+\u001b[39m context_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     10\u001b[0m context_ids \u001b[38;5;241m=\u001b[39m [vocab\u001b[38;5;241m.\u001b[39mword2idx[w] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m context]\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# we use plural here because it will be batched\u001b[39;00m\n\u001b[0;32m     12\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(sample_ids), \n\u001b[0;32m     13\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(context_ids)}\n",
            "\u001b[1;31mTypeError\u001b[0m: new(): invalid data type 'str'"
          ]
        }
      ],
      "source": [
        "class NCEDataset(Dataset):\n",
        "    def __init__(self, examples):\n",
        "        self.examples = list(examples)\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "    def __getitem__(self, index):\n",
        "        return self.examples[index]\n",
        "    \n",
        "dataset = NCEDataset(examples)\n",
        "dataloader = DataLoader(dataset, batch_size=2, drop_last=True, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def describe_batch(batch, vocab):\n",
        "    results = []\n",
        "    for doc_id, context_ids, sample_ids in zip(batch[\"doc_ids\"], batch[\"context_ids\"], batch[\"sample_ids\"]):\n",
        "        context = [vocab.words[i] for i in context_ids]\n",
        "        context.insert(len(context_ids) // 2, \"____\")\n",
        "        samples = [vocab.words[i] for i in sample_ids]\n",
        "        result = {\"doc_id\": doc_id,\n",
        "                  \"context\": \" \".join(context), \n",
        "                  \"context_ids\": context_ids, \n",
        "                  \"samples\": samples, \n",
        "                  \"sample_ids\": sample_ids}\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "describe_batch(next(iter(dataloader)), vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DistributedMemory(nn.Module):\n",
        "    def __init__(self, vec_dim, n_docs, n_words):\n",
        "        super(DistributedMemory, self).__init__()\n",
        "        self.paragraph_matrix = nn.Parameter(torch.randn(n_docs, vec_dim))\n",
        "        self.word_matrix = nn.Parameter(torch.randn(n_words, vec_dim))\n",
        "        self.outputs = nn.Parameter(torch.zeros(vec_dim, n_words))\n",
        "    \n",
        "    def forward(self, doc_ids, context_ids, sample_ids):\n",
        "                                                                               # first add doc ids to context word ids to make the inputs\n",
        "        inputs = torch.add(self.paragraph_matrix[doc_ids,:],                   # (batch_size, vec_dim)\n",
        "                           torch.sum(self.word_matrix[context_ids,:], dim=1))  # (batch_size, 2x context, vec_dim) -> sum to (batch_size, vec_dim)\n",
        "                                                                               #\n",
        "                                                                               # select the subset of the output layer for the NCE test\n",
        "        outputs = self.outputs[:,sample_ids]                                   # (vec_dim, batch_size, n_negative_samples + 1)\n",
        "                                                                               #\n",
        "        return torch.bmm(inputs.unsqueeze(dim=1),                              # then multiply with some munging to make the tensor shapes line up \n",
        "                         outputs.permute(1, 0, 2)).squeeze()                   # -> (batch_size, n_negative_samples + 1)\n",
        "\n",
        "model = DistributedMemory(vec_dim=50,\n",
        "                          n_docs=len(df),\n",
        "                          n_words=len(vocab.words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    logits = model.forward(**next(iter(dataloader)))\n",
        "logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, dataloader, epochs=40, lr=1e-3):\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    training_losses = []\n",
        "    try:\n",
        "        for epoch in trange(epochs, desc=\"Epochs\"):\n",
        "            epoch_losses = []\n",
        "            for batch in dataloader:\n",
        "                model.zero_grad()\n",
        "                logits = model.forward(**batch)\n",
        "                batch_loss = loss(logits)\n",
        "                epoch_losses.append(batch_loss.item())\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            training_losses.append(np.mean(epoch_losses))\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"Interrupted on epoch {epoch}!\")\n",
        "    finally:\n",
        "        return training_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_losses = train(model, dataloader, epochs=40, lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_loss = pd.DataFrame(enumerate(training_losses), columns=[\"epoch\", \"training_loss\"])\n",
        "alt.Chart(df_loss).mark_bar().encode(alt.X(\"epoch\"), alt.Y(\"training_loss\", scale=alt.Scale(type=\"log\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def most_similar(paragraph_matrix, docs_df, index, n=None):\n",
        "    pm = normalize(paragraph_matrix, norm=\"l2\")  # in a smarter implementation we would cache this somewhere\n",
        "    sims = np.dot(pm, pm[index,:])\n",
        "    df = pd.DataFrame(enumerate(sims), columns=[\"doc_id\", \"similarity\"])\n",
        "    n = n if n is not None else len(sims)\n",
        "    return df.merge(docs_df[[\"text\"]].reset_index(drop=True), left_index=True, right_index=True).sort_values(by=\"similarity\", ascending=False)[:n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "most_similar(model.paragraph_matrix.data, df, 1, n=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
