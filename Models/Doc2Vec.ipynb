{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://github.com/cbowdon/doc2vec-pytorch/blob/master/doc2vec.ipynb\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam  # ilenic uses Adam, but gensim uses plain SGD\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import PCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read train claims\n",
        "with open('../data/train-claims.json', 'r') as f:\n",
        "    claims = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1228"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(claims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\n",
            "['evidence-442946', 'evidence-1194317', 'evidence-12171']\n"
          ]
        }
      ],
      "source": [
        "print(claims['claim-1937']['claim_text'])\n",
        "print(claims['claim-1937']['evidences'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read evidence\n",
        "with open('../data/evidence.json', 'r') as f:\n",
        "    evidences = json.load(f)\n",
        "evidences = {i: str.lower(j) for i,j in evidences.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1208827"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(evidences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'at very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evidences['evidence-442946']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate vocabulary size\n",
        "# Collect all texts from claims and evidences\n",
        "corpus = []\n",
        "for claim in claims.values():\n",
        "    corpus.append(str.strip(str.lower(claim['claim_text'])))  # Add claim text\n",
        "\n",
        "#for evidence_id, evidence_text in evidences.items():\n",
        "#    corpus.append(str.strip(evidence_text))  # Add evidence text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_text(df):\n",
        "    df[\"tokens\"] = df.text.apply(lambda x: [token.text.strip() for token in nlp(x) if token.text.isalnum()])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the list of documents into a pandas DataFrame\n",
        "df = pd.DataFrame(corpus, columns=['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>el niño drove record highs in global temperatu...</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in 1946, pdo switched to a cool phase.</td>\n",
              "      <td>[in, 1946, pdo, switched, to, a, cool, phase]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>weather channel co-founder john coleman provid...</td>\n",
              "      <td>[weather, channel, co, founder, john, coleman,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"january 2008 capped a 12 month period of glob...</td>\n",
              "      <td>[january, 2008, capped, a, 12, month, period, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1223</th>\n",
              "      <td>climate scientists say that aspects of the cas...</td>\n",
              "      <td>[climate, scientists, say, that, aspects, of, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1224</th>\n",
              "      <td>in its 5th assessment report in 2013, the ipcc...</td>\n",
              "      <td>[in, its, 5th, assessment, report, in, 2013, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1225</th>\n",
              "      <td>since the mid 1970s, global temperatures have ...</td>\n",
              "      <td>[since, the, mid, 1970s, global, temperatures,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1226</th>\n",
              "      <td>but abnormal temperature spikes in february an...</td>\n",
              "      <td>[but, abnormal, temperature, spikes, in, febru...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1227</th>\n",
              "      <td>sending oscillating microwaves from an antenna...</td>\n",
              "      <td>[sending, oscillating, microwaves, from, an, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1228 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  \\\n",
              "0     not only is there no scientific evidence that ...   \n",
              "1     el niño drove record highs in global temperatu...   \n",
              "2                in 1946, pdo switched to a cool phase.   \n",
              "3     weather channel co-founder john coleman provid...   \n",
              "4     \"january 2008 capped a 12 month period of glob...   \n",
              "...                                                 ...   \n",
              "1223  climate scientists say that aspects of the cas...   \n",
              "1224  in its 5th assessment report in 2013, the ipcc...   \n",
              "1225  since the mid 1970s, global temperatures have ...   \n",
              "1226  but abnormal temperature spikes in february an...   \n",
              "1227  sending oscillating microwaves from an antenna...   \n",
              "\n",
              "                                                 tokens  \n",
              "0     [not, only, is, there, no, scientific, evidenc...  \n",
              "1     [el, niño, drove, record, highs, in, global, t...  \n",
              "2         [in, 1946, pdo, switched, to, a, cool, phase]  \n",
              "3     [weather, channel, co, founder, john, coleman,...  \n",
              "4     [january, 2008, capped, a, 12, month, period, ...  \n",
              "...                                                 ...  \n",
              "1223  [climate, scientists, say, that, aspects, of, ...  \n",
              "1224  [in, its, 5th, assessment, report, in, 2013, t...  \n",
              "1225  [since, the, mid, 1970s, global, temperatures,...  \n",
              "1226  [but, abnormal, temperature, spikes, in, febru...  \n",
              "1227  [sending, oscillating, microwaves, from, an, a...  \n",
              "\n",
              "[1228 rows x 2 columns]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_df = tokenize_text(df)\n",
        "example_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, all_tokens, min_count=2):\n",
        "        self.min_count = min_count\n",
        "        self.freqs = {t:n for t, n in Counter(all_tokens).items() if n >= min_count}\n",
        "        self.words = sorted(self.freqs.keys())\n",
        "        self.word2idx = {w: i for i, w in enumerate(self.words)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset comprises 1228 documents and 3868 unique words (over the limit of 1 occurrences)\n"
          ]
        }
      ],
      "source": [
        "vocab = Vocab([tok for tokens in example_df.tokens for tok in tokens], min_count=1)\n",
        "print(f\"Dataset comprises {len(example_df)} documents and {len(vocab.words)} unique words (over the limit of {vocab.min_count} occurrences)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_tokens(df, vocab):\n",
        "    df[\"length\"] = df.tokens.apply(len)\n",
        "    df[\"clean_tokens\"] = df.tokens.apply(lambda x: [t for t in x if t in vocab.freqs.keys()])\n",
        "    df[\"clean_length\"] = df.clean_tokens.apply(len)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "      <th>length</th>\n",
              "      <th>clean_tokens</th>\n",
              "      <th>clean_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>not only is there no scientific evidence that ...</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>24</td>\n",
              "      <td>[not, only, is, there, no, scientific, evidenc...</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>el niño drove record highs in global temperatu...</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "      <td>18</td>\n",
              "      <td>[el, niño, drove, record, highs, in, global, t...</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in 1946, pdo switched to a cool phase.</td>\n",
              "      <td>[in, 1946, pdo, switched, to, a, cool, phase]</td>\n",
              "      <td>8</td>\n",
              "      <td>[in, 1946, pdo, switched, to, a, cool, phase]</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>weather channel co-founder john coleman provid...</td>\n",
              "      <td>[weather, channel, co, founder, john, coleman,...</td>\n",
              "      <td>17</td>\n",
              "      <td>[weather, channel, co, founder, john, coleman,...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"january 2008 capped a 12 month period of glob...</td>\n",
              "      <td>[january, 2008, capped, a, 12, month, period, ...</td>\n",
              "      <td>19</td>\n",
              "      <td>[january, 2008, capped, a, 12, month, period, ...</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  not only is there no scientific evidence that ...   \n",
              "1  el niño drove record highs in global temperatu...   \n",
              "2             in 1946, pdo switched to a cool phase.   \n",
              "3  weather channel co-founder john coleman provid...   \n",
              "4  \"january 2008 capped a 12 month period of glob...   \n",
              "\n",
              "                                              tokens  length  \\\n",
              "0  [not, only, is, there, no, scientific, evidenc...      24   \n",
              "1  [el, niño, drove, record, highs, in, global, t...      18   \n",
              "2      [in, 1946, pdo, switched, to, a, cool, phase]       8   \n",
              "3  [weather, channel, co, founder, john, coleman,...      17   \n",
              "4  [january, 2008, capped, a, 12, month, period, ...      19   \n",
              "\n",
              "                                        clean_tokens  clean_length  \n",
              "0  [not, only, is, there, no, scientific, evidenc...            24  \n",
              "1  [el, niño, drove, record, highs, in, global, t...            18  \n",
              "2      [in, 1946, pdo, switched, to, a, cool, phase]             8  \n",
              "3  [weather, channel, co, founder, john, coleman,...            17  \n",
              "4  [january, 2008, capped, a, 12, month, period, ...            19  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_df = clean_tokens(example_df, vocab)\n",
        "example_df[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NegativeSampling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NegativeSampling, self).__init__()\n",
        "        self.log_sigmoid = nn.LogSigmoid()\n",
        "    def forward(self, scores):\n",
        "        batch_size = scores.shape[0]\n",
        "        n_negative_samples = scores.shape[1] - 1   # TODO average or sum the negative samples? Summing seems to be correct by the paper\n",
        "        positive = self.log_sigmoid(scores[:,0])\n",
        "        negatives = torch.sum(self.log_sigmoid(-scores[:,1:]), dim=1)\n",
        "        return -torch.sum(positive + negatives) / batch_size  # average for batch\n",
        "\n",
        "loss = NegativeSampling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>scores</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, -1, -1, -1]</td>\n",
              "      <td>tensor(1.2530)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.5, -1, -1, -1]</td>\n",
              "      <td>tensor(1.4139)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, -1, -1, -1]</td>\n",
              "      <td>tensor(1.6329)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0, 0, 0, 0]</td>\n",
              "      <td>tensor(2.7726)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0, 0, 0, 1]</td>\n",
              "      <td>tensor(3.3927)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[0, 1, 1, 1]</td>\n",
              "      <td>tensor(4.6329)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[0.5, 1, 1, 1]</td>\n",
              "      <td>tensor(4.4139)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[1, 1, 1, 1]</td>\n",
              "      <td>tensor(4.2530)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              scores            loss\n",
              "0    [1, -1, -1, -1]  tensor(1.2530)\n",
              "1  [0.5, -1, -1, -1]  tensor(1.4139)\n",
              "2    [0, -1, -1, -1]  tensor(1.6329)\n",
              "3       [0, 0, 0, 0]  tensor(2.7726)\n",
              "4       [0, 0, 0, 1]  tensor(3.3927)\n",
              "5       [0, 1, 1, 1]  tensor(4.6329)\n",
              "6     [0.5, 1, 1, 1]  tensor(4.4139)\n",
              "7       [1, 1, 1, 1]  tensor(4.2530)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = [[[1, -1, -1, -1]],  # this dummy data uses -1 to 1, but the real model is unconstrained\n",
        "        [[0.5, -1, -1, -1]],\n",
        "        [[0, -1, -1, -1]],\n",
        "        [[0, 0, 0, 0]],\n",
        "        [[0, 0, 0, 1]],\n",
        "        [[0, 1, 1, 1]],\n",
        "        [[0.5, 1, 1, 1]],\n",
        "        [[1, 1, 1, 1]]]\n",
        "\n",
        "loss_df = pd.DataFrame(data, columns=[\"scores\"])\n",
        "loss_df[\"loss\"] = loss_df.scores.apply(lambda x: loss(torch.FloatTensor([x])))\n",
        "\n",
        "loss_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-5a79da544b804c2e931e54b37a2cb94d.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-5a79da544b804c2e931e54b37a2cb94d.vega-embed details,\n",
              "  #altair-viz-5a79da544b804c2e931e54b37a2cb94d.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-5a79da544b804c2e931e54b37a2cb94d\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-5a79da544b804c2e931e54b37a2cb94d\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-5a79da544b804c2e931e54b37a2cb94d\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-7d9b70d50e307a2b293711054637ca33\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"x\": {\"field\": \"x\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"y\", \"type\": \"quantitative\"}}, \"title\": \"x^0.75\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-7d9b70d50e307a2b293711054637ca33\": [{\"x\": 0.0, \"y\": 0.0}, {\"x\": 0.01, \"y\": 0.03162277660168379}, {\"x\": 0.02, \"y\": 0.053182958969449884}, {\"x\": 0.03, \"y\": 0.07208434242404263}, {\"x\": 0.04, \"y\": 0.08944271909999159}, {\"x\": 0.05, \"y\": 0.10573712634405642}, {\"x\": 0.06, \"y\": 0.12123093028059741}, {\"x\": 0.07, \"y\": 0.13608915892697748}, {\"x\": 0.08, \"y\": 0.15042412372345573}, {\"x\": 0.09, \"y\": 0.16431676725154984}, {\"x\": 0.1, \"y\": 0.1778279410038923}, {\"x\": 0.11, \"y\": 0.19100490227716513}, {\"x\": 0.12, \"y\": 0.2038853093816547}, {\"x\": 0.13, \"y\": 0.2164998073464082}, {\"x\": 0.14, \"y\": 0.22887377179317683}, {\"x\": 0.15, \"y\": 0.2410285256833955}, {\"x\": 0.16, \"y\": 0.25298221281347033}, {\"x\": 0.17, \"y\": 0.26475044029330763}, {\"x\": 0.18, \"y\": 0.2763467610958144}, {\"x\": 0.19, \"y\": 0.28778304315451386}, {\"x\": 0.2, \"y\": 0.29906975624424414}, {\"x\": 0.21, \"y\": 0.3102161981490854}, {\"x\": 0.22, \"y\": 0.32123067524150845}, {\"x\": 0.23, \"y\": 0.33212064831351956}, {\"x\": 0.24, \"y\": 0.34289285156385596}, {\"x\": 0.25, \"y\": 0.3535533905932738}, {\"x\": 0.26, \"y\": 0.3641078238014289}, {\"x\": 0.27, \"y\": 0.37456123052590357}, {\"x\": 0.28, \"y\": 0.38491826849295824}, {\"x\": 0.29, \"y\": 0.39518322257770583}, {\"x\": 0.3, \"y\": 0.4053600464421103}, {\"x\": 0.31, \"y\": 0.41545239829339137}, {\"x\": 0.32, \"y\": 0.42546367175559907}, {\"x\": 0.33, \"y\": 0.43539702265375557}, {\"x\": 0.34, \"y\": 0.4452553923589699}, {\"x\": 0.35000000000000003, \"y\": 0.45504152822405847}, {\"x\": 0.36, \"y\": 0.46475800154489}, {\"x\": 0.37, \"y\": 0.47440722340731084}, {\"x\": 0.38, \"y\": 0.4839914587188715}, {\"x\": 0.39, \"y\": 0.4935128386754873}, {\"x\": 0.4, \"y\": 0.5029733718731741}, {\"x\": 0.41000000000000003, \"y\": 0.5123749542422491}, {\"x\": 0.42, \"y\": 0.5217193779544038}, {\"x\": 0.43, \"y\": 0.5310083394307343}, {\"x\": 0.44, \"y\": 0.5402434465602292}, {\"x\": 0.45, \"y\": 0.549426225222706}, {\"x\": 0.46, \"y\": 0.5585581251971565}, {\"x\": 0.47000000000000003, \"y\": 0.5676405255254853}, {\"x\": 0.48, \"y\": 0.576674739392341}, {\"x\": 0.49, \"y\": 0.5856620185738529}, {\"x\": 0.5, \"y\": 0.5946035575013605}, {\"x\": 0.51, \"y\": 0.6035004969804791}, {\"x\": 0.52, \"y\": 0.6123539276009055}, {\"x\": 0.53, \"y\": 0.6211648928681236}, {\"x\": 0.54, \"y\": 0.629934392084505}, {\"x\": 0.55, \"y\": 0.6386633830041155}, {\"x\": 0.56, \"y\": 0.6473527842827909}, {\"x\": 0.5700000000000001, \"y\": 0.6560034777426358}, {\"x\": 0.58, \"y\": 0.6646163104680073}, {\"x\": 0.59, \"y\": 0.6731920967482075}, {\"x\": 0.6, \"y\": 0.6817316198804996}, {\"x\": 0.61, \"y\": 0.6902356338456498}, {\"x\": 0.62, \"y\": 0.6987048648669424}, {\"x\": 0.63, \"y\": 0.7071400128625219}, {\"x\": 0.64, \"y\": 0.7155417527999327}, {\"x\": 0.65, \"y\": 0.7239107359608682}, {\"x\": 0.66, \"y\": 0.7322475911233668}, {\"x\": 0.67, \"y\": 0.7405529256680135}, {\"x\": 0.68, \"y\": 0.7488273266140879}, {\"x\": 0.6900000000000001, \"y\": 0.7570713615910638}, {\"x\": 0.7000000000000001, \"y\": 0.7652855797503655}, {\"x\": 0.71, \"y\": 0.7734705126218591}, {\"x\": 0.72, \"y\": 0.7816266749191567}, {\"x\": 0.73, \"y\": 0.7897545652974598}, {\"x\": 0.74, \"y\": 0.7978546670673515}, {\"x\": 0.75, \"y\": 0.8059274488676564}, {\"x\": 0.76, \"y\": 0.8139733653002305}, {\"x\": 0.77, \"y\": 0.8219928575293057}, {\"x\": 0.78, \"y\": 0.829986353847804}, {\"x\": 0.79, \"y\": 0.837954270212839}, {\"x\": 0.8, \"y\": 0.8458970107524514}, {\"x\": 0.81, \"y\": 0.8538149682454624}, {\"x\": 0.8200000000000001, \"y\": 0.8617085245761865}, {\"x\": 0.8300000000000001, \"y\": 0.869578051165608}, {\"x\": 0.84, \"y\": 0.8774239093805121}, {\"x\": 0.85, \"y\": 0.8852464509219427}, {\"x\": 0.86, \"y\": 0.8930460181942644}, {\"x\": 0.87, \"y\": 0.9008229446560111}, {\"x\": 0.88, \"y\": 0.9085775551536168}, {\"x\": 0.89, \"y\": 0.9163101662390513}, {\"x\": 0.9, \"y\": 0.9240210864723069}, {\"x\": 0.91, \"y\": 0.9317106167096201}, {\"x\": 0.92, \"y\": 0.9393790503782488}, {\"x\": 0.93, \"y\": 0.9470266737385726}, {\"x\": 0.9400000000000001, \"y\": 0.9546537661342305}, {\"x\": 0.9500000000000001, \"y\": 0.9622606002309622}, {\"x\": 0.96, \"y\": 0.9698474422447793}, {\"x\": 0.97, \"y\": 0.9774145521600454}, {\"x\": 0.98, \"y\": 0.9849621839380145}, {\"x\": 0.99, \"y\": 0.992490585716335}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.DataFrame(zip(np.arange(0,1,0.01), np.power(np.arange(0,1,0.01), 0.75)), columns=[\"x\", \"y\"])\n",
        "alt.Chart(data, title=\"x^0.75\").mark_line().encode(x=\"x\", y=\"y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NoiseDistribution:\n",
        "    def __init__(self, vocab):\n",
        "        self.probs = np.array([vocab.freqs[w] for w in vocab.words])\n",
        "        self.probs = np.power(self.probs, 0.75)\n",
        "        self.probs /= np.sum(self.probs)\n",
        "    def sample(self, n):\n",
        "        \"Returns the indices of n words randomly sampled from the vocabulary.\"\n",
        "        return np.random.choice(a=self.probs.shape[0], size=n, p=self.probs)\n",
        "        \n",
        "noise = NoiseDistribution(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def example_generator(df, context_size, noise, n_negative_samples, vocab):\n",
        "    for doc_id, doc in df.iterrows():\n",
        "        for i in range(context_size, len(doc.clean_tokens) - context_size):\n",
        "            positive_sample = vocab.word2idx[doc.clean_tokens[i]]\n",
        "            sample_ids = noise.sample(n_negative_samples).tolist()\n",
        "            # Fix a wee bug - ensure negative samples don't accidentally include the positive\n",
        "            sample_ids = [sample_id if sample_id != positive_sample else -1 for sample_id in sample_ids]\n",
        "            sample_ids.insert(0, positive_sample)                \n",
        "            context = doc.clean_tokens[i - context_size:i] + doc.clean_tokens[i + 1:i + context_size + 1]\n",
        "            context_ids = [vocab.word2idx[w] for w in context]\n",
        "            yield {\"doc_ids\": torch.tensor(doc_id),  # we use plural here because it will be batched\n",
        "                   \"sample_ids\": torch.tensor(sample_ids), \n",
        "                   \"context_ids\": torch.tensor(context_ids)}\n",
        "            \n",
        "examples = example_generator(example_df, context_size=5, noise=noise, n_negative_samples=5, vocab=vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NCEDataset(Dataset):\n",
        "    def __init__(self, examples):\n",
        "        self.examples = list(examples)  # just naively evaluate the whole damn thing - suboptimal!\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "    def __getitem__(self, index):\n",
        "        return self.examples[index]\n",
        "    \n",
        "dataset = NCEDataset(examples)\n",
        "dataloader = DataLoader(dataset, batch_size=2, drop_last=True, shuffle=True)  # TODO bigger batch size when not dummy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'doc_id': tensor(1227),\n",
              "  'context': 'a dielectric material such as ____ creates radio frequency heating at',\n",
              "  'context_ids': tensor([ 177, 1055, 2170, 3361,  396,  920, 2803, 1513, 1691,  411]),\n",
              "  'samples': ['water', 'a', 'them', 'is', 'certainly', 'periods'],\n",
              "  'sample_ids': tensor([3754,  177, 3478, 1916,  658, 2564])},\n",
              " {'doc_id': tensor(469),\n",
              "  'context': 'climate change which is sort ____ the voice of the consensus',\n",
              "  'context_ids': tensor([ 715,  665, 3797, 1916, 3234, 3476, 3724, 2433, 3476,  812]),\n",
              "  'samples': ['of', 'and', 'five', 'reduction', 'casts', 'be'],\n",
              "  'sample_ids': tensor([2433,  341, 1448, 2863,  634,  463])}]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def describe_batch(batch, vocab):\n",
        "    results = []\n",
        "    for doc_id, context_ids, sample_ids in zip(batch[\"doc_ids\"], batch[\"context_ids\"], batch[\"sample_ids\"]):\n",
        "        context = [vocab.words[i] for i in context_ids]\n",
        "        context.insert(len(context_ids) // 2, \"____\")\n",
        "        samples = [vocab.words[i] for i in sample_ids]\n",
        "        result = {\"doc_id\": doc_id,\n",
        "                  \"context\": \" \".join(context), \n",
        "                  \"context_ids\": context_ids, \n",
        "                  \"samples\": samples, \n",
        "                  \"sample_ids\": sample_ids}\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "describe_batch(next(iter(dataloader)), vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DistributedMemory(nn.Module):\n",
        "    def __init__(self, vec_dim, n_docs, n_words):\n",
        "        super(DistributedMemory, self).__init__()\n",
        "        self.paragraph_matrix = nn.Parameter(torch.randn(n_docs, vec_dim))\n",
        "        self.word_matrix = nn.Parameter(torch.randn(n_words, vec_dim))\n",
        "        self.outputs = nn.Parameter(torch.zeros(vec_dim, n_words))\n",
        "    \n",
        "    def forward(self, doc_ids, context_ids, sample_ids):\n",
        "                                                                               # first add doc ids to context word ids to make the inputs\n",
        "        inputs = torch.add(self.paragraph_matrix[doc_ids,:],                   # (batch_size, vec_dim)\n",
        "                           torch.sum(self.word_matrix[context_ids,:], dim=1))  # (batch_size, 2x context, vec_dim) -> sum to (batch_size, vec_dim)\n",
        "                                                                               #\n",
        "                                                                               # select the subset of the output layer for the NCE test\n",
        "        outputs = self.outputs[:,sample_ids]                                   # (vec_dim, batch_size, n_negative_samples + 1)\n",
        "                                                                               #\n",
        "        return torch.bmm(inputs.unsqueeze(dim=1),                              # then multiply with some munging to make the tensor shapes line up \n",
        "                         outputs.permute(1, 0, 2)).squeeze()                   # -> (batch_size, n_negative_samples + 1)\n",
        "\n",
        "model = DistributedMemory(vec_dim=50,\n",
        "                          n_docs=len(example_df),\n",
        "                          n_words=len(vocab.words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    logits = model.forward(**next(iter(dataloader)))\n",
        "logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, dataloader, epochs=40, lr=1e-3):\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    training_losses = []\n",
        "    try:\n",
        "        for epoch in trange(epochs, desc=\"Epochs\"):\n",
        "            epoch_losses = []\n",
        "            for batch in dataloader:\n",
        "                model.zero_grad()\n",
        "                logits = model.forward(**batch)\n",
        "                batch_loss = loss(logits)\n",
        "                epoch_losses.append(batch_loss.item())\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            training_losses.append(np.mean(epoch_losses))\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"Interrupted on epoch {epoch}!\")\n",
        "    finally:\n",
        "        return training_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 40/40 [59:25<00:00, 89.15s/it]\n"
          ]
        }
      ],
      "source": [
        "training_losses = train(model, dataloader, epochs=40, lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-d19444b761534fe4943621f24311230a.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-d19444b761534fe4943621f24311230a.vega-embed details,\n",
              "  #altair-viz-d19444b761534fe4943621f24311230a.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-d19444b761534fe4943621f24311230a\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-d19444b761534fe4943621f24311230a\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-d19444b761534fe4943621f24311230a\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-b9664cc19ae416be387deb3587e08ba4\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"training_loss\", \"scale\": {\"type\": \"log\"}, \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-b9664cc19ae416be387deb3587e08ba4\": [{\"epoch\": 0, \"training_loss\": 3.272966948866528}, {\"epoch\": 1, \"training_loss\": 1.7123802999926745}, {\"epoch\": 2, \"training_loss\": 1.1684716125307435}, {\"epoch\": 3, \"training_loss\": 0.8594902951999468}, {\"epoch\": 4, \"training_loss\": 0.6396245378193427}, {\"epoch\": 5, \"training_loss\": 0.46647218123609235}, {\"epoch\": 6, \"training_loss\": 0.3302782711527863}, {\"epoch\": 7, \"training_loss\": 0.23218006399550833}, {\"epoch\": 8, \"training_loss\": 0.16095345987635287}, {\"epoch\": 9, \"training_loss\": 0.11023146041991297}, {\"epoch\": 10, \"training_loss\": 0.07700391165001831}, {\"epoch\": 11, \"training_loss\": 0.05386290073189592}, {\"epoch\": 12, \"training_loss\": 0.039607221282250936}, {\"epoch\": 13, \"training_loss\": 0.029785797991094425}, {\"epoch\": 14, \"training_loss\": 0.022683474252415615}, {\"epoch\": 15, \"training_loss\": 0.019123364608338477}, {\"epoch\": 16, \"training_loss\": 0.014351126663571065}, {\"epoch\": 17, \"training_loss\": 0.01254106805497177}, {\"epoch\": 18, \"training_loss\": 0.008715598902799507}, {\"epoch\": 19, \"training_loss\": 0.00801337998364133}, {\"epoch\": 20, \"training_loss\": 0.006890205559088871}, {\"epoch\": 21, \"training_loss\": 0.005272386048999665}, {\"epoch\": 22, \"training_loss\": 0.004520591177361353}, {\"epoch\": 23, \"training_loss\": 0.0036998193753596848}, {\"epoch\": 24, \"training_loss\": 0.0035967887494017135}, {\"epoch\": 25, \"training_loss\": 0.002623208005759823}, {\"epoch\": 26, \"training_loss\": 0.002319064345224951}, {\"epoch\": 27, \"training_loss\": 0.001818696598009521}, {\"epoch\": 28, \"training_loss\": 0.0020991305559876117}, {\"epoch\": 29, \"training_loss\": 0.0012065875194224908}, {\"epoch\": 30, \"training_loss\": 0.0016257859155589624}, {\"epoch\": 31, \"training_loss\": 0.0012975730080027998}, {\"epoch\": 32, \"training_loss\": 0.0010604782097337958}, {\"epoch\": 33, \"training_loss\": 0.001040604036956818}, {\"epoch\": 34, \"training_loss\": 0.0008294862790022755}, {\"epoch\": 35, \"training_loss\": 0.0008771852505527367}, {\"epoch\": 36, \"training_loss\": 0.00046680505039765636}, {\"epoch\": 37, \"training_loss\": 0.0004328029891997554}, {\"epoch\": 38, \"training_loss\": 0.0003541912843765567}, {\"epoch\": 39, \"training_loss\": 0.0007417858847454793}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_loss = pd.DataFrame(enumerate(training_losses), columns=[\"epoch\", \"training_loss\"])\n",
        "alt.Chart(df_loss).mark_bar().encode(alt.X(\"epoch\"), alt.Y(\"training_loss\", scale=alt.Scale(type=\"log\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def most_similar(paragraph_matrix, docs_df, index, n=None):\n",
        "    pm = normalize(paragraph_matrix, norm=\"l2\")  # in a smarter implementation we would cache this somewhere\n",
        "    sims = np.dot(pm, pm[index,:])\n",
        "    df = pd.DataFrame(enumerate(sims), columns=[\"doc_id\", \"similarity\"])\n",
        "    n = n if n is not None else len(sims)\n",
        "    return df.merge(docs_df[[\"text\"]].reset_index(drop=True), left_index=True, right_index=True).sort_values(by=\"similarity\", ascending=False)[:n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>similarity</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>el niño drove record highs in global temperatu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>804</th>\n",
              "      <td>804</td>\n",
              "      <td>0.489355</td>\n",
              "      <td>the satellite sensors show less warming in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>672</td>\n",
              "      <td>0.480265</td>\n",
              "      <td>the iris hypothesis has not withstood the test...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420</th>\n",
              "      <td>420</td>\n",
              "      <td>0.469281</td>\n",
              "      <td>the ­atmospheric residency time of carbon diox...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>274</td>\n",
              "      <td>0.467659</td>\n",
              "      <td>after the 9/11 terrorist attacks grounded comm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>683</th>\n",
              "      <td>683</td>\n",
              "      <td>0.462144</td>\n",
              "      <td>over the last decade, heatwaves are five times...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>805</th>\n",
              "      <td>805</td>\n",
              "      <td>0.455915</td>\n",
              "      <td>whether antarctic mass loss keeps worsening de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>334</th>\n",
              "      <td>334</td>\n",
              "      <td>0.451902</td>\n",
              "      <td>ben santer could not have and did not single-h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080</th>\n",
              "      <td>1080</td>\n",
              "      <td>0.443797</td>\n",
              "      <td>the most recent survey of climate scientists s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>834</th>\n",
              "      <td>834</td>\n",
              "      <td>0.441108</td>\n",
              "      <td>“climate economists see a positive externality...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      doc_id  similarity                                               text\n",
              "1          1    1.000000  el niño drove record highs in global temperatu...\n",
              "804      804    0.489355  the satellite sensors show less warming in the...\n",
              "672      672    0.480265  the iris hypothesis has not withstood the test...\n",
              "420      420    0.469281  the ­atmospheric residency time of carbon diox...\n",
              "274      274    0.467659  after the 9/11 terrorist attacks grounded comm...\n",
              "683      683    0.462144  over the last decade, heatwaves are five times...\n",
              "805      805    0.455915  whether antarctic mass loss keeps worsening de...\n",
              "334      334    0.451902  ben santer could not have and did not single-h...\n",
              "1080    1080    0.443797  the most recent survey of climate scientists s...\n",
              "834      834    0.441108  “climate economists see a positive externality..."
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "most_similar(model.paragraph_matrix.data, example_df, 1, n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2-component PCA, explains 7.53% of variance\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Length of values (4) does not match length of index (1228)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[62], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m groups\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m----> 9\u001b[0m example_2d \u001b[38;5;241m=\u001b[39m \u001b[43mpca_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparagraph_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m alt\u001b[38;5;241m.\u001b[39mChart(example_2d)\u001b[38;5;241m.\u001b[39mmark_point()\u001b[38;5;241m.\u001b[39mencode(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[62], line 6\u001b[0m, in \u001b[0;36mpca_2d\u001b[1;34m(paragraph_matrix, groups)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2-component PCA, explains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(pca\u001b[38;5;241m.\u001b[39mexplained_variance_)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% of variance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(reduced_dims, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m groups\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\comp90042w\\lib\\site-packages\\pandas\\core\\frame.py:3950\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3949\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3950\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\comp90042w\\lib\\site-packages\\pandas\\core\\frame.py:4143\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4135\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4136\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4141\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4143\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4146\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4147\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4148\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4149\u001b[0m     ):\n\u001b[0;32m   4150\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\comp90042w\\lib\\site-packages\\pandas\\core\\frame.py:4870\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4870\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\comp90042w\\lib\\site-packages\\pandas\\core\\common.py:576\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: Length of values (4) does not match length of index (1228)"
          ]
        }
      ],
      "source": [
        "def pca_2d(paragraph_matrix, groups):\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_dims = pca.fit_transform(paragraph_matrix)\n",
        "    print(f\"2-component PCA, explains {sum(pca.explained_variance_):.2f}% of variance\")\n",
        "    df = pd.DataFrame(reduced_dims, columns=[\"x\", \"y\"])\n",
        "    df[\"group\"] = groups\n",
        "    return df\n",
        "\n",
        "example_2d = pca_2d(model.paragraph_matrix.data, [\"0\",\"1\",\"2\",\"3\"])\n",
        "alt.Chart(example_2d).mark_point().encode(x=\"x\", y=\"y\", color=\"group\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
