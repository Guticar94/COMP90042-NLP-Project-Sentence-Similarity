{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://github.com/cbowdon/doc2vec-pytorch/blob/master/doc2vec.ipynb\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam  # ilenic uses Adam, but gensim uses plain SGD\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import PCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read train claims\n",
        "with open('../data/train-claims.json', 'r') as f:\n",
        "    claims = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1228"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(claims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\n",
            "['evidence-442946', 'evidence-1194317', 'evidence-12171']\n"
          ]
        }
      ],
      "source": [
        "print(claims['claim-1937']['claim_text'])\n",
        "print(claims['claim-1937']['evidences'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read evidence\n",
        "with open('../data/evidence.json', 'r') as f:\n",
        "    evidences = json.load(f)\n",
        "evidences = {i: str.lower(j) for i,j in evidences.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1208827"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(evidences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'at very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evidences['evidence-442946']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate vocabulary size\n",
        "# Collect all texts from claims and evidences\n",
        "corpus = []\n",
        "for claim in claims.values():\n",
        "    corpus.append(str.lower(claim['claim_text']))  # Add claim text\n",
        "\n",
        "for evidence_id, evidence_text in evidences.items():\n",
        "    corpus.append(evidence_text)  # Add evidence text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_text(df):\n",
        "    df[\"tokens\"] = df.text.str.lower().str.strip().apply(lambda x: [token.text.strip() for token in nlp(x) if token.text.isalnum()])\n",
        "    return df\n",
        "\n",
        "# Convert the list of documents into a pandas DataFrame\n",
        "df = pd.DataFrame(corpus, columns=['text'])\n",
        "\n",
        "example_df = tokenize_text(df)\n",
        "\n",
        "example_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, all_tokens, min_count=2):\n",
        "        self.min_count = min_count\n",
        "        self.freqs = {t:n for t, n in Counter(all_tokens).items() if n >= min_count}\n",
        "        self.words = sorted(self.freqs.keys())\n",
        "        self.word2idx = {w: i for i, w in enumerate(self.words)}\n",
        "        \n",
        "vocab = Vocab([tok for tokens in example_df.tokens for tok in tokens], min_count=1)\n",
        "\n",
        "print(f\"Dataset comprises {len(example_df)} documents and {len(vocab.words)} unique words (over the limit of {vocab.min_count} occurrences)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_tokens(df, vocab):\n",
        "    df[\"length\"] = df.tokens.apply(len)\n",
        "    df[\"clean_tokens\"] = df.tokens.apply(lambda x: [t for t in x if t in vocab.freqs.keys()])\n",
        "    df[\"clean_length\"] = df.clean_tokens.apply(len)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_df = clean_tokens(example_df, vocab)\n",
        "example_df[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NegativeSampling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NegativeSampling, self).__init__()\n",
        "        self.log_sigmoid = nn.LogSigmoid()\n",
        "    def forward(self, scores):\n",
        "        batch_size = scores.shape[0]\n",
        "        n_negative_samples = scores.shape[1] - 1   # TODO average or sum the negative samples? Summing seems to be correct by the paper\n",
        "        positive = self.log_sigmoid(scores[:,0])\n",
        "        negatives = torch.sum(self.log_sigmoid(-scores[:,1:]), dim=1)\n",
        "        return -torch.sum(positive + negatives) / batch_size  # average for batch\n",
        "\n",
        "loss = NegativeSampling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [[[1, -1, -1, -1]],  # this dummy data uses -1 to 1, but the real model is unconstrained\n",
        "        [[0.5, -1, -1, -1]],\n",
        "        [[0, -1, -1, -1]],\n",
        "        [[0, 0, 0, 0]],\n",
        "        [[0, 0, 0, 1]],\n",
        "        [[0, 1, 1, 1]],\n",
        "        [[0.5, 1, 1, 1]],\n",
        "        [[1, 1, 1, 1]]]\n",
        "\n",
        "loss_df = pd.DataFrame(data, columns=[\"scores\"])\n",
        "loss_df[\"loss\"] = loss_df.scores.apply(lambda x: loss(torch.FloatTensor([x])))\n",
        "\n",
        "loss_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.DataFrame(zip(np.arange(0,1,0.01), np.power(np.arange(0,1,0.01), 0.75)), columns=[\"x\", \"y\"])\n",
        "alt.Chart(data, title=\"x^0.75\").mark_line().encode(x=\"x\", y=\"y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NoiseDistribution:\n",
        "    def __init__(self, vocab):\n",
        "        self.probs = np.array([vocab.freqs[w] for w in vocab.words])\n",
        "        self.probs = np.power(self.probs, 0.75)\n",
        "        self.probs /= np.sum(self.probs)\n",
        "    def sample(self, n):\n",
        "        \"Returns the indices of n words randomly sampled from the vocabulary.\"\n",
        "        return np.random.choice(a=self.probs.shape[0], size=n, p=self.probs)\n",
        "        \n",
        "noise = NoiseDistribution(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def example_generator(df, context_size, noise, n_negative_samples, vocab):\n",
        "    for doc_id, doc in df.iterrows():\n",
        "        for i in range(context_size, len(doc.clean_tokens) - context_size):\n",
        "            positive_sample = vocab.word2idx[doc.clean_tokens[i]]\n",
        "            sample_ids = noise.sample(n_negative_samples).tolist()\n",
        "            # Fix a wee bug - ensure negative samples don't accidentally include the positive\n",
        "            sample_ids = [sample_id if sample_id != positive_sample else -1 for sample_id in sample_ids]\n",
        "            sample_ids.insert(0, positive_sample)                \n",
        "            context = doc.clean_tokens[i - context_size:i] + doc.clean_tokens[i + 1:i + context_size + 1]\n",
        "            context_ids = [vocab.word2idx[w] for w in context]\n",
        "            yield {\"doc_ids\": torch.tensor(doc_id),  # we use plural here because it will be batched\n",
        "                   \"sample_ids\": torch.tensor(sample_ids), \n",
        "                   \"context_ids\": torch.tensor(context_ids)}\n",
        "            \n",
        "examples = example_generator(example_df, context_size=5, noise=noise, n_negative_samples=5, vocab=vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NCEDataset(Dataset):\n",
        "    def __init__(self, examples):\n",
        "        self.examples = list(examples)  # just naively evaluate the whole damn thing - suboptimal!\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "    def __getitem__(self, index):\n",
        "        return self.examples[index]\n",
        "    \n",
        "dataset = NCEDataset(examples)\n",
        "dataloader = DataLoader(dataset, batch_size=2, drop_last=True, shuffle=True)  # TODO bigger batch size when not dummy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def describe_batch(batch, vocab):\n",
        "    results = []\n",
        "    for doc_id, context_ids, sample_ids in zip(batch[\"doc_ids\"], batch[\"context_ids\"], batch[\"sample_ids\"]):\n",
        "        context = [vocab.words[i] for i in context_ids]\n",
        "        context.insert(len(context_ids) // 2, \"____\")\n",
        "        samples = [vocab.words[i] for i in sample_ids]\n",
        "        result = {\"doc_id\": doc_id,\n",
        "                  \"context\": \" \".join(context), \n",
        "                  \"context_ids\": context_ids, \n",
        "                  \"samples\": samples, \n",
        "                  \"sample_ids\": sample_ids}\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "describe_batch(next(iter(dataloader)), vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DistributedMemory(nn.Module):\n",
        "    def __init__(self, vec_dim, n_docs, n_words):\n",
        "        super(DistributedMemory, self).__init__()\n",
        "        self.paragraph_matrix = nn.Parameter(torch.randn(n_docs, vec_dim))\n",
        "        self.word_matrix = nn.Parameter(torch.randn(n_words, vec_dim))\n",
        "        self.outputs = nn.Parameter(torch.zeros(vec_dim, n_words))\n",
        "    \n",
        "    def forward(self, doc_ids, context_ids, sample_ids):\n",
        "                                                                               # first add doc ids to context word ids to make the inputs\n",
        "        inputs = torch.add(self.paragraph_matrix[doc_ids,:],                   # (batch_size, vec_dim)\n",
        "                           torch.sum(self.word_matrix[context_ids,:], dim=1))  # (batch_size, 2x context, vec_dim) -> sum to (batch_size, vec_dim)\n",
        "                                                                               #\n",
        "                                                                               # select the subset of the output layer for the NCE test\n",
        "        outputs = self.outputs[:,sample_ids]                                   # (vec_dim, batch_size, n_negative_samples + 1)\n",
        "                                                                               #\n",
        "        return torch.bmm(inputs.unsqueeze(dim=1),                              # then multiply with some munging to make the tensor shapes line up \n",
        "                         outputs.permute(1, 0, 2)).squeeze()                   # -> (batch_size, n_negative_samples + 1)\n",
        "\n",
        "model = DistributedMemory(vec_dim=50,\n",
        "                          n_docs=len(example_df),\n",
        "                          n_words=len(vocab.words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    logits = model.forward(**next(iter(dataloader)))\n",
        "logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, dataloader, epochs=40, lr=1e-3):\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    training_losses = []\n",
        "    try:\n",
        "        for epoch in trange(epochs, desc=\"Epochs\"):\n",
        "            epoch_losses = []\n",
        "            for batch in dataloader:\n",
        "                model.zero_grad()\n",
        "                logits = model.forward(**batch)\n",
        "                batch_loss = loss(logits)\n",
        "                epoch_losses.append(batch_loss.item())\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            training_losses.append(np.mean(epoch_losses))\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"Interrupted on epoch {epoch}!\")\n",
        "    finally:\n",
        "        return training_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_losses = train(model, dataloader, epochs=40, lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_loss = pd.DataFrame(enumerate(training_losses), columns=[\"epoch\", \"training_loss\"])\n",
        "alt.Chart(df_loss).mark_bar().encode(alt.X(\"epoch\"), alt.Y(\"training_loss\", scale=alt.Scale(type=\"log\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def most_similar(paragraph_matrix, docs_df, index, n=None):\n",
        "    pm = normalize(paragraph_matrix, norm=\"l2\")  # in a smarter implementation we would cache this somewhere\n",
        "    sims = np.dot(pm, pm[index,:])\n",
        "    df = pd.DataFrame(enumerate(sims), columns=[\"doc_id\", \"similarity\"])\n",
        "    n = n if n is not None else len(sims)\n",
        "    return df.merge(docs_df[[\"text\"]].reset_index(drop=True), left_index=True, right_index=True).sort_values(by=\"similarity\", ascending=False)[:n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "most_similar(model.paragraph_matrix.data, example_df, 1, n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pca_2d(paragraph_matrix, groups):\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_dims = pca.fit_transform(paragraph_matrix)\n",
        "    print(f\"2-component PCA, explains {sum(pca.explained_variance_):.2f}% of variance\")\n",
        "    df = pd.DataFrame(reduced_dims, columns=[\"x\", \"y\"])\n",
        "    df[\"group\"] = groups\n",
        "    return df\n",
        "\n",
        "example_2d = pca_2d(model.paragraph_matrix.data, [\"0\",\"1\",\"2\",\"3\"])\n",
        "alt.Chart(example_2d).mark_point().encode(x=\"x\", y=\"y\", color=\"group\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
