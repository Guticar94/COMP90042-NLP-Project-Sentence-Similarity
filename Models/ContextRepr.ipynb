{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read evidence\n",
    "with open('../data/evidence.json', 'r') as f:\n",
    "    evidence = json.load(f)\n",
    "eviden = pd.DataFrame.from_dict(evidence, orient='index', columns=['evidence'])\n",
    "ev_txt = eviden['evidence'].values\n",
    "max_len = max([len(j.split()) for i,j in evidence.items()])\n",
    "\n",
    "# Read train claims\n",
    "with open('../data/train-claims.json', 'r') as f:\n",
    "    df_train = pd.DataFrame(json.load(f)).transpose()\n",
    "df_train = df_train.explode(\"evidences\")\n",
    "df_train['evidences_text'] = [evidence[item] for item in df_train['evidences']]\n",
    "\n",
    "# Read dev claims\n",
    "with open('../data/dev-claims.json', 'r') as f:\n",
    "    df_dev = pd.DataFrame(json.load(f)).transpose()\n",
    "df_dev['split'] = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set words to lower and tokenize\n",
    "tok_ev = [word_tokenize(i.lower()) for i in df_train['evidences_text']]\n",
    "tok_cl = [word_tokenize(i.lower()) for i in df_train['claim_text']]\n",
    "# Drop unknown characters (This may be modified depending model performance)\n",
    "tok_ev = [' '.join([w for w in seq if re.match('^[\\w\\d]+$', w)]) for seq in tok_ev]\n",
    "tok_cl = [' '.join([w for w in seq if re.match('^[\\w\\d]+$', w)]) for seq in tok_cl]\n",
    "# Join claims and evidences\n",
    "# input = [\" \".join([\"[CLS]\"] + i.split() + [\"[SEP]\"] + j.split()) for i, j in zip(tok_cl, tok_ev)]\n",
    "input = [[i, j]for i, j in zip(tok_cl, tok_ev)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer class ( Can be improved)\n",
    "class tokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"[PAD]\": 0, \"[CLS]\": 1, \"[SEP]\": 2, \"[MASK]\": 3}\n",
    "        self.index2word = {0: \"[PAD]\", 1: \"[CLS]\", 2: \"[SEP]\", 3: \"[MASK]\"}\n",
    "        self.n_words = 4  # Count CLS and SEP\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "# Words to idx\n",
    "tokenizer = tokenizer()\n",
    "for i, j in zip(tok_cl, tok_ev):\n",
    "    tokenizer.addSentence(i)\n",
    "    tokenizer.addSentence(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 6,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 12,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.word2index[i] for i in input[0][0].split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data to tensor batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.1 Bert embeding</h3></center>\n",
    "\n",
    "<center><img src=../Images/BERT_emb.png alt=\"drawing\" width=\"500\"></center>\n",
    "<center><img src=../Images/BERT_emb_example.png alt=\"drawing\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=max_len):\n",
    "        self.text = texts\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        # Step 1: get random sentence pair, either negative or positive\n",
    "        sent1, sent2, label = self.get_sent()\n",
    "        sent1 = [tokenizer.word2index[i] for i in sent1.split()]\n",
    "        sent2 = [tokenizer.word2index[i] for i in sent2.split()]\n",
    "        \n",
    "        # Step 2: replace random words in sentence with mask / random words\n",
    "        sent1_mask, sent1_label = self.masking(sent1)\n",
    "        sent2_mask, sent2_label = self.masking(sent2)\n",
    "\n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences\n",
    "        # Adding PAD token for labels\n",
    "        sent1 = [self.tokenizer.word2index['[CLS]']] + sent1_mask + [self.tokenizer.word2index['[SEP]']]\n",
    "        sent2 = sent2_mask + [self.tokenizer.word2index['[SEP]']]\n",
    "        sent1_label = [self.tokenizer.word2index['[PAD]']] + sent1_label + [self.tokenizer.word2index['[PAD]']]\n",
    "        sent2_label = sent2_label + [self.tokenizer.word2index['[PAD]']]\n",
    "\n",
    "        # Step 4: combine sentence 1 and 2 as one input\n",
    "        # adding PAD tokens to make the sentence same length as seq_len\n",
    "        \n",
    "        return texts, labels\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------\n",
    "    # Return tuple fo 2 sentences plus relation\n",
    "    def get_sent(self):\n",
    "        sentence1, sentence2 = self.text[0], self.text[1]\n",
    "        # randomly return pair of sentences\n",
    "        if random.random() > 0.5:\n",
    "            return sentence1, sentence2, 1                              # 1: Relation\n",
    "        else:\n",
    "            return sentence1, ev_txt[random.randrange(len(ev_txt))], 0  # O: No relation\n",
    "    #------------------------------------------------------------------------------------------\n",
    "    # Function to mask/randomize tokens\n",
    "    def masking(self, tokens, to_replace = 0.15):\n",
    "        # tokens = input.split()\n",
    "        output = []\n",
    "        label = []\n",
    "        for token in tokens:\n",
    "            prob = random.random()\n",
    "            # 15% of the tokens would be replaced\n",
    "            if prob <= to_replace:\n",
    "                # 10% chance change token to current token\n",
    "                if prob < to_replace*.1:\n",
    "                    output.append(token)\n",
    "                # 10% chance change token to random\n",
    "                elif prob < to_replace*.1*2:\n",
    "                    output.append(random.choice(list(self.tokenizer.word2index.values())))\n",
    "                # 10% chance change token to random\n",
    "                else:\n",
    "                    output.append(self.tokenizer.word2index[\"[MASK]\"])\n",
    "                label.append(token)\n",
    "            else:\n",
    "                output.append(token)\n",
    "                label.append(0)\n",
    "        return output, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collate (pre_process) function\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True).to(device)\n",
    "    return texts, labels\n",
    "\n",
    "# Instanciate DataLoader\n",
    "bs = 32\n",
    "tr_ds = Dataset(device)\n",
    "# dv_ds = Dataset(num_quer, range(len(num_quer)))\n",
    "\n",
    "tr_dl = DataLoader(tr_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "# dv_dl = DataLoader(dv_ds, batch_size=bs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.1.1 Positional encoding to embed the data</h3></center>\n",
    "\n",
    "<center><img src=../Images/pos_encoder.png alt=\"drawing\" width=\"300\"></center>\n",
    "\n",
    "<center>Details on:</center>\n",
    "<center><a href=\"https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\"><ph>A Gentle Introduction to Positional Encoding in Transformer Models</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "max_len_ = max([len(i) for i in num_evid]) # Maximum number of tokens in a sentence\n",
    "# Positional embeding function\n",
    "class positionalEmbeding(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop = 0.2, max_len = max_len_):\n",
    "        # Inputs:\n",
    "        # embedding_dim: Length of input embeding\n",
    "        # max_len: Max number of tokens in an input sentence\n",
    "        # Return: Positional Embeding Matrix\n",
    "        super(positionalEmbeding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=drop)                                                                           # Dropout layer\n",
    "        \n",
    "        # Positional embeding matrix \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)                                         # Positional increasing vector [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))      # Division term for the sin/cos functions\n",
    "        pe = torch.zeros(max_len, embedding_dim)                                                                    # Matrix of 0's [max_len, embedding_dim]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)                                                                # 0::2 means starting with index 0, step = 2\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)                                                                # 1::2 means starting with index 1, step = 2\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)                                                                        # Resize pos encoder [max_len, 1, embedding_dim]\n",
    "        self.register_buffer('pe', pe)                                                                              # Adds pos encoder to the model state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input:\n",
    "        # x: Embeding matrix [batch_size, text_length, embedding_dim]\n",
    "        x = x + self.pe[:x.size(0), :x.size(1)]      # Sum the position embeding\n",
    "        return self.dropout(x)              # Apply dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.2 Multihead attention</h3></center>\n",
    "<center><img src=../Images/attention.png alt=\"drawing\" width=\"600\"></center>\n",
    "\n",
    "<center>Details on:</center>\n",
    "<center><a href=\"https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\"><ph>Build your own Transformer from scratch using Pytorch</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert embedding_dim % num_heads == 0, \"in_size must be divisible by num_heads\"\n",
    "\n",
    "        self.embedding_dim = embedding_dim                      # Embeding input size\n",
    "        self.num_heads = num_heads                              # Num heads of multihead attention model\n",
    "        self.head_dim = embedding_dim // num_heads              # Embedding parameters for each head\n",
    "        \n",
    "        # Instanciate weights\n",
    "        self.W_q = nn.Linear(embedding_dim, embedding_dim)      # Query weights\n",
    "        self.W_k = nn.Linear(embedding_dim, embedding_dim)      # Key weights\n",
    "        self.W_v = nn.Linear(embedding_dim, embedding_dim)      # Values weights\n",
    "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    # scaled_dot_product_attention\n",
    "    def dot_prd_attn(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)   # MatMult (Q*K)\n",
    "        if mask is not None: attn_scores = attn_scores.masked_fill(mask == 0, -1e9)     # Masking (Optional)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)                                 # Softmax\n",
    "        output = torch.matmul(attn_probs, V)                                            # MatMult (Probs*V)\n",
    "        return output\n",
    "    \n",
    "    # Function to split attention heads\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, embedding_dim = x.size()\n",
    "        return x.view(batch_size, self.num_heads, seq_length, self.head_dim)\n",
    "    # Function to join attention heads\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, head_dim = x.size()\n",
    "        return x.view(batch_size, seq_length, self.embedding_dim)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        # Weights linear pass (Random inicialization) + Split heads\n",
    "        Q = self.split_heads(self.W_q(x))\n",
    "        K = self.split_heads(self.W_k(x))\n",
    "        V = self.split_heads(self.W_v(x))\n",
    "        # Multihead attention\n",
    "        attn = self.dot_prd_attn(Q, K, V, mask)                 # scaled_dot_product_attention\n",
    "        attn = self.combine_heads(attn)                         # Concat heads\n",
    "        attn = self.linear(attn)                                # Linear pass\n",
    "        return attn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.5 Transformer model (Passage Ranking)</h3></center>\n",
    "<center>Source papers:</center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Attention Is All You Need</ph></a></center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Text and Code Embeddings by Contrastive Pre-Training</ph></a></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Encoder:</center>\n",
    "<center><img src=../Images/encoder.png alt=\"drawing\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class based \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                vocab_size,                            # Size of vocabulary\n",
    "                embedding_dim,                         # Embedding dimension\n",
    "                n_head,                                # Number of heads  in the multihead attention model\n",
    "                hidden_dim,                            # Hiden dims for the feed forward pass\n",
    "                dropout = 0.5):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)                  # Embeding layer\n",
    "        self.pos_encoder = positionalEmbeding(embedding_dim, dropout)           # Positional embeding\n",
    "        self.multihead = MultiHeadAttention(embedding_dim, n_head)              # Multihead attention layer\n",
    "        self.normalization = nn.LayerNorm(embedding_dim)                        # Normalization layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(embedding_dim, 1)                               # Output layer\n",
    "\n",
    "        # Feed forward pass\n",
    "        self.feed_forward = nn.Sequential()\n",
    "        self.feed_forward.add_module('fc1', nn.Linear(embedding_dim, hidden_dim))\n",
    "        self.feed_forward.add_module('relu', nn.ReLU())\n",
    "        self.feed_forward.add_module('fc2', nn.Linear(hidden_dim, embedding_dim))\n",
    "\n",
    "    def forward(self, text):\n",
    "        encoder = self.encoder(text) * math.sqrt(self.embedding_dim)            # Encode imput text [batch_size, text_length, embedding_dim]\n",
    "        pos_enc = self.pos_encoder(encoder)                                     # Reurn pos encoder [batch_size, text_length, embedding_dim]\n",
    "        attn = self.multihead(pos_enc)                                          # Multihead encoder\n",
    "        normal = self.normalization(text.unsqueeze(2) + self.dropout(attn))     # Add & Normalize pass #1  UNSQUEEZE\n",
    "        forward = self.feed_forward(normal)                                     # Feed Forward pass\n",
    "        encoded = self.normalization(normal + self.dropout(forward))            # Add & Normalize pass #2\n",
    "        lin_vec = self.linear(encoded)\n",
    "        return lin_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # Encoder is a stack of N encoder layers. \n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for i in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, text, mask = None, src_key_padding_mask = None):\n",
    "        output = text\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max([max(i) for i in num_evid]) + max(num_quer)\n",
    "embedding_dim = 300\n",
    "n_head = 2\n",
    "dropout = 0.5\n",
    "hidden_dim = 2048\n",
    "num_layers = 2\n",
    "encoder_layer = EncoderLayer(vocab_size, embedding_dim, n_head, hidden_dim, dropout).to(device)\n",
    "encoder = Encoder(encoder_layer, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:08<00:00, 57.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from numpy.linalg import norm\n",
    "cos_sim = []\n",
    "y = encoder_layer(next(iter(dv_dl))[0].unsqueeze(1)).reshape(-1).detach().numpy()\n",
    "for x, _ in tqdm(tr_dl):\n",
    "    enc = encoder_layer(x)\n",
    "    for line in enc:\n",
    "        X = line.reshape(-1).detach().numpy()\n",
    "        cos_sim.append(np.dot(X[:len(y)], y[:len(X)])/(norm(X)*norm(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>Evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8048</th>\n",
       "      <td>0.567922</td>\n",
       "      <td>within countries as differing political moveme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13066</th>\n",
       "      <td>0.566776</td>\n",
       "      <td>or sitamun was a princess of the early eightee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6668</th>\n",
       "      <td>0.535906</td>\n",
       "      <td>geese are waterfowl belonging to the tribe ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7129</th>\n",
       "      <td>0.517724</td>\n",
       "      <td>the galaxy y duos is a mobile phone from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11642</th>\n",
       "      <td>0.494271</td>\n",
       "      <td>jacques schotte september was a belgian psychi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>-0.541321</td>\n",
       "      <td>casley never played for torquay league team in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8748</th>\n",
       "      <td>-0.550421</td>\n",
       "      <td>while the characters and instances in the movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11473</th>\n",
       "      <td>-0.575943</td>\n",
       "      <td>momofuku is a cookbook by american chef david ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11450</th>\n",
       "      <td>-0.587998</td>\n",
       "      <td>ross mccloud 1819 august was a california pion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13463</th>\n",
       "      <td>-0.625665</td>\n",
       "      <td>it is a teaching hospital and major provincial...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       similarity                                           Evidence\n",
       "8048     0.567922  within countries as differing political moveme...\n",
       "13066    0.566776  or sitamun was a princess of the early eightee...\n",
       "6668     0.535906  geese are waterfowl belonging to the tribe ans...\n",
       "7129     0.517724           the galaxy y duos is a mobile phone from\n",
       "11642    0.494271  jacques schotte september was a belgian psychi...\n",
       "...           ...                                                ...\n",
       "381     -0.541321  casley never played for torquay league team in...\n",
       "8748    -0.550421  while the characters and instances in the movi...\n",
       "11473   -0.575943  momofuku is a cookbook by american chef david ...\n",
       "11450   -0.587998  ross mccloud 1819 august was a california pion...\n",
       "13463   -0.625665  it is a teaching hospital and major provincial...\n",
       "\n",
       "[15000 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(cos_sim, columns=['similarity'])\n",
    "df['Evidence'] = ev[:15000]\n",
    "df.sort_values('similarity', ascending=False, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['claim_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>Evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12171</th>\n",
       "      <td>-0.201789</td>\n",
       "      <td>higher carbon dioxide concentrations will favo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       similarity                                           Evidence\n",
       "12171   -0.201789  higher carbon dioxide concentrations will favo..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.index == 12171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(dv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5v/mt404vm51mvcpt_t2j5z7dbm0000gn/T/ipykernel_39035/3878134485.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_train['evidences'][0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['evidence-442946', 'evidence-1194317', 'evidence-12171']"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['evidences'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4933"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(sequence.word2index.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# def reject(df, eviden):\n",
    "#     reject = []\n",
    "#     for row in tqdm(df.index):\n",
    "#         ev = df.loc[row, 'evidences']\n",
    "#         rej = []\n",
    "#         samp = True\n",
    "#         while samp:\n",
    "#             sp = eviden.sample(len(ev))['evidence'].values\n",
    "#             for val in sp:\n",
    "#                 if val not in ev:\n",
    "#                     rej.append(sp)\n",
    "#                     samp = False\n",
    "#         reject.append(rej[0])\n",
    "#     return reject\n",
    "\n",
    "# df_train_ref = df_train[['claim_text','claim_label']].copy()\n",
    "# df_train_ref['evidences'] = reject(df_train, eviden)\n",
    "# df_train_ref['split'] = 'train'\n",
    "# df_train_ref['label'] = 'disengagement'\n",
    "\n",
    "# df_train['evidences'] = [[evidence[ev] for ev in val] for val in df_train['evidences']]\n",
    "\n",
    "# df = pd.concat([df_train, df_train_ref])[['claim_text', 'evidences', 'label', 'split']]\n",
    "# df = df.explode('evidences')\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
