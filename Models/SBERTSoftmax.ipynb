{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas nltk numpy torch torchvision\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117 --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\anaconda3\\envs\\comp90042Project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm, tqdm_notebook # show progress bar\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read evidence\n",
    "with open('../data/evidence.json', 'r') as f:\n",
    "    evidence = json.load(f)\n",
    "eviden = pd.DataFrame.from_dict(evidence, orient='index', columns=['evidence'])\n",
    "ev_txt = eviden['evidence'].values\n",
    "max_len = max([len(j.split()) for i,j in evidence.items()])\n",
    "\n",
    "# Read train claims\n",
    "with open('../data/train-claims.json', 'r') as f:\n",
    "    df_train = pd.DataFrame(json.load(f)).transpose()\n",
    "\n",
    "# Read dev claims\n",
    "with open('../data/dev-claims.json', 'r') as f:\n",
    "    df_dev = pd.DataFrame(json.load(f)).transpose()\n",
    "\n",
    "# Read test claims\n",
    "with open('../data/test-claims-unlabelled.json', 'r') as f:\n",
    "    df_test = pd.DataFrame(json.load(f)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer function (Map words to indexes)\n",
    "class token:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"[PAD]\": 0, \"[CLS]\": 1, \"[SEP]\": 2, \"[MASK]\": 3}\n",
    "        self.index2word = {0: \"[PAD]\", 1: \"[CLS]\", 2: \"[SEP]\", 3: \"[MASK]\"}\n",
    "        self.n_words = 4  # Count CLS and SEP\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "# tokenizer function\n",
    "tokenizer = token()\n",
    "def tok(corpus):\n",
    "    [tokenizer.addSentence(i) for i in corpus]\n",
    "\n",
    "# Function to get negative relations\n",
    "def neg(claims, number):\n",
    "    # Define lists\n",
    "    tok_ev, tok_cl, label = [], [], []\n",
    "    # Get negative list\n",
    "    negative = eviden.sample(n = number)['evidence'].values\n",
    "    for cl in claims:\n",
    "        # Add negative relations to list\n",
    "        tok_ev.extend(negative)\n",
    "        tok_cl.extend([cl] *len(negative))\n",
    "        label.extend([0] * len(negative))\n",
    "    return tok_ev, tok_cl, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process of tokenization and string removal from evidence dataset. File saved to reduce memory usage\n",
    "if not os.path.exists('../data/evidence_tokenized.json'):\n",
    "    # Tokenize and lower text\n",
    "    ev = [word_tokenize(i.lower()) for i in ev_txt]\n",
    "    # Drop unknown characters\n",
    "    ev_txt = [' '.join([w for w in seq if re.match('^[\\w\\d]+$', w)]) for seq in ev]\n",
    "    with open('../data/evidence_tokenized.json', 'w') as f:\n",
    "        json.dump(ev_txt, f)\n",
    "else:\n",
    "    with open('../data/evidence_tokenized.json', 'r') as f:\n",
    "        ev_txt = json.load(f)\n",
    "        tok(ev_txt)\n",
    "        eviden['evidence'] = ev_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DISPUTED': 0, 'REFUTES': 1, 'SUPPORTS': 2, 'NOT_ENOUGH_INFO': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL_DICTIONARY = {}\n",
    "ID_DICTIONARY = {}\n",
    "for i, label in enumerate(df_train['claim_label'].unique()):\n",
    "    LABEL_DICTIONARY[label] = i\n",
    "    ID_DICTIONARY[i] = label\n",
    "LABEL_DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Label Distribution\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "\n",
    "axs[0].set_title(\"Train\")\n",
    "axs[1].set_title(\"Validation\")\n",
    "tlabel = axs[0].hist(sorted([l for l in df_train['claim_label']]))\n",
    "vlabel = axs[1].hist(sorted([l for l in df_train['claim_label']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_text</th>\n",
       "      <th>evidences_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claim-2130</th>\n",
       "      <td>Plant stomata show higher and more variable CO...</td>\n",
       "      <td>in some places the rain fell over 50 mm in an ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim-44</th>\n",
       "      <td>Human-produced carbon might be one of the fact...</td>\n",
       "      <td>more recent studies of seafloor microbes cast ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim-1774</th>\n",
       "      <td>The CERN CLOUD experiment only tested one-thir...</td>\n",
       "      <td>This result does not support the hypothesis th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim-2020</th>\n",
       "      <td>Weather Channel Co-Founder John Coleman Calls ...</td>\n",
       "      <td>confused by the conflicting indications an ope...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim-1112</th>\n",
       "      <td>“The jet stream forms a boundary between the c...</td>\n",
       "      <td>it lies south of wellington and has a populati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim-1367</th>\n",
       "      <td>But we do have other reliable indicators of te...</td>\n",
       "      <td>shortly thereafter agramunt himself was one of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim-450</th>\n",
       "      <td>If we double atmospheric carbon dioxide[…] we’...</td>\n",
       "      <td>globally during the 1980s one new nuclear reac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim-1506</th>\n",
       "      <td>Most likely the primary control knob on climat...</td>\n",
       "      <td>United States Secretary of Energy Rick Perry, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim-710</th>\n",
       "      <td>With marine ice cliff instability, sea-level r...</td>\n",
       "      <td>According to the Fourth (2017) National Climat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim-2818</th>\n",
       "      <td>The 30,000 scientists and science graduates li...</td>\n",
       "      <td>Robinson asserted in 2008 that the petition ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8244 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   claim_text  \\\n",
       "claim-2130  Plant stomata show higher and more variable CO...   \n",
       "claim-44    Human-produced carbon might be one of the fact...   \n",
       "claim-1774  The CERN CLOUD experiment only tested one-thir...   \n",
       "claim-2020  Weather Channel Co-Founder John Coleman Calls ...   \n",
       "claim-1112  “The jet stream forms a boundary between the c...   \n",
       "...                                                       ...   \n",
       "claim-1367  But we do have other reliable indicators of te...   \n",
       "claim-450   If we double atmospheric carbon dioxide[…] we’...   \n",
       "claim-1506  Most likely the primary control knob on climat...   \n",
       "claim-710   With marine ice cliff instability, sea-level r...   \n",
       "claim-2818  The 30,000 scientists and science graduates li...   \n",
       "\n",
       "                                               evidences_text  label  \n",
       "claim-2130  in some places the rain fell over 50 mm in an ...      0  \n",
       "claim-44    more recent studies of seafloor microbes cast ...      0  \n",
       "claim-1774  This result does not support the hypothesis th...      1  \n",
       "claim-2020  confused by the conflicting indications an ope...      0  \n",
       "claim-1112  it lies south of wellington and has a populati...      0  \n",
       "...                                                       ...    ...  \n",
       "claim-1367  shortly thereafter agramunt himself was one of...      0  \n",
       "claim-450   globally during the 1980s one new nuclear reac...      0  \n",
       "claim-1506  United States Secretary of Energy Rick Perry, ...      1  \n",
       "claim-710   According to the Fourth (2017) National Climat...      1  \n",
       "claim-2818  Robinson asserted in 2008 that the petition ha...      1  \n",
       "\n",
       "[8244 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to prepare datasets\n",
    "def prepare_df(df):\n",
    "    # Support labels\n",
    "    df_1 = df.explode(\"evidences\")\n",
    "    df_1['evidences_text'] = [evidence[item] for item in df_1['evidences']]\n",
    "    df_1['label'] = 1\n",
    "    # Refuse labels\n",
    "    df_2 = df_1[['claim_text']].copy()\n",
    "    df_2['evidences_text'] = [random.choice(ev_txt) for i in range(df_2.shape[0])]\n",
    "    df_2['label'] = 0\n",
    "    df = pd.concat([df_1[['claim_text' , 'evidences_text', 'label']], df_2]).sample(frac=1)\n",
    "    return df\n",
    "\n",
    "df = prepare_df(df_train)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to prepare datasets\n",
    "def prepare_df(df):\n",
    "    # Support labels\n",
    "    df_1 = df.explode(\"evidences\")\n",
    "    df_1['evidences_text'] = [evidence[item] for item in df_1['evidences']]\n",
    "    df_1['label'] = 1\n",
    "    # Refuse labels\n",
    "    df_2 = df_1[['claim_text']].copy()\n",
    "    df_2['evidences_text'] = [random.choice(ev_txt) for i in range(df_2.shape[0])]\n",
    "    df_2['label'] = 0\n",
    "    df = pd.concat([df_1[['claim_text' , 'evidences_text', 'label']], df_2]).sample(frac=1)\n",
    "    return df\n",
    "\n",
    "# Select columns to work on and retrieve tokenized and preprocesed vectors \n",
    "def feature_selection(df):\n",
    "    # Prepare df\n",
    "    df_ = prepare_df(df)\n",
    "    # Set words to lower and tokenize\n",
    "    tok_evidence = [word_tokenize(i.lower()) for i in df_['evidences_text']]\n",
    "    tok_claim = [word_tokenize(i.lower()) for i in df_['claim_text']]\n",
    "    # Drop unknown characters (This may be modified depending model performance)\n",
    "    tok_evidence = [' '.join([w for w in seq if re.match('^[\\w\\d]+$', w)]) for seq in tok_evidence]\n",
    "    tok_claim = [' '.join([w for w in seq if re.match('^[\\w\\d]+$', w)]) for seq in tok_claim]\n",
    "    # Class label\n",
    "    label = df_['label']\n",
    "    return tok_claim, tok_evidence, label\n",
    "\n",
    "# Get data features\n",
    "tr_tok_cl, tr_tok_ev, tr_label = feature_selection(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(tr_label).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess dev set\n",
    "# Instanciate lists\n",
    "dv_tok_ev = []\n",
    "dv_tok_cl = []\n",
    "# Get positive pair relations\n",
    "positive = df_dev.explode('evidences')[['claim_text', 'evidences']]\n",
    "# Add positive relations to list\n",
    "dv_tok_ev.extend([eviden.loc[i].values[0] for i in positive['evidences']])\n",
    "dv_tok_cl.extend(positive['claim_text'].values)\n",
    "dv_label =[1] * len(positive)\n",
    "\n",
    "# Create negative relations\n",
    "negative = neg(dv_tok_cl, 1)\n",
    "dv_tok_ev.extend(negative[0])\n",
    "dv_tok_cl.extend(negative[1])\n",
    "dv_label.extend(negative[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dv_label).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test to tokenize\n",
    "claim_test = [word_tokenize(i.lower()) for i in df_test['claim_text']]\n",
    "claim_test = [' '.join([w for w in seq if re.match('^[\\w\\d]+$', w)]) for seq in claim_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tokens to dict\n",
    "tok(ev_txt)\n",
    "tok(tr_tok_cl)\n",
    "tok(dv_tok_cl)\n",
    "tok(claim_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data to tensor batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.1 Bert embeding</h3></center>\n",
    "\n",
    "<center><img src=../main/Images/BERT_emb.png alt=\"drawing\" width=\"500\"></center>\n",
    "<center><img src=../Images/BERT_emb_example.png alt=\"drawing\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=max_len):\n",
    "        self.text = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        # Step 1: get text tokens\n",
    "        sent = [self.tokenizer.word2index[i] for i in self.text[idx].split()]\n",
    "        \n",
    "        # Step 2: replace random words in sentence with mask / random words\n",
    "        sent_mask, labels = self.masking(sent)\n",
    "\n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentence\n",
    "        # Adding PAD token for labels\n",
    "        sent = [self.tokenizer.word2index['[CLS]']] + sent_mask + [self.tokenizer.word2index['[SEP]']]\n",
    "        labels = [self.tokenizer.word2index['[PAD]']] + labels + [self.tokenizer.word2index['[PAD]']]\n",
    "\n",
    "        # Step 4: Add PAD tokens to make the sentence same length as seq_len\n",
    "        padding = [self.tokenizer.word2index['[PAD]'] for empty in range(self.seq_len - len(sent))]\n",
    "        sent.extend(padding)\n",
    "        labels.extend(padding)\n",
    "        return np.array(sent), np.array(labels)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------\n",
    "    # Function to mask/randomize tokens\n",
    "    def masking(self, tokens, to_replace = 0.15):\n",
    "        # tokens = input.split()\n",
    "        output = []\n",
    "        label = []\n",
    "        for token in tokens:\n",
    "            prob = random.random()\n",
    "            # 15% of the tokens would be replaced\n",
    "            if prob <= to_replace:\n",
    "                # 10% chance change token to current token\n",
    "                if prob < to_replace*.1:\n",
    "                    output.append(token)\n",
    "                # 10% chance change token to random\n",
    "                elif prob < to_replace*.1*2:\n",
    "                    output.append(random.choice(list(self.tokenizer.word2index.values())))\n",
    "                # 10% chance change token to random\n",
    "                else:\n",
    "                    output.append(self.tokenizer.word2index[\"[MASK]\"])\n",
    "                label.append(token)\n",
    "            else:\n",
    "                output.append(token)\n",
    "                label.append(0)\n",
    "        return output, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collate (pre_process) function\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = torch.from_numpy(np.array(texts)).to(device)\n",
    "    labels = torch.from_numpy(np.array(labels)).to(device)\n",
    "    return texts, labels\n",
    "\n",
    "# Instanciate DataLoader\n",
    "bs = 32\n",
    "\n",
    "# ______________________________Traing data______________________________\n",
    "# Datasets\n",
    "tr_ev_ds = Dataset(tr_tok_ev, tokenizer)\n",
    "tr_cl_ds = Dataset(tr_tok_cl, tokenizer)\n",
    "\n",
    "# Dataloaders\n",
    "tr_ev_dl = DataLoader(tr_ev_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "tr_cl_dl = DataLoader(tr_cl_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "tr_y_dl = DataLoader(tr_label, batch_size=bs)\n",
    "\n",
    "# ______________________________Test data______________________________\n",
    "# Datasets\n",
    "dv_ev_ds = Dataset(dv_tok_ev, tokenizer)\n",
    "dv_cl_ds = Dataset(dv_tok_cl, tokenizer)\n",
    "\n",
    "# Dataloaders\n",
    "dv_ev_dl = DataLoader(dv_ev_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "dv_cl_dl = DataLoader(dv_cl_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "dv_y_dl = DataLoader(dv_label, batch_size=bs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.1 Positional encoding to embed the data</h3></center>\n",
    "\n",
    "<center><img src=../Images/pos_encoder.png alt=\"drawing\" width=\"300\"></center>\n",
    "\n",
    "<center>Details on:</center>\n",
    "<center><a href=\"https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\"><ph>A Gentle Introduction to Positional Encoding in Transformer Models</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional embeding function\n",
    "class positionalEmbeding(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop = 0.2, max_len = max_len):\n",
    "        # Inputs:\n",
    "        # embedding_dim: Length of input embeding\n",
    "        # max_len: Max number of tokens in an input sentence\n",
    "        # Return: Positional Embeding Matrix\n",
    "        super(positionalEmbeding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=drop)                                                                           # Dropout layer\n",
    "        \n",
    "        # Positional embeding matrix \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)                                         # Positional increasing vector [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))      # Division term for the sin/cos functions\n",
    "        pe = torch.zeros(max_len, embedding_dim).float()                                                            # Matrix of 0's [max_len, embedding_dim]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)                                                                # 0::2 means starting with index 0, step = 2\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)                                                                # 1::2 means starting with index 1, step = 2\n",
    "        pe = pe.unsqueeze(0)                                                                                        # Resize pos encoder [1, max_len, embedding_dim]\n",
    "        self.register_buffer('pe', pe)                                                                              # Adds pos encoder to the model state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input:\n",
    "        # x: Embeding matrix [batch_size, text_length, embedding_dim]\n",
    "        x = x + self.pe.requires_grad_(False)                      # Sum the position embeding\n",
    "        return self.dropout(x)                                     # Apply dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.2 Multihead attention</h3></center>\n",
    "<center><img src=../Images/attention.png alt=\"drawing\" width=\"600\"></center>\n",
    "\n",
    "<center>Details on:</center>\n",
    "<center><a href=\"https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\"><ph>Build your own Transformer from scratch using Pytorch</ph></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert embedding_dim % num_heads == 0, \"in_size must be divisible by num_heads\"\n",
    "\n",
    "        self.embedding_dim = embedding_dim                      # Embeding input size\n",
    "        self.num_heads = num_heads                              # Num heads of multihead attention model\n",
    "        self.head_dim = embedding_dim // num_heads              # Embedding parameters for each head\n",
    "        \n",
    "        # Instanciate weights\n",
    "        self.W_q = nn.Linear(embedding_dim, embedding_dim)      # Query weights\n",
    "        self.W_k = nn.Linear(embedding_dim, embedding_dim)      # Key weights\n",
    "        self.W_v = nn.Linear(embedding_dim, embedding_dim)      # Values weights\n",
    "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # scaled_dot_product_attention\n",
    "    def dot_prd_attn(self, Q, K, V, mask):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)   # MatMult (Q*K)\n",
    "\n",
    "        # Fill 0 mask with super small number so it wont affect the softmax weight\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)     \n",
    "\n",
    "        # softmax to put attention weight for all non-pad tokens\n",
    "        attn_probs = self.dropout(torch.softmax(attn_scores, dim=-1))                   # Softmax\n",
    "        context = torch.matmul(attn_probs, V)                                           # MatMult (Probs*V)\n",
    "        return context\n",
    "    \n",
    "    # Function to split attention heads\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, embedding_dim = x.size()\n",
    "        return x.view(batch_size, self.num_heads, seq_length, self.head_dim)\n",
    "    # Function to join attention heads\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, head_dim = x.size()\n",
    "        return x.view(batch_size, seq_length, self.embedding_dim)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # Weights linear pass (Random inicialization) + Split heads\n",
    "        Q = self.split_heads(self.W_q(x))\n",
    "        K = self.split_heads(self.W_k(x))\n",
    "        V = self.split_heads(self.W_v(x))\n",
    "        # Multihead attention\n",
    "        attn = self.dot_prd_attn(Q, K, V, mask)                 # scaled_dot_product_attention\n",
    "        attn = self.combine_heads(attn)                         # Concat heads\n",
    "        attn = self.linear(attn)                                # Linear pass\n",
    "        return attn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.3 Encoder model (Passage Ranking)</h3></center>\n",
    "<center>Source papers:</center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Attention Is All You Need</ph></a></center>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.03762\"><ph>Text and Code Embeddings by Contrastive Pre-Training</ph></a></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Encoder:</center>\n",
    "<center><img src=../Images/encoder.png alt=\"drawing\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class based \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                vocab_size,                            # Size of vocabulary\n",
    "                embedding_dim,                         # Embedding dimension\n",
    "                n_head,                                # Number of heads  in the multihead attention model\n",
    "                hidden_dim = 300,                      # Hiden dims for the feed forward pass\n",
    "                dropout = 0.5):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.multihead = MultiHeadAttention(embedding_dim, n_head)              # Multihead attention layer\n",
    "        self.normalization = nn.LayerNorm(embedding_dim)                        # Normalization layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(embedding_dim, 1)                               # Output layer\n",
    "\n",
    "        # Feed forward pass\n",
    "        self.feed_forward = nn.Sequential().to(device)\n",
    "        self.feed_forward.add_module('fc1', nn.Linear(embedding_dim, hidden_dim))\n",
    "        self.feed_forward.add_module('relu', nn.GELU())\n",
    "        self.feed_forward.add_module('fc2', nn.Linear(hidden_dim, embedding_dim))\n",
    "\n",
    "    def forward(self, embeding, mask):\n",
    "        attn = self.dropout(self.multihead(embeding, mask))                      # Multihead attention\n",
    "        normal = self.normalization(embeding + attn)                             # Add & Normalize pass\n",
    "        forward = self.dropout(self.feed_forward(normal))                       # Feed Forward pass\n",
    "        encoded = self.normalization(normal + forward)                          # Add & Normalize pass #2\n",
    "        return encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.4 BERT model</h3></center>\n",
    "<center><img src=../Images/BERT_enc.png alt=\"drawing\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert model\n",
    "class BERT(nn.Module):\n",
    "    # Encoder is a stack of N encoder layers. \n",
    "    def __init__(self, vocab_size, d_model, num_layers, n_head, dropout):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = num_layers\n",
    "        self.heads = n_head\n",
    "\n",
    "        # paper noted they used 4 * hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = d_model * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional and token embeddings (No sentence since it is a SBERT)\n",
    "        self.encoder = nn.Embedding(vocab_size, d_model, padding_idx=0)   # Embeding layer\n",
    "        self.pos_encoder = positionalEmbeding(d_model, dropout)           # Positional embeding\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.encoder_blocks = torch.nn.ModuleList(\n",
    "            [EncoderLayer(vocab_size = vocab_size, embedding_dim = d_model, n_head = n_head, hidden_dim = 500, dropout = 0.5)\\\n",
    "                .to(device) for _ in range(num_layers)])\n",
    "        \n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        mask = (text > 0).unsqueeze(1).repeat(1, text.size(1), 1).unsqueeze(1)  # Redim mask [batch_size, 1, 1, max_len]\n",
    "        encoder = self.encoder(text) * math.sqrt(self.d_model)                  # Text embeding imput\n",
    "        pos_enc = self.pos_encoder(encoder)                                     # Positional embeding + Text embeding\n",
    "        # running over multiple transformer blocks\n",
    "        for layer in self.encoder_blocks:\n",
    "            output = layer(pos_enc, mask)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 2.5 SBERT model</h3></center>\n",
    "\n",
    "<center><img src=../Images/SBERT_classification.png alt=\"drawing\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = len(tokenizer.word2index)+1\n",
    "d_model = 300\n",
    "n_head = 1\n",
    "dropout = 0.1\n",
    "hidden_dim = 2048\n",
    "num_layers = 3\n",
    "# Instanciate model\n",
    "model = BERT(vocab_size, d_model, num_layers, n_head, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss fn\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())    # lr=2e−5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SBERT model\n",
    "def train_model():\n",
    "    # Cosine similarity function\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    # Iterate dataloader\n",
    "    for t1, t2, y in tqdm(zip(tr_ev_dl, tr_cl_dl, tr_y_dl)):\n",
    "        # Set parameters\n",
    "        sent_a, m1 = t1\n",
    "        sent_b, m2 = t2\n",
    "        y = y.float().to(device)\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encoder layer\n",
    "        enc_a = model(sent_a, m1)\n",
    "        enc_b = model(sent_b, m2)\n",
    "\n",
    "        # Pooling layer mean\n",
    "        u = torch.mean(enc_a, 1) \n",
    "        v = torch.mean(enc_b, 1) \n",
    "\n",
    "        # Similarity metric\n",
    "        similarity = cos(u, v)\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(similarity, y)\n",
    "        acc = torch.sum((similarity>=0.5).float() == y)\n",
    "        total = y.size()[0]\n",
    "\n",
    "        # Metrics\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()             # Backpropagation\n",
    "        optimizer.step()            # Update parameters\n",
    "\n",
    "    # Print results\n",
    "    d_acc = (acc)/(total)\n",
    "    loss = train_loss/len(tr_y_dl)\n",
    "\n",
    "    tqdm.write(\n",
    "        f'Train Accuracy: {d_acc:.3f}\\\n",
    "        Train Loss: {loss:.3f}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    test_loss = 0\n",
    "    # Iterate dataloader\n",
    "    for t1, t2, y in tqdm(zip(dv_ev_dl, dv_cl_dl, dv_y_dl)):\n",
    "        # Set parameters\n",
    "        sent_a, m1 = t1\n",
    "        sent_b, m2 = t2\n",
    "        y = y.float().to(device)\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        # Encoder layer\n",
    "        enc_a = model(sent_a, m1)\n",
    "        enc_b = model(sent_b, m2)\n",
    "    \n",
    "        # Pooling layer mean\n",
    "        u = torch.mean(enc_a, 1) \n",
    "        v = torch.mean(enc_b, 1) \n",
    "\n",
    "        # Similarity metric\n",
    "        similarity = cos(u, v)\n",
    "    \n",
    "        # Loss\n",
    "        loss = loss_fn((similarity>=0.5).float(), y)\n",
    "        acc = torch.sum((similarity>=0.5).float() == y)\n",
    "        total = y.size()[0]\n",
    "    \n",
    "        # Metrics\n",
    "        test_loss += loss.item()\n",
    "    \n",
    "    # Print results\n",
    "    d_acc = (acc)/(total)\n",
    "    loss = test_loss/len(dv_y_dl)\n",
    "    \n",
    "    tqdm.write(\n",
    "        f'Test Accuracy: {d_acc:.3f}\\\n",
    "        Test Loss: {loss:.3f}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Epochs\n",
    "epochs = 10\n",
    "print(\"Training SBERT model!\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: %d'% (epoch))\n",
    "    train_model()\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a bigger sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dev set\n",
    "# Instanciate lists\n",
    "dv_tok_ev = []\n",
    "dv_tok_cl = []\n",
    "# Get positive pair relations\n",
    "positive = df_dev.explode('evidences')[['claim_text', 'evidences']]\n",
    "# Add positive relations to list\n",
    "dv_tok_ev.extend([eviden.loc[i].values[0] for i in positive['evidences']])\n",
    "dv_tok_cl.extend(positive['claim_text'].values)\n",
    "dv_label =[1] * len(positive)\n",
    "\n",
    "# Create negative relations\n",
    "negative = neg(dv_tok_cl, 100)\n",
    "dv_tok_ev.extend(negative[0])\n",
    "dv_tok_cl.extend(negative[1])\n",
    "dv_label.extend(negative[2])\n",
    "\n",
    "# ______________________________Test data______________________________\n",
    "# Datasets\n",
    "dv_ev_ds = Dataset(dv_tok_ev, tokenizer)\n",
    "dv_cl_ds = Dataset(dv_tok_cl, tokenizer)\n",
    "\n",
    "# Dataloaders\n",
    "dv_ev_dl = DataLoader(dv_ev_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "dv_cl_dl = DataLoader(dv_cl_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "dv_y_dl = DataLoader(dv_label, batch_size=bs)\n",
    "\n",
    "# ______________________________Test data______________________________\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), '../data/SBERT')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> 3 Get embeding matrices</h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = len(tokenizer.word2index)+1\n",
    "d_model = 300\n",
    "n_head = 1\n",
    "dropout = 0.1\n",
    "hidden_dim = 2048\n",
    "num_layers = 3\n",
    "# Instanciate model\n",
    "model = BERT(vocab_size, d_model, num_layers, n_head, dropout).to(device)\n",
    "model.load_state_dict(torch.load('../data/SBERT'))\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on evidence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evidence\n",
    "ev_claims = list(eviden['evidence'])\n",
    "# Loader\n",
    "bs = 32 * 4\n",
    "evidence_ds = Dataset(ev_claims, tokenizer)\n",
    "evidence_dl = DataLoader(evidence_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "# Predict\n",
    "ev_enc = []\n",
    "for batch in tqdm(evidence_dl):\n",
    "    # Set parameters\n",
    "    sent, mask = batch\n",
    "    model.eval()\n",
    "    # Encoder\n",
    "    enc = model(sent, mask)\n",
    "    # Mean pooling\n",
    "    enc = torch.mean(enc, 1) \n",
    "    ev_enc.extend(enc.cpu().detach().numpy())\n",
    "ev_enc = np.array(ev_enc)\n",
    "print(ev_enc.shape)\n",
    "# Save to json\n",
    "with open('encoded_evidence.json', 'wb') as f:\n",
    "    np.save(f, ev_enc)\n",
    "# load file\n",
    "with open('encoded_evidence.json', 'rb') as f:\n",
    "    ev_enc_ = np.load(f)\n",
    "assert ev_enc.shape == ev_enc_.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on development claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get claims\n",
    "dv_claims = list(df_dev['claim_text'])\n",
    "# Loader\n",
    "dv_claims_ds = Dataset(dv_claims, tokenizer)\n",
    "dv_claims_dl = DataLoader(dv_claims_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "# Predict\n",
    "dv_enc = []\n",
    "for batch in tqdm(dv_claims_dl):\n",
    "    # Set parameters\n",
    "    sent, mask = batch\n",
    "    model.eval()\n",
    "    # Encoder\n",
    "    enc = model(sent, mask)\n",
    "    # Mean pooling\n",
    "    enc = torch.mean(enc, 1) \n",
    "    dv_enc.extend(enc.cpu().detach().numpy())\n",
    "dv_enc = np.array(dv_enc)\n",
    "print(dv_enc.shape)\n",
    "# Save to json\n",
    "with open('encoded_dev_claims.json', 'wb') as f:\n",
    "    np.save(f, dv_enc)\n",
    "# load file\n",
    "with open('encoded_dev_claims.json', 'rb') as f:\n",
    "    dv_enc_ = np.load(f)\n",
    "assert dv_enc.shape == dv_enc_.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataloader\n",
    "ts_cl_ds = Dataset(claim_test, tokenizer)\n",
    "ts_cl_dl = DataLoader(ts_cl_ds, batch_size=bs, collate_fn=collate_batch)\n",
    "# Predict\n",
    "ts_enc = []\n",
    "for batch in tqdm(ts_cl_dl):\n",
    "    # Set parameters\n",
    "    sent, mask = batch\n",
    "    model.eval()\n",
    "    # Encoder\n",
    "    enc = model(sent, mask)\n",
    "    # Mean pooling\n",
    "    enc = torch.mean(enc, 1) \n",
    "    ts_enc.extend(enc.cpu().detach().numpy())\n",
    "ts_enc = np.array(ts_enc)\n",
    "print(dv_enc.shape)\n",
    "# Save to json\n",
    "with open('encoded_ts_claims.json', 'wb') as f:\n",
    "    np.save(f, ts_enc)\n",
    "# load file\n",
    "with open('encoded_ts_claims.json', 'rb') as f:\n",
    "    ts_enc_ = np.load(f)\n",
    "assert ts_enc.shape == ts_enc_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
